

Machine Learning for process systems analysis
copilot/Agentic assisted framework for systems analysis.
systems analysis: Technoeconomic analysis and Life cycle assessment 

MCP feature for tool calling
LLM enabled interactions
Python background
Predictive tools


Opportunities , assests and tools useful
OpenLCA copilot for GUI copilot
Biosteam: library for TEA
Brightway python library for LCA

SWOM
Strength



- [ ] a formal 8–12 page proposal (with figures, a Gantt timeline, and a short related-work section)


## Draft 1

Agentic Machine Learning for Integrated Techno-Economic and Life-Cycle Systems Analysis

## Abstract

Process systems analysis (PSA) underpins technology assessment, investment decisions, and policy design, yet it is slow, expert-intensive, and difficult to reproduce. This PhD proposes an **agentic, LLM-enabled co-pilot** that orchestrates open-source PSA tools through the **Model Context Protocol (MCP)** and Python toolchains to deliver **techno-economic analysis (TEA)** and **life-cycle assessment (LCA)** with rigorous uncertainty, interpretability, and human-in-the-loop governance.
Case studies in hydrogen (PEM), biorefineries, and plastics recycling demonstrate: (i) reliable cost/impact estimates with quantified uncertainty; (ii) design space exploration for cost–carbon–water trade-offs; and (iii) an auditable conversational interface that shortens expert workflows and improves reproducibility.
Deliverables include open protocols for TEA–LCA coupling, benchmark datasets, an MCP tool suite, and validated ML models for robust decision support.

## 1) Motivation & Research Gap

- **Fragmented workflows:** TEA and LCA are often performed in separate tools with manual data handoffs, leading to mismatches in system boundaries, functional units, and cut-off rules.
- **Slow iteration:** Rigorous flowsheet simulation and LCI compilation are compute- and labor-intensive; sensitivity and Monte Carlo propagate this cost.
- **Limited accessibility:** Expert knowledge (allocation, background database choices, coproduct handling) is tacit; documentation and provenance are inconsistent.
- **Emerging opportunity:** **Agentic LLMs** can plan tasks, enforce schemas, call tools, and keep audit trails; **MCP** standardizes tool calling; **surrogates** and **active learning** can replace many expensive simulations while preserving fidelity and uncertainty.
## 2) Objectives & Hypotheses

**O1.** Build an **agentic co-pilot** that plans, executes, and explains PSA tasks across TEA and LCA tools using MCP.
**O2.** Develop **physics-aware predictive models** (e.g., GPs, PINNs, and gradient-boosted trees) as surrogates for flowsheet/calculator evaluations with calibrated uncertainty.
**O3.** Create a **robust TEA–LCA coupling layer** that harmonizes system boundaries, functional units, and allocation while retaining provenance for auditability.
**O4.** Demonstrate **uncertainty-aware, multi-objective optimization** of cost, GHG, water, and energy; quantify robustness under data and model uncertainty.
**O5.** Evaluate usability and reproducibility: measure time-to-result, error rates, and decision quality with and without the co-pilot.

**H1.** An MCP-orchestrated agent can **reduce expert time** to complete end-to-end TEA+LCA by ≥50% while maintaining methodological correctness.  
**H2.** Surrogates trained via active learning can achieve **≤5–10% error** on key metrics (e.g., MSP, GWP100) at **10–100× speedup**.  
**H3.** Provenance-first, schema-enforced workflows increase **reproducibility** and reduce methodological inconsistencies across studies.



## 5) Case Studies

1. **PEM Hydrogen Production**
    - Design variables: current density, pressure, temperature, membrane thickness, stack sizing, BoP configurations, electricity mixes.
    - Outputs: LCOH/MSP; GWP100; sensitivity to grid CI, stack life, membrane cost; **robust design** under market volatility.
        
2. **Lignocellulosic Biorefinery (BioSTEAM)**
    - Feedstock blends, enzyme loadings, distillation/fermentation choices; coproduct crediting vs allocation.
    - TEA–LCA coupling to compare product slates (ethanol, SAF blendstocks, biochemicals).
        
3. **Plastics Recycling & Waste-to-Energy Interactions**
    - MRF configurations, mechanical vs chemical recycling, WTE synergies; EPR and recycled content scenarios.
    - Policy scenarios: carbon price, EPR fees; **marginal abatement cost** curves.



----


## 1. Introduction and Problem Statement

### 1.1 Current Challenges in Process Systems Analysis

Process systems analysis, encompassing technoeconomic analysis and life cycle assessment, represents a critical bottleneck in sustainable process development. Current methodologies suffer from several limitations:

- **Expertise Barriers**: TEA and LCA require specialized domain knowledge, limiting accessibility to non-experts
- **Time Intensity**: Manual analysis processes can take weeks to months for comprehensive assessments
- **Inconsistency**: Human analysts introduce variability in methodological choices and interpretation
- **Integration Complexity**: Coupling TEA and LCA requires navigating disparate software ecosystems and data formats
- **Limited Predictive Capability**: Traditional approaches are reactive rather than predictive, limiting proactive optimization

### 1.2 Emerging Opportunities

Recent advances in machine learning, particularly large language models and agentic AI systems, present unprecedented opportunities to address these challenges. The convergence of several technological trends creates a unique research opportunity:

- **LLM Reasoning Capabilities**: Modern LLMs demonstrate sophisticated reasoning about technical domains
- **Tool Integration Protocols**: MCP enables seamless integration of specialized analytical tools
- **Mature Python Ecosystem**: Libraries like Biosteam and Brightway provide robust computational foundations
- **Agentic AI Frameworks**: Autonomous agents can orchestrate complex analytical workflows


## 2. Literature Review and Research Gap

### 2.1 Current State of Process Systems Analysis

Traditional process systems analysis relies heavily on specialized software tools and expert knowledge. Biosteam has emerged as a leading platform for biorefinery TEA, offering comprehensive process modeling capabilities. Similarly, Brightway provides LCA computational frameworks with extensive database integration capabilities. However, these tools require significant expertise and manual configuration.

Recent work in automated process analysis has focused primarily on optimization algorithms and sensitivity analysis. Limited research has explored the integration of natural language interfaces with specialized analytical tools for autonomous systems analysis.

### 2.2 Machine Learning in Process Engineering

Machine learning applications in process engineering have primarily focused on process control, optimization, and predictive maintenance. Emerging research explores LLM applications in chemical engineering, but comprehensive frameworks for autonomous systems analysis remain unexplored.

### 2.3 Research Gap

The literature reveals a significant gap in intelligent, autonomous frameworks capable of conducting end-to-end process systems analysis. No existing work combines LLM reasoning capabilities with specialized TEA/LCA tools through standardized protocols like MCP to create truly autonomous analytical copilots.


## 3. Research Objectives

### 3.1 Primary Objective

Develop and validate an intelligent, machine learning-enabled agentic framework that autonomously conducts comprehensive process systems analysis, integrating technoeconomic assessment and life cycle analysis through natural language interfaces and tool orchestration.

### 3.2 Specific Objectives

1. **Framework Architecture**: Design a modular agentic architecture integrating LLMs with specialized analytical tools via MCP
2. **Tool Integration**: Develop MCP-compliant interfaces for Biosteam, Brightway, and complementary analytical tools
3. **Intelligent Reasoning**: Implement machine learning models for automated methodological decision-making in TEA/LCA
4. **Predictive Analytics**: Create ML models for performance prediction and optimization recommendation
5. **Validation Framework**: Establish comprehensive validation methodologies for autonomous analytical results
6. **User Interface Development**: Design intuitive natural language interfaces for non-expert users


## 4. Methodology

### 4.1 Framework Architecture

The proposed framework adopts a multi-agent architecture with specialized agents for different analytical domains:

**Core Components:**

- **Orchestrator Agent**: Central coordination and workflow management
- **TEA Agent**: Specialized in technoeconomic analysis using Biosteam
- **LCA Agent**: Focused on life cycle assessment using Brightway
- **Integration Agent**: Manages data flow and result synthesis
- **Validation Agent**: Ensures result quality and methodological consistency

### 4.2 Technical Implementation Strategy

#### Phase 1: Foundation Development (Months 1-12)
- Develop MCP interfaces for Biosteam and Brightway integration
- Create basic agentic framework architecture
- Implement core LLM reasoning capabilities for process analysis
- Establish data management and workflow orchestration systems

#### Phase 2: Intelligence Enhancement (Months 13-24)
- Develop machine learning models for automated parameter selection
- Implement predictive analytics for process performance estimation
- Create natural language processing capabilities for requirement interpretation
- Build automated methodology selection algorithms

#### Phase 3: Integration and Optimization (Months 25-36)
- Integrate TEA and LCA capabilities into unified analytical workflows
- Develop GUI copilot interfaces for OpenLCA integration
- Implement advanced optimization and sensitivity analysis capabilities
- Create comprehensive validation and uncertainty quantification frameworks

#### Phase 4: Validation and Deployment (Months 37-48)
- Conduct extensive validation studies across multiple process domains
- Develop user studies and usability assessments
- Create deployment frameworks and documentation
- Establish performance benchmarks and comparative analyses



### 4.3 Machine Learning Components

#### 4.3.1 Predictive Models
- **Parameter Estimation**: Automated estimation of missing process parameters using similarity-based learning, data fetching through api, physic infromed design
- **Uncertainty Quantification**: Bayesian approaches for quantifying analytical uncertainty

#### 4.3.2 Decision Support Systems

- **Scenario Generation**: Generative models for comprehensive scenario analysis
- **Optimization Guidance**: Reinforcement learning for process optimization recommendations


### 4.4 Tool Integration Strategy

#### 4.4.1 MCP Implementation

Develop comprehensive MCP servers for:

- **Biosteam Integration**: Complete process modeling and TEA capabilities
- **Brightway Integration**: LCA computation and database access
- **Data Management**: Standardized data exchange protocols
- **Visualization**: Automated result visualization and reporting

#### 4.4.2 Python Ecosystem Leverage

Utilize the rich Python ecosystem for:

- **Scientific Computing**: NumPy, SciPy for numerical analysis
- **Machine Learning**: Scikit-learn, TensorFlow for predictive modeling
- **Data Processing**: Pandas for data manipulation and analysis
- **Visualization**: Matplotlib, Plotly for result presentation


## 5. Expected Contributions and Novelty

### 5.1 Theoretical Contributions
- **Agentic Framework Theory**: Novel architectural principles for autonomous analytical systems
- **ML-TEA/LCA Integration**: Theoretical foundations for machine learning integration in systems analysis
- **Uncertainty Propagation**: Advanced methodologies for uncertainty quantification in automated analysis
### 5.2 Methodological Contributions
- **Automated Methodology Selection**: Algorithms for optimal analytical approach selection
- **Intelligent Parameter Estimation**: ML-based approaches for missing data imputation
- **Integrated Analysis Workflows**: Seamless TEA-LCA integration methodologies
### 5.3 Technological Contributions
- **MCP-Based Tool Integration**: Standardized protocols for analytical tool orchestration
- **Natural Language Interfaces**: Intuitive interaction paradigms for complex analytical tools
- **Autonomous Validation**: Self-validating analytical frameworks
### 5.4 Practical Contributions
- **Democratized Access**: Making advanced systems analysis accessible to non-experts
- **Accelerated Analysis**: Reducing analysis time from weeks to hours
- **Improved Consistency**: Standardized analytical approaches reducing human variability


## 6. Research Plan and Timeline

### Year 1: Foundation and Architecture
- **Q1-Q2**: Literature review completion, framework design
- **Q3-Q4**: Core architecture implementation, basic MCP integration
### Year 2: Intelligence and Integration
- **Q1-Q2**: ML model development, predictive analytics implementation
- **Q3-Q4**: Advanced tool integration, natural language interface development
### Year 3: Optimization and Validation
- **Q1-Q2**: Comprehensive framework integration, optimization implementation
- **Q3-Q4**: Extensive validation studies, performance benchmarking
### Year 4: Refinement and Dissemination
- **Q1-Q2**: Framework refinement based on validation results
- **Q3-Q4**: Thesis writing, publication preparation, deployment preparation
## 7. Resources and Infrastructure Requirements

### 7.1 Computational Resources
- High-performance computing access for ML model training
- Cloud infrastructure for scalable deployment and testing
- GPU resources for deep learning model development
### 7.2 Software and Tools
- Python development environment with specialized libraries
- Access to commercial LCA databases through Brightway
- Development licenses for complementary analytical tools

### 7.3 Data Requirements
- Historical process data for ML model training
- Validated TEA/LCA case studies for framework validation
- Expert annotations for methodology validation





-----

# **PhD Research Proposal:** An Agentic Machine-Learning Framework for Autonomous Process Systems Analysis Integrating Techno-Economic Analysis and Life Cycle Assessment

## **1. Introduction and Problem Statement**

Process Systems Analysis (PSA) is critical in chemical engineering for evaluating the viability and sustainability of process designs through techniques like techno-economic analysis (TEA) and life cycle assessment (LCA). TEA quantifies capital and operating costs, economics, and techno-feasibility, while LCA quantifies environmental impacts across a product’s life cycle[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20an%20open,outputs%20over%20its%20life%20cycle). Despite their importance, current PSA practices face significant challenges. Performing a rigorous LCA demands _“a great deal of time and data”_, which often limits its application during early process development[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=%28evaluation%20of%20results%29%20,and%20can%20improve%20data%20quality). TEA and LCA are typically carried out by different domain experts using separate tools (e.g. process simulators for TEA, LCA software for environmental analysis), leading to **expertise bottlenecks** and **integration complexity**. Indeed, LCA methodologies have historically been _“disconnected from biorefinery design, simulation, and techno-economic analysis”_, hampering accurate sustainability projections[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=Objective%20Biorefineries%20will%20play%20a,technology%20performance%2C%20and%20contextual%20landscape). This disconnect means engineers must manually reconcile results from disparate TEA and LCA analyses – a time-consuming and error-prone endeavor requiring multidisciplinary expertise.

Recent reviews highlight a pressing need for automation and integration in PSA. Köck _et al._ (2023) found that automating data flow between process simulations and LCA can _“make integration of environmental impacts into decisions easier, less time-consuming, and improve data quality”_[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=%28evaluation%20of%20results%29%20,and%20can%20improve%20data%20quality)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=reviews%20current%20developments%20in%20the,and%20can%20improve%20data%20quality). However, existing efforts at PSA automation remain limited. Automation has been implemented mostly via custom interfaces linking simulators to LCA databases, or via machine learning (e.g. neural networks for predicting molecular properties)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=engineering,created%20resources%20was%20only%20followed). Fewer than one-third of published methods share open code, and only 10 of 30 surveyed papers included uncertainty analysis[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=the%20models%20was%20reached,development%20of%20new%20automation%20methodologies) – indicating that current solutions are often proprietary, not reproducible, and may neglect uncertainty quantification. In summary, **the problem is that current PSA practices are labor-intensive, siloed, and not sufficiently leveraging modern AI/ML tools, resulting in a bottleneck for rapid sustainable process innovation**.

**Research Gap:** No existing framework unifies TEA, LCA, large language models (LLMs), and predictive machine learning in an _autonomous, agentic system_ for process analysis. Tools like **BioSTEAM** (an open-source process simulation and TEA platform) and its LCA extension have demonstrated the value of tighter integration – e.g. BioSTEAM-LCA automates early-stage TEA-LCA for biorefineries, enabling simultaneous economic and environmental evaluation under uncertainty[cabbi.bio](https://cabbi.bio/datasets/biosteam-and-biosteam-lca/#:~:text=Additionally%2C%20an%20agile%20life%20cycle,performance%2C%20economics%2C%20and%20environmental%20impacts)[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=projections%20of%20their%20environmental%20impacts,technology%20performance%2C%20and%20contextual%20landscape). BioSTEAM integrates uncertainty into simulations to _“streamline and automate early-stage technology evaluations”_, providing rigorous sensitivity analyses[cabbi.bio](https://cabbi.bio/datasets/biosteam-and-biosteam-lca/#:~:text=BioSTEAM%20is%20a%20fast%20and,platform%2C%20BioSTEAM%20aims%20to%20foster). It _“enables the integration of design, simulation, TEA and LCA to improve consistency and transparency”_ of sustainability metrics[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=,). Yet, even these advanced tools do not incorporate the **agentic intelligence** now possible with AI. Large Language Models have recently shown remarkable ability to plan and execute complex tasks autonomously in scientific workflows[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=,4o)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=Claude,local%20attention%20and%20global%20oversight). In chemical engineering, early studies have harnessed LLM-based agents for process control and simulation: e.g. Vyas and Mercangoz (2025) demonstrated an LLM-driven agent that plans fault recovery steps and controls a chemical process, outperforming classical methods in complex scenarios[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=,loop%20iteratively%20refines%20invalid%20plans)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=a%20laboratory%20TCLab%20platform%20,driven%20automation%20in%20chemical%20engineering). Liu _et al._ (2024) developed an autonomous simulation agent using GPT-4 that can iterate through simulation experiments and analyses with minimal human input[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=experimental%20processes%20and%20computational%20simulations,flawless)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=execution%20on%20designated%20research%20missions%2C,local%20attention%20and%20global%20oversight). These works hint at a new paradigm where AI agents handle both _symbolic reasoning_ (e.g. planning experimental or analytical steps) and _numeric computation_ (running simulations or controllers). **However, such agentic frameworks have not been applied to end-to-end process systems analysis combining TEA and LCA.** Moreover, incorporating predictive ML models (e.g. surrogates for expensive simulations) and decision-support (e.g. scenario generation, optimization) into the loop remains an open challenge. This proposal addresses that gap by developing an _agentic, machine-learning-enabled framework for autonomous PSA_, in which an AI agent orchestrates process simulations, economic calculations, environmental assessments, and optimization routines in an integrated manner. We aim to overcome current bottlenecks by automating expertise-intensive tasks, ensuring TEA-LCA consistency, and greatly accelerating design-space exploration.

In the following, we outline the proposed framework and its technical architecture (Section 2), the research objectives and methodology (Section 3), case study plans for validation (Section 4), a timeline for completion (Section 5), expected contributions (Section 6), and ethical considerations (Section 7). A brief review of related work is also included (Section 2.1) to position this research in the context of recent advances in ML-for-PSA and automation.

## **2. Background and Related Work**

**2.1 Integrated TEA-LCA in Process Analysis:** Traditional approaches treat TEA and LCA separately, often leading to misaligned assumptions. Recent efforts have integrated these analyses to provide unified insights. The BioSTEAM platform and its LCA extension (BioSTEAM-LCA) are notable examples[cabbi.bio](https://cabbi.bio/datasets/biosteam-and-biosteam-lca/#:~:text=Additionally%2C%20an%20agile%20life%20cycle,performance%2C%20economics%2C%20and%20environmental%20impacts). BioSTEAM is an open-source steady-state process simulator enabling design, simulation, and TEA under uncertainty[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=BioSTEAM%20%E2%80%93%20the%20Biorefinery%20Simulation,evaluated%20in%20BioSTEAM%20closely%20match). It automates unit operation sizing, computes capital and operating costs, and can generate data needed for LCA (e.g. waste streams, resource consumption)[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=benchmark%20designs%20modeled%20in%20proprietary,prioritize%20research%2C%20development%2C%20and%20deployment). Shi _et al._ (2020) layered an LCA module onto BioSTEAM, creating an agile TEA-LCA tool that characterizes environmental impacts alongside economics across thousands of scenarios[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=projections%20of%20their%20environmental%20impacts,data%20as%20kernel%20density%20maps)[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs). Using this, they showed that evaluating _1000 process design scenarios took only ~5 minutes_ on a PC[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs), highlighting the power of automation and surrogate-assisted computation in PSA. The integrated framework also enabled global sensitivity analysis to identify key cost and emissions drivers[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs). Other researchers have emphasized the need for such integrated analyses. A review by Yuan _et al._ (2024) notes that combining _“process performance, economics, and environmental impacts”_ in a transparent way is crucial for responsible AI in chemical engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=which%20is%20defined%20based%20on,as%20an%20example%20to%20enhance)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=matter%20in%20a%20way%20that,solving%20bottleneck%20challenges%20in%20CE). Our framework builds on these successes, using BioSTEAM (or similar) for core process simulation and TEA, and Brightway or OpenLCA for LCA, but extends them with an autonomous AI agent and advanced ML components.

**2.2 Machine Learning and Surrogate Modeling in PSA:** Machine learning has been applied to expedite process simulations and optimize designs. Gaussian Process (GP) regression is widely used to create surrogate models of expensive simulation outputs, providing fast predictions with uncertainty estimates – a cornerstone of Bayesian optimization. Physics-Informed Neural Networks (PINNs) have emerged as a way to embed physical laws into neural network models, enabling surrogates that respect conservation laws or known kinetics, thus improving extrapolative reliability. Gradient boosting machines (GBMs) and other ensemble methods are also popular for their efficiency in approximating complex relationships with modest data. In process engineering, such surrogates have proven valuable. For example, GP and neural network surrogates have been used to approximate flowsheet performance, allowing rapid evaluation in optimization algorithms (e.g. in biorefinery superstructure optimizations[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=,refinery%20structure.)). Surrogates can also facilitate uncertainty propagation by enabling Monte Carlo simulations that would be prohibitively slow with full models. Despite these benefits, integrating surrogate modeling into a general PSA workflow is non-trivial. Automated collection of training data from process simulators and continuous retraining as new scenarios are explored require coordination – a role suited for an intelligent agent. Our proposed system incorporates GP surrogates for unit process models (e.g. predicting yield or energy use as a function of operating conditions), PINNs for processes where first-principles are known (e.g. reactor kinetics), and GBMs for quick predictive tasks (such as mapping feedstock properties to process performance). These models will be orchestrated by the agent to speed up scenario evaluations and guide optimization, as detailed in Section 3.

**2.3 Autonomous Agents and LLMs in Chemical Engineering:** The advent of large language models has unlocked new paradigms for automation. LLM-based agents can interpret high-level instructions, generate plans, call software tools, and self-correct based on feedback. In chemical engineering and related fields, this concept is only beginning to be explored. Vyas & Mercangoz (2025) introduced an _“agentic framework for industrial automation”_ using LLMs for both discrete planning and continuous control[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=,loop%20iteratively%20refines%20invalid%20plans). Their system uses a finite-state-machine guided approach: an LLM planner proposes a sequence of actions to handle a process fault, a simulation agent executes each action, and a validator loop checks outcomes – iterating until a valid solution is found[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=introduce%20a%20unified%20agentic%20framework,source%20LLMs%20in%20both%20accuracy)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=recovery%20sequences%20through%20the%20FSM%2C,attains%20similar%20performance%2C%20while%20ablation). Impressively, GPT-4-based agents in this framework achieved 100% success in finding valid recovery plans across hundreds of trials, and even maintained control of a physical twin column with performance comparable to a tuned PID controller[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=In%20Case%20Study%201%2C%20across,following%20lapses%20and%20coarse%20ODE)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=a%20laboratory%20TCLab%20platform%20,driven%20automation%20in%20chemical%20engineering). These results demonstrate that LLM agents can _“unify high-level symbolic planning and low-level control”_ in chemical processes[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,driven%20automation%20in%20chemical%20engineering). Another study by Zhihan Liu _et al._ (2024) developed an Autonomous Simulation Agent (ASA) that _“automates the entire simulation research process”_ using LLM prompt engineering[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=,4o). Given a research goal, their ASA designs experiments, runs simulations on remote servers, analyzes data, and compiles reports – iterating up to 20 cycles without human intervention[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=experimental%20processes%20and%20computational%20simulations,flawless)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=execution%20on%20designated%20research%20missions%2C,local%20attention%20and%20global%20oversight). It achieved near-flawless execution on a polymer simulation case, showcasing reliability of LLM agents in long-horizon tasks[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=of%20polymer%20chain%20conformations%20as,validation%20mechanisms%2C%20and). These pioneering works inspire our approach: we will leverage LLMs (such as GPT-4 or open-source equivalents) as the “brain” of an autonomous PSA system. The agent will break down complex PSA problems into actionable steps (e.g. _“simulate process at X conditions”_, _“perform LCA with dataset Y”_, _“train a surrogate model for Z”_), execute them via integrated tools, and interpret the results to decide next steps.

**2.4 Reinforcement Learning and Decision Support for Process Design:** Beyond static optimization, reinforcement learning (RL) has been explored for process synthesis and decision-making. Gao _et al._ (2023) showed that RL algorithms can _“learn to build process flowsheets”_ by sequentially adding unit operations and connections, in tandem with a process simulator (DWSIM) providing rewards[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=learn%20to%20build%20process%20flowsheets,that%20stores%20knowledge%20gained%20while). A challenge noted is the heavy computational demand: an RL agent may require thousands of simulator calls, so researchers often resort to simplified “shortcut” models to train the agent, sacrificing accuracy[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=learn%20to%20build%20process%20flowsheets,that%20stores%20knowledge%20gained%20while). Gao _et al._ addressed this via transfer learning, pre-training the RL agent on easier models and then fine-tuning on the rigorous simulator, which yielded an economically feasible flowsheet with 8% higher revenue while halving learning time[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=RL%20agent%20demands%20numerous%20process,process%20design%20and%20apply%20it)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=separation%2C%20and%20recycles%2C%20our%20method,can%20be%20reduced%20by%20a). Other studies have applied hierarchical RL to flowsheet synthesis, and actor–critic methods to process design, confirming RL’s potential but also its need for efficient simulation (or surrogate) support[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=facilitate%20process%20design,however%2C%20lead%20to%20inaccurate%20results)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=established%20approach%20from%20machine%20learning,can%20be%20reduced%20by%20a). Our framework does not primarily focus on RL for building flowsheets from scratch; however, we incorporate RL concepts for _scenario generation and sequential decision support_. For example, the agent could use an RL policy to choose which process modifications to explore next (analogous to an expert system suggesting experiments), balancing exploration of novel configurations with exploitation of promising regions identified by surrogates. Additionally, multi-objective optimization (MOO) – particularly Multi-Objective Bayesian Optimization (MOBO) – is a key decision-support capability we will implement. MOBO will allow the automated exploration of trade-offs between economic and environmental objectives (e.g. minimizing cost versus greenhouse gas emissions) by intelligently sampling design alternatives and using surrogate models (GPs) to approximate the Pareto frontier. This is aligned with suggestions in literature to consider _“global warming impact parallel to cost estimation via multi-objective optimization”_[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=maximal%20growth%20with%20product%20concentration,). Overall, our system will synergize these AI techniques: the LLM agent can orchestrate MOBO loops and possibly embed an RL-based strategy for complex decision flows, all while ensuring the TEA and LCA data are consistently handled.

In summary, **the state of the art** reveals isolated pieces – integrated TEA/LCA toolkits, ML surrogates for speeding computation, LLM agents for automation, and RL for design – but no unified solution. This proposal’s novelty lies in **combining these advances into one coherent framework**: an autonomous “copilot” for process systems engineering that can perform end-to-end analyses (from technical simulation to economic and environmental evaluation) with minimal human intervention. The next section details the architecture and components of this proposed framework.

## **3. Proposed Framework and Methodology**

**3.1 Architecture Overview:** The system will be built around an LLM-driven _agentic orchestrator_ that interfaces with simulation engines, databases, and machine learning modules. **Figure 1** depicts the high-level architecture of the proposed framework. The central AI agent (implemented via a state-machine approach using the Model Context Protocol, MCP) serves as the coordinator, receiving high-level goals from the user and managing the workflow through various tools and sub-modules.

![](blob:https://chatgpt.com/bc8e9b59-bfb5-43ce-b13f-9d7233c9f75c)

_Figure 1: Proposed architecture of the autonomous PSA framework. An LLM-based agent orchestrator (left) communicates with process simulation & TEA tools (top center, e.g. BioSTEAM or Aspen), LCA engines (center, e.g. Brightway or OpenLCA), and an ML toolkit (right) for surrogates and optimization. The agent uses the Model Context Protocol (MCP) to manage tool interactions with well-defined states (planning, executing, reviewing), enabling it to plan experiments, execute simulations, invoke LCA calculations (with background databases), train models, and iterate towards optimal solutions. Dashed lines indicate information flows or user inputs/oversight._

At the core of the architecture is the **LLM Agent Orchestrator**, which will be implemented as an AI agent with defined states and transition logic (e.g., using the MCP framework as in Claude’s agent orchestration[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts)). The agent operates through a cycle of states such as _PLANNING_ (devising a plan of action), _EXECUTING_ (calling tools to perform tasks), _REVIEWING_ (evaluating results and checking constraints), and _ERROR handling_[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts). This stateful design ensures the agent’s actions are transparent and logically structured, rather than monolithic black-box decisions. The agent is “agentic” in that it can make decisions to pursue sub-goals autonomously: for example, if initial results show high uncertainty in a cost estimate, the agent might decide to run additional simulations or refine a surrogate model to reduce that uncertainty, without explicit human instruction.

Key components and data flows in Figure 1 include:

- **Process Simulator & TEA Module:** We will use an open-source simulator (BioSTEAM as a primary candidate) to perform process material and energy balance calculations and compute techno-economic metrics (capital cost, operating cost, net present value, etc.). The agent provides _design parameters_ or flowsheet configurations to this module (e.g. operating conditions, equipment sizes, feedstock choices), and receives back simulation results: mass and energy balances, stream flows, utility usage, and itemized costs[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=benchmark%20designs%20modeled%20in%20proprietary,prioritize%20research%2C%20development%2C%20and%20deployment). BioSTEAM’s API will facilitate programmatic modification of process configurations and retrieval of results. Notably, BioSTEAM automates equipment sizing and costing, and can handle uncertainty distributions for inputs[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=through%20its%20fast%20and%20flexible,via), which we will exploit for stochastic analysis.
    
- **LCA Engine:** For life-cycle assessment, we plan to integrate the **Brightway2** LCA framework[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20an%20open,outputs%20over%20its%20life%20cycle) or alternatively leverage OpenLCA’s backend via its Python API (openLCA provides a JSON-RPC interface for external calls[greendelta.github.io](https://greendelta.github.io/openLCA-ApiDoc/#:~:text=openLCA%20provides%20an%20API%20for,to%20call%20functions%20in%20openLCA)[greendelta.github.io](https://greendelta.github.io/openLCA-ApiDoc/#:~:text=In%20the%20dialog%2C%20you%20can,browser%20using%20the%20Fetch%20API)). The agent will take inventory data from the TEA simulation (e.g. raw material requirements per functional unit, energy consumed, emissions generated on-site) and feed that into the LCA engine along with background datasets. Brightway offers high flexibility in constructing LCA models from Python and efficiently calculating impact indicators[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20designed%20to%20make,users%20from%20industry%20and%20consulting). Meanwhile, OpenLCA’s API allows connection to extensive databases (like ecoinvent or USEEIO) and even the possibility of the agent operating the OpenLCA GUI via a “copilot” mode for data curation and QA. By _“curation and QA”_, we refer to ensuring that the mapping between process simulation outputs and LCA inputs is correct – for instance, if the process produces a waste sludge, the agent (via an LLM) could search the LCA database for an appropriate disposal process, or prompt the user if multiple choices exist. We will encode guidelines so that the agent selects LCA data consistent with TEA assumptions (e.g. same geographical region, technology maturity). The LCA engine returns environmental impact results (e.g. GHG emissions, energy footprint, water use) which the agent can then interpret alongside economic results.
    
- **ML Toolkit:** This module encompasses all machine-learning components for accelerating and enhancing analysis. It includes surrogate model training (regression models like GPs, neural nets, etc.), design of experiments, and optimization algorithms. The agent can invoke functions here to, for example, fit a Gaussian Process model to simulation input-output data accumulated from multiple runs. This surrogate can then predict outcomes for new designs almost instantly, which the agent can use inside an optimization routine. We will implement Multi-Objective Bayesian Optimization (MOBO) where the agent calls a BO algorithm that uses the GP surrogates of objectives (e.g. minimized cost, minimized CO₂ emissions) to propose next candidate designs. The agent thus iteratively alternates between calling the **Process Simulator/TEA** (to get data and refine surrogates) and calling the **Optimizer** (to get improved design suggestions), until convergence on a Pareto-optimal set or until a specified number of iterations is reached. In addition, the ML toolkit can include a library of **Physics-Informed Neural Networks (PINNs)** for specific unit operations – for instance, a PINN that solves reactor differential equations faster than a conventional solver, or a PINN that predicts catalyst deactivation over time. The agent could choose to train a PINN if a certain unit’s simulation is the bottleneck in speed and if sufficient data or equations are available. Reinforcement Learning (RL) elements also reside in the ML toolkit: for example, an RL-based scenario generator that the agent can call to stochastically explore variations of a process configuration (useful in highly combinatorial design spaces where random exploration is needed beyond BO’s local search). The ML toolkit is thus the agent’s “analytical arm,” enabling it to go beyond brute-force simulation by leveraging learned models.
    
- **User Interface and Knowledge Base:** While the framework aspires to autonomy, the user (e.g. a process engineer or researcher) remains in the loop for setting initial goals, providing constraints, and reviewing final recommendations. The agent will accept a high-level problem statement from the user – for example: _“Evaluate the economic and environmental feasibility of producing 50,000 tonne/year of bio-ethanol via lignocellulosic feedstock, and suggest process improvements to minimize carbon footprint while maintaining profitability.”_ From this, the agent formulates specific tasks. The user can also impose constraints (e.g. “avoid using chromium catalysts due to toxicity” or “ensure IRR > 15%”) which the agent will treat as guardrails during optimization. Throughout the process, the agent maintains a **provenance log** of actions taken, tools invoked, data sources, and intermediate results. This log serves both transparency and reproducibility – every result can be traced (e.g. “Emission value X comes from Brightway using ecoinvent v3.8 process Y”). The agent will periodically report to the user, especially at decision points or upon completion, presenting findings in an interpretable form (charts, tables, explanations). Although much can be automated, **human oversight** is vital: at any point, the user may intervene, adjust assumptions, or override the agent’s decisions. This ensures that the domain expertise of engineers guides the AI and that trust is maintained.
    

**3.2 Tool Orchestration via MCP and Agent Design:** We will implement the agent using the **Model Context Protocol (MCP)** – an emerging open standard for connecting LLMs with tools and stateful control[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts). MCP allows us to define each external tool (e.g. a Biosteam simulation function, a Brightway LCA calculation function, a plotting or analysis function) with decorators and integrate them into the agent’s prompting framework[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=Add%20new%20tools%20by%20creating,mcp.tool). The agent’s prompt context will include descriptions of available actions (tools), similar to how a “function calling” API works with LLMs. For instance, we will have tools like `run_process_simulation(config)` which the agent can call with a JSON `config` of process parameters, or `perform_LCA(inventory)` to run an LCA on given inventory data. The MCP orchestration ensures that the agent can maintain a working memory of the conversation and state (through the state machine) and use tools appropriately. By adopting a state-based design, we can impose **guardrails**: e.g. in the PLANNING state, the agent is only allowed to output a plan (and not directly call tools); in the EXECUTING state, it can call tools but not generate new plans; in REVIEWING, it must analyze results and decide whether criteria are met or another iteration is needed. This structured approach prevents chaotic or unsafe agent behavior and makes the agent’s operation more interpretable. The guardrails will also include checks on results – for example, after a simulation, the agent might verify mass balance closure or check if cost results are within expected ranges, flagging any anomalies for review. If a plan fails (e.g. a simulation doesn’t converge, or a tool returns an error), the agent transitions to an ERROR state with a predefined strategy: it can either try an alternative method, adjust parameters (perhaps using an LLM-based guess of what might fix the error), or ask for human guidance if the issue persists. This aligns with the practice observed by Vyas _et al._ where a _“Validator-Reprompting loop”_ caught and corrected invalid plans[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=introduce%20a%20unified%20agentic%20framework,source%20LLMs%20in%20both%20accuracy)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,that%2C%20with%20structured%20feedback%20and). Our agent will similarly self-correct by analyzing error messages and using the LLM’s reasoning to adjust (e.g. if simulation failed due to a temperature out of range, the agent can reduce the temperature and retry, or query the surrogate model if available).

**3.3 TEA-LCA Coupling Approach:** A crucial methodological aspect is how we link TEA and LCA seamlessly. We will adopt a _gate-to-gate_ LCA approach focused on the process being designed, with up-stream and downstream impacts sourced from databases. The process simulator provides a detailed inventory of inputs and outputs for the system foreground (e.g. raw biomass input per batch, electricity used per kWh, emissions from fermentation, etc.). The agent will compile this inventory and map each item to an LCA dataset (for background processes). For example, if the simulation uses 1 tonne of corn stover, the agent will find an LCA entry for corn stover production; if electricity is consumed, it will choose an appropriate grid mix dataset based on region (optionally user-specified). This mapping can be done via Brightway’s search functions or via OpenLCA’s ontology if needed. To ensure consistency, **common assumptions** (like capacity factors, utility emissions factors) will be shared between TEA and LCA – possibly through a unified data schema. We will develop a schema or template that the agent fills, which includes all necessary info for both analyses (e.g. a JSON with sections for process design, economic factors, LCA factors). During each iteration of analysis, after simulation, the agent updates the inventory and pushes it to the LCA engine. The results from LCA (impact metrics) are then associated with the same scenario as the TEA results. This one-to-one coupling at each scenario enables multi-objective evaluation. If multiple scenarios are being compared (e.g. different process designs in a Pareto set), the agent ensures both TEA and LCA are computed for all before declaring one “dominant” – thereby avoiding bias towards designs that are only optimal in one dimension. We will also implement **uncertainty propagation** through this coupling: if certain inputs have probability distributions (e.g. yield uncertainty, price volatility), the agent can perform Monte Carlo sampling, feeding random draws into both TEA and LCA calculations for each run. Using vectorized or surrogate-assisted simulation, it could generate thousands of joint TEA-LCA outcomes rapidly, enabling a holistic uncertainty analysis (similar to BioSTEAM-LCA’s 1000-scenario demonstration[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs)). The agent will then use statistical analysis tools (in the ML toolkit) to summarize these results (e.g. probability of meeting cost target and emissions target simultaneously, sensitivity of outcomes to each uncertain parameter). Such integrated uncertainty analysis is a methodological advance ensuring that the framework not only finds “optimal” designs, but also quantifies confidence and risk.

**3.4 Machine Learning Components:** Several specific ML models and algorithms will be employed: (i) **Gaussian Process (GP) Surrogates** – for any model outputs that are computationally expensive or noisy. For instance, if optimizing a flowsheet with many continuous decision variables, instead of calling BioSTEAM for every evaluation, the agent will use GP regression to learn the mapping from decision variables to outcomes (cost, emissions). The GP’s inherent uncertainty estimate will guide the Bayesian optimizer in deciding where to sample next, focusing on regions of objective improvement or high uncertainty. (ii) **Physics-Informed Neural Networks (PINNs)** – for sub-process models like reactors, we will incorporate PINNs that the agent can train on-the-fly or load if pre-trained. For example, a PINN could solve the mass-transfer equations in a distillation column faster than the discrete tray-by-tray simulation, once trained on that column’s operating range. We will investigate training PINNs using data generated by first-principles models from BioSTEAM or external simulators, so that subsequent evaluations are instant. The agent’s logic will be: if a particular unit’s simulation accounts for a large fraction of total runtime and needs to be evaluated across many conditions, then train a PINN surrogate for it. This introduces adaptability in the framework’s use of ML. (iii) **Gradient Boosted Trees / Random Forests** – these will be used for quick approximations and feature importance analysis. For example, after gathering a dataset of ~100 scenario results (with various inputs), the agent might fit a tree-based model to rank which input uncertainties contribute most to outcome variance (providing explainability). Tree models can also serve as fallback surrogates when data is sparse or relationships are highly non-linear in ways GPs struggle with. (iv) **Multi-Objective Optimization algorithms** – aside from MOBO (which relies on GP), we will also implement or integrate evolutionary algorithms (like NSGA-II) as a comparison or backup for multi-objective optimization, particularly for non-smooth search spaces or when we want a diversity of solutions. The agent can deploy such an algorithm on the surrogate model (or even directly on simulation if feasible) to identify an approximate Pareto front, which it then refines via MOBO around the interesting regions. (v) **Reinforcement Learning for scenario policy** – as mentioned, we may encode certain decision problems as an RL environment. For instance, controlling a semi-batch process could be framed as an RL task where the agent chooses control actions over time to maximize yield and minimize energy. A policy learned in such environment (perhaps using deep Q-learning or actor-critic methods) could be embedded into the overall framework to handle dynamic optimization sub-problems. The agent’s planning step would then include using that policy to set, say, the heating profile of a reactor for each scenario rather than optimizing it with static DOEs. This way, continuous process control and static design decisions can be tackled together (similar in spirit to Vyas _et al._’s unification of planning and control[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,driven%20automation%20in%20chemical%20engineering)).

Throughout the development of these ML components, emphasis will be placed on **validation and verification**: each surrogate model will be validated against a holdout of simulation data or known physics to ensure fidelity. The agent will only rely on surrogates within the domain they are validated for – otherwise it will schedule new simulation runs to extend the training data (active learning). This guarantees that automated decisions are based on trustworthy models.

**3.5 Agentic Decision-Making and Workflow:** Bringing everything together, a typical workflow the agent would execute is:

1. **Initialization:** User defines the problem (e.g. process type, feedstock, product, scale) and objectives. The agent gathers necessary baseline data (perhaps running a base case simulation in BioSTEAM to establish feasibility).
    
2. **Planning:** The LLM agent in PLANNING state outlines a plan, for example: _“1) Evaluate base case TEA and LCA. 2) Identify major cost and emission drivers. 3) Train surrogates for those drivers. 4) Optimize process parameters to reduce cost and emissions. 5) Validate optimized design with full simulation and LCA. 6) Iterate if needed or analyze uncertainty.”_ This plan is internally represented and then executed step by step.
    
3. **Execution – Base Case:** The agent transitions to EXECUTING state and calls the simulation tool with base case parameters (via Biosteam API). It obtains results, then calls the LCA tool with the resulting inventory. Once results are in, it enters REVIEWING state: the LLM summarizes the base case outcomes (e.g. _“Base case cost = $X/ton, GHG = Y kg CO₂/ton. High cost due to expensive enzyme; high emissions due to natural gas usage in boiler.”_). The agent identifies that enzyme cost and boiler emissions are drivers.
    
4. **Execution – Surrogate Training:** The agent decides to vary enzyme loading rate and boiler fuel type in the simulation to see impact. It generates a design of experiments (maybe Latin hypercube) for these factors plus any others of interest (e.g. reactor temperature, catalyst loading, etc.), and batch-runs simulations (possibly in parallel, as BioSTEAM is fast). With the collected data, it trains a GP surrogate for cost as a function of those factors, and another for emissions. It reviews the model fit (e.g. R², error metrics). If fit is poor, it can decide to get more data in regions of high error (active learning).
    
5. **Execution – Optimization:** With surrogates ready, the agent calls the MOBO routine to find Pareto-optimal solutions (minimizing cost and emissions). Suppose it finds that lowering enzyme loading reduces cost but increases emissions due to longer processing time (needing more energy) – so there’s a trade-off. It identifies a few candidate solutions (one favoring cost, one favoring environment, one balanced). The agent then runs full simulations for these candidates to verify surrogate predictions, and performs LCA on them. It might refine the surrogate if discrepancies are found.
    
6. **Decision:** In REVIEWING state, the agent compares the candidates. It may apply further decision logic, e.g., eliminate any that violate constraints (like if a candidate has slightly lower cost but massively higher emissions, it might be deemed unsustainable). It might also consider risk: using uncertainty propagation, it can say _“Design A has 90% chance to meet cost target vs 70% for Design B, under market volatility.”_ Let’s say the agent picks the balanced design as recommended.
    
7. **Reporting:** The agent generates a report (in natural language with supporting tables/figures) summarizing the analysis: _“The autonomous analysis suggests that reducing enzyme usage by 20% and switching boiler fuel to biomass pellets can reduce cost by 15% while cutting GHG emissions by 25%. The expected minimum ethanol selling price is $1.30/L, with a 95% confidence interval of $1.20–1.45/L under feedstock and yield uncertainties. This design meets the sustainability target of >20% GHG reduction. Key drivers are enzyme cost and utility emissions; further R&D on enzyme efficiency could improve results. All steps and data are documented for review.”_ The user can then review this, inspect the provenance log (e.g. check if the agent considered all relevant constraints), and validate the findings.
    

If the user or agent identifies any issues (say a constraint was missed, or a scenario not considered), the process can iterate. Otherwise, the result is an optimized, thoroughly assessed process design delivered much faster and with more thorough analysis than a manual approach. This level of automation, we hypothesize, can cut down early-stage design evaluation from weeks or months of an expert’s time to mere hours of computation, thereby accelerating innovation in sustainable chemical processes.

## **4. Validation Plan and Case Studies**

We will evaluate the framework on **three case studies** spanning different sectors and challenges to demonstrate generality: (1) **PEM Hydrogen Production**, (2) **Lignocellulosic Biorefinery**, and (3) **Plastics Recycling**. These cases were chosen because they are industrially relevant, involve significant TEA and LCA considerations, and can benefit from ML acceleration due to complexity.

**Case 1: Proton Exchange Membrane (PEM) Hydrogen Production** – This case involves hydrogen generation via water electrolysis using PEM technology, a cornerstone of green hydrogen strategies. The process includes an electrolyzer unit (with catalysts, membrane), power electronics, and gas separation, with electricity as the major input. TEA issues: high capital cost of electrolyzers, electricity cost sensitivity, economies of scale. LCA issues: carbon intensity of electricity (dominant factor in GHG emissions of H₂), materials (platinum group metals in catalyst), and byproduct oxygen credit. We will set up a BioSTEAM simulation or use existing models (e.g. H2A model data) for a PEM system at a given scale (e.g. 100 MW). The agent will analyze scenarios such as different electricity sources (grid vs renewable), different load factors, and improved electrolyzer efficiency. Surrogate modeling will be valuable here to explore how efficiency and stack degradation rate affect TEA/LCA, without needing a detailed electrochemical model each time. For instance, a PINN could be trained on a mechanistic PEM cell model to quickly predict performance at various current densities and temperatures, which then feeds into the economic model. We expect surrogate-driven optimization to suggest an optimal operating current that minimizes levelized hydrogen cost while balancing efficiency (this might involve a trade-off between efficiency and capital utilization). Uncertainty analysis will include electricity price volatility and electrolyzer life uncertainty. The validation will check if our agent’s recommendations (e.g. “operate at X A/cm² using solar power for lowest emissions at acceptable cost”) align with known studies or manual analyses. We’ll measure speedup: e.g. performing an exhaustive search or Monte Carlo of 1000 scenarios in the integrated TEA-LCA might take days normally, but with our surrogates and agent coordination we aim for minutes. The outputs will be compared with DOE H₂ cost targets and literature LCA benchmarks to ensure plausibility.

**Case 2: Lignocellulosic Biorefinery (Ethanol or Bioproducts)** – This represents a complex process with biomass feed (e.g. corn stover or sugarcane bagasse) undergoing pretreatment, fermentation, and product recovery. We will likely leverage existing BioSTEAM biorefinery models (as the developers have published models for corn stover ethanol[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=scenarios%20for%20conceptual%20and%20emerging,compare%20established%20and%20early%20stage)). The TEA is complicated by many interconnected units (pretreatment reactors, enzymes, distillation, waste treatment) and significant uncertainty in yields, conversion efficiencies, enzyme costs. LCA is critical since the promise of biofuels lies in GHG reduction versus fossil fuels; factors like co-product allocation and land-use impacts must be considered. For this case, the framework will be pushed to handle a high-dimensional space: dozens of design parameters (enzyme loading, pretreatment conditions, separation reflux ratios, etc.) and similarly many outputs. The agent will focus on key drivers identified via global sensitivity (BioSTEAM’s uncertainty module can be used to sample and identify sensitive parameters). We anticipate using GP surrogates to approximate the overall process economic and environmental outcome as functions of, say, 5–10 key parameters. This will enable multi-objective optimization for minimum _MFSP_ (minimum fuel selling price) and minimum GHG emissions per MJ fuel. A specific test could be evaluating the effect of switching process configurations (like separate hydrolysis and fermentation vs. simultaneous saccharification and fermentation – a discrete choice); the agent should be able to navigate even such discrete decisions, potentially by treating configuration as categorical variables in optimization or by trying both pathways and comparing. We will validate the agent’s outputs against known results: for instance, Shi _et al._ (2020) found a certain range for sugarcane ethanol MFSP and emissions[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Demonstrated%20BioSTEAM,%E2%80%9D%20ACS)[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=projections%20of%20their%20environmental%20impacts,data%20as%20kernel%20density%20maps) – our agent’s base case should match those within tolerance. If our agent suggests a design (e.g. “increase pretreatment severity to improve yield, even if it costs more energy, because it improves downstream fermentation enough to net benefit”), we will manually verify that suggestion with targeted simulations. This case will also demonstrate the agent’s ability to maintain **reproducibility**: we will ensure that all data (feedstock compositions, LCA database entries used like fertilizer production, etc.) are documented so that an external party could replicate the analysis given the same framework. The success criterion is that the agent can find a design that significantly improves on the base case (say >10% cost reduction and >20% GHG reduction) and that it can quantify the uncertainty (perhaps the probability that the design remains economically viable if feedstock cost is 20% higher, etc.).

**Case 3: Plastics Recycling (Chemical Recycling of Waste Plastics)** – This case study addresses a different domain: the circular economy. We consider a process such as mixed plastic waste pyrolysis to produce an oil that can substitute crude in petrochemical processes. TEA factors: highly dependent on scale and feedstock cost (waste tipping fee or cost), product quality and yield (which influences revenue), and operational costs of pyrolysis and upgrading. LCA factors: credits for avoided incineration or landfill, emissions from pyrolysis (which can be energy-intensive), and benefits of offsetting virgin plastic production. We will use or build a model (possibly using BioSTEAM or another tool like Pyrolysis simulation in Python) for a plastics pyrolysis plant. The agent will explore scenarios like different reaction temperatures (affecting yield distribution of gases vs liquids vs char), addition of catalysts, or integration with downstream processes (like cracking of pyrolysis oil). This case will test the agent’s capability to handle **discrete choices and configuration**: e.g. including a hydrogenation step to upgrade oil vs selling as is, or mechanical recycling vs chemical recycling routes. We can frame this as a superstructure with options, and the agent can decide via an optimization or heuristic search which route is optimal. If this becomes combinatorially large, an RL approach (with an agent incrementally “building” the process flowsheet) could be tried, akin to Gao et al.’s approach[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the) but guided by our LLM agent at a higher level. Surrogates might be used for the pyrolysis yield as a function of temperature and residence time (since simulating the complex kinetics repeatedly is slow). We will validate results by comparing to literature: e.g. recent studies on chemical recycling TEA/LCA often find that without policy incentives, purely economic optimization might favor disposing of low-value plastic rather than recycling, whereas an environmental objective pushes toward recycling with a cost trade-off. Our agent should be able to reveal such tension and possibly suggest conditions or policies (it might incorporate a CO₂ price in TEA to internalize the LCA result). A validation metric: how well the agent’s identified Pareto front matches known trade-off curves in literature for plastic recycling vs energy recovery. Additionally, we will check that the framework can incorporate external data – for instance, if certain LCI data for plastic waste is not in our local DB, the agent should prompt for it or allow easy addition, demonstrating the _curation_ capability. We will use the OpenLCA copilot concept here: the agent might use OpenLCA’s Python interface to generate a product system for “1000 kg mixed plastic waste recycled” linking to sub-processes (collection, sorting, etc.), and ensure no double-counting or omission in LCA.

Across all case studies, we will assess the **performance** (speed, number of simulations required), **quality of results** (do the outcomes make sense, are they near-optimal compared to exhaustive search or expert expectation), and **agent robustness** (can it recover from errors like a simulation failing to converge? does it handle changes in problem definition gracefully?). We will also test partial automation modes – e.g. let the agent run in fully autonomous mode vs. a mode where it pauses for user approval at key steps – to see the impact on results and to demonstrate safe operation.

Crucially, each case study will produce a set of artifacts: a detailed report generated by the agent, the full provenance log, and comparative evaluation by us. This will allow us to showcase reproducibility and answer the question: _“Would a human analyst reach the same conclusions, and how much effort would it take them versus the agent?”_ We will gather qualitative feedback from collaborating domain experts by showing them the agent’s reports and seeing if the reasoning is convincing and if the solutions would be actionable in practice.

## **5. Work Plan and Timeline**

The project is planned for a duration of approximately 2 to 2.5 years (24–30 months), which is aggressive but feasible given the modular nature of development. Below is a Gantt chart illustrating the timeline of major tasks and milestones over this period:

![](blob:https://chatgpt.com/3227cdce-a5aa-4ddc-940b-f0551333cae5)

_Figure 2: Tentative 2.5-year PhD timeline with key tasks. Overlapping bars indicate concurrent progress on multiple fronts. Early stages focus on framework design and integration (architecture, TEA-LCA coupling), followed by surrogate modeling and agent development. Optimization (MOBO) integration and case study implementations overlap in mid-phase. Final stages emphasize evaluation, dissemination (writing) and ensuring robustness of the system. Actual timing may adjust as needed._

As shown in **Figure 2**, the work plan is divided into overlapping phases:

- **Months 0–4: Schema & Architecture Development.** In this initial phase, the focus is on designing the overall framework schema and setting up the development environment. This includes defining data structures for TEA-LCA data exchange, selecting initial tools (installing BioSTEAM, Brightway, etc.), and sketching the MCP-based agent architecture. By Month 3, we aim to have a simplified “hello world” integration where the agent can run a trivial simulation and LCA (e.g. a one-unit process) via tool calls – essentially a vertical slice of the system. Literature review and related work study will be concentrated in the first 2 months to inform design choices.
    
- **Months 3–8: TEA-LCA Integration Prototype.** Here we will implement the coupling between Biosteam and Brightway in earnest. By Month ~6, the goal is a working prototype that given a fixed process design, automatically computes TEA and LCA and aggregates results. This involves writing adapters for BioSTEAM outputs to Brightway inputs, establishing a local LCI database (e.g. importing ecoinvent data into Brightway or connecting to OpenLCA IPC server). We will test this on a simple known process (perhaps an ammonia production or a toy process) for verification. In parallel, we start defining the agent’s structure (state machine logic without LLM initially, using rule-based transitions as a placeholder).
    
- **Months 6–14: Surrogate Modeling & ML Integration.** During this period, we develop the ML toolkit. We will create modules for training GPs (likely using libraries like GPyTorch or scikit-learn), PINN models (using PyTorch/TensorFlow – possibly adapting open PINN libraries for our cases), and integrate an existing Bayesian optimizer library. We will test surrogates on known functions and also on data from BioSTEAM (for example, generate data for a unit operation and see if GP can fit it well). By Month ~10, the surrogate and optimization capability should be integrated enough that we can run a closed-loop optimization on a simple problem (e.g. optimize a reactor temperature in BioSTEAM to minimize cost, using GP+BO). Simultaneously, the LLM agent interface will be developed: choosing which LLM to use (GPT-4 via API, or an open-source model locally for flexibility), and setting up MCP tool definitions for our functions. By Month 12, the agent should be able to take a simple goal and execute a short plan (e.g. optimize one parameter) using the tools. This phase also includes developing guardrails and basic validation routines (mass balance checkers, etc.). We plan to publish a conference paper or workshop paper around Month 12 on the surrogate modeling approach for integrated TEA-LCA, as a mid-term academic output.
    
- **Months 8–16: Agent Development & Tool Orchestration.** Building on the earlier groundwork, we fully implement the agent’s planning and reasoning capabilities in this phase. This involves prompt engineering for the LLM (designing prompts that instruct it on how to use tools, give it examples of plans, etc.), integrating the MCP state logic (using the MCP Python SDK[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=quality)), and testing the agent in stepwise scenarios. We will progressively increase complexity: first one tool usage, then sequences. The **agentic features** like error handling loops and provenance tracking will be implemented here. By Month ~16, we expect to have a functional autonomous agent that can run through a simple case study end-to-end (perhaps Case 1, PEM hydrogen, at a simplified level), albeit with likely need for further refinement.
    
- **Months 14–20: Multi-Objective Optimization (MOBO) and Decision Support.** At this stage, the focus is on robust decision-making integration. We will finalize the MOBO integration so that the agent can handle dual objectives (cost, emission) optimization reliably. We also incorporate any RL components or advanced scenario generation methods here if found necessary. For example, if Case 3 (recycling) requires exploring different configurations, we might implement a hierarchical decision approach (which could be RL-based or integer optimization). By Month 18, the agent should handle multi-objective scenarios and produce Pareto analyses. This is also when we start applying the agent to **full-fledged case studies** – starting likely with Case 1 (as it is relatively contained). We anticipate iterative debugging as the agent encounters real complexities of the case. Knowledge gained here will be used to refine earlier modules (e.g. add new tool abilities, improve prompts for reasoning about trade-offs).
    
- **Months 15–24: Case Study Implementations and Validation.** In this timeframe, all three case studies will be implemented and run. We plan roughly: PEM hydrogen case around Month 15-18, Lignocellulosic biorefinery around 18-22, and Plastics recycling around 20-24. There will be overlap, and learnings from one will carry to the next. For each case, we allocate time to gather any needed external data (e.g. for LCA, ensure we have relevant background data), build any missing pieces (like a pyrolysis model for plastics if not available), and run the agent in autonomous mode to generate results. We will then spend time analyzing those results, comparing with literature or alternative methods, and possibly adjusting the agent’s strategy (for instance, we might learn that the agent needs a better strategy to search discrete options, so we implement that). By Month ~24, all case studies should be completed with results documented. This phase also includes writing up results for journal publications: we anticipate at least one comprehensive journal paper covering the framework and a couple of case studies, submitted around Month 24.
    
- **Months 24–28: Evaluation, Uncertainty & Reproducibility Analysis.** In the final stretch, we will rigorously evaluate the system’s performance. We will run extensive Monte Carlo tests to assess how well the agent handles uncertainties (e.g. run the agent multiple times with different random seeds or perturbed data to see consistency). We will finalize the uncertainty propagation features if not fully utilized earlier. Also, an external validation may be done by giving the tool to a colleague to see if they can reproduce our case study results easily (testing the documentation/provenance aspect). Any remaining ethical/safety tests (like ensuring the agent doesn’t produce any unsafe recommendations without flagging, etc.) will be done now. We will compile the insights into the dissertation draft.
    
- **Months 28–30: Documentation and Dissertation Writing.** The last two months focus on writing the PhD dissertation and associated defense preparation. By this point, we expect to have multiple publications to incorporate. The documentation of the software (open-source release on a platform like GitHub) will be finalized so that others can use the framework. The timeline includes buffer for revision and addressing unforeseen challenges (for example, if an LLM API changes or if a case study requires more time due to data issues).
    

The Gantt chart (Figure 2) summarizes this schedule, showing overlapping tasks which reflect the iterative and modular development approach. Notably, many tasks run in parallel (e.g. agent development overlaps with surrogate modeling) – this is intentional, as progress in one area (like ML) can feed back to another (like agent logic) continuously.

By adhering to this timeline, we aim to complete the core research within 2 years and use the additional 0.5 year for polishing and final write-up, aligning with the goal of a timely PhD completion at a top technical university.

## **6. Expected Contributions**

This research will yield contributions across theoretical, methodological, technological, and practical dimensions:

- **Theoretical Contributions:** We will advance the theory of **integrated sustainable process design** by formulating a unified problem space that combines TEA and LCA as a multi-objective optimization under uncertainty. This includes new formulations of objective functions that blend economic and environmental criteria, and potentially new theoretical insights into how AI planning can be combined with engineering optimization. For instance, our use of LLMs for constrained planning in engineering can contribute to the theory of _AI planning with domain knowledge_ (showing how chemical engineering knowledge can be embedded in prompts or tool use). We also anticipate contributing to reinforcement learning theory for process synthesis by demonstrating how transfer learning or hierarchical RL can overcome some complexity barriers[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=RL%20agent%20demands%20numerous%20process,process%20design%20and%20apply%20it).
    
- **Methodological Contributions:** The project will develop a **novel methodology for autonomous process systems analysis**. This methodology outlines how to orchestrate simulations, data-driven models, and AI reasoning in a closed loop. The state-machine agent approach (inspired by MCP) applied to engineering analysis is a methodological innovation. We will document the design patterns for ensuring consistency between TEA and LCA models (essentially a methodology for TEA-LCA coupling that others can replicate, using open-source tools). Another key contribution is the methodology for **surrogate-based multi-objective optimization with an AI agent in the loop** – our approach to have the agent intelligently gather data and refine models on the fly can be applied to other domains as well. Additionally, we’ll propose best practices for **ensuring provenance and transparency** in AI-driven research workflows in engineering, addressing a current gap in methodology for explainable and auditable AI in process engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=together%20with%20state,solving%20bottleneck%20challenges%20in%20CE).
    
- **Technological/Software Contributions:** We will deliver a working **software framework (prototype)** that implements the proposed system. This will likely be in the form of a Python-based platform (integrating BioSTEAM, Brightway/OpenLCA, and our agent code) that can be used and extended by others. It essentially acts as a _“Digital Copilot”_ for process engineers. We plan to open-source the code under a permissive license (e.g. MIT or BSD) to align with open science principles – addressing the noted issue that only ~20% of automation studies share code[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=the%20models%20was%20reached,development%20of%20new%20automation%20methodologies). This software will include modules for: connecting to simulation engines, performing LCA calculations from Python, training ML models, and the LLM agent control logic. Even beyond our specific use cases, these modules themselves are valuable – e.g. a generic Biosteam–Brightway linkage package, or an MCP tool library for engineering tasks. The technology also includes any new PINNs or surrogate models tailored to certain processes (like a PINN for pyrolysis kinetics); these models can be the basis for future research or could be integrated into process simulators. We will also contribute to data by curating any process/LCA datasets needed for the case studies and making them available (respecting licensing, e.g. using open LCA databases or providing dummy data if needed).
    
- **Practical/Empirical Contributions:** Through the case studies, we will produce **practical insights for each domain** (H₂ production, biorefineries, plastic recycling). For example, we might uncover a promising operating regime for PEM electrolyzers or identify the conditions under which chemical recycling becomes environmentally favorable. These findings can guide industry or policy. More broadly, the demonstration that an autonomous framework can do in-depth TEA-LCA in a short time can influence how feasibility studies are conducted in practice – potentially shifting some of the workload from consultants and analysts to AI assistants, thereby reducing cost and time. The framework can serve as a training tool for new engineers – they could use it to explore scenarios quickly and learn which factors matter, augmenting human expertise. We expect to show that our approach can achieve comparable or better results than a traditional analysis done by an expert, thus proving the practicality of AI augmentation in process design. If adopted, this could accelerate development of sustainable technologies by enabling rapid iterative assessment of concepts (failing fast on non-viable ideas and honing in on viable ones). Our work will also set a **benchmark** for future AI-in-the-loop PSA – as one of the first comprehensive examples, it will be something others can improve upon.
    

In summary, the project aims to contribute a new paradigm of autonomous, AI-driven process analysis that merges the strengths of chemical engineering models with modern AI. We anticipate at least 2–3 publications in high-impact journals out of this (e.g. in sustainable engineering, process systems engineering, or AI for science venues), and the software release will amplify its impact by allowing others to build upon it. Ultimately, the contributions can be seen as a step towards the broader vision of a “self-driving laboratory” for process design – where AI agents can design, simulate, evaluate, and learn in an endless loop, constrained by human-defined goals and ethics.

## **7. Ethical Considerations**

Developing an autonomous decision-making system for engineering raises important ethical and responsible innovation issues. We address these proactively in our framework design:

- **Transparency and Explainability:** A known concern is the _“opacity of AI algorithms”_ which can impede trust in critical domains like chemical engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=The%20issue%20of%20opacity%20within,the%20mechanistic). We tackle this by ensuring that every decision by the agent is traceable. The provenance log will record not just what the agent did, but why (e.g., it will include intermediate reasoning output from the LLM where possible, and links between outcomes and the data that informed them). We will use techniques from explainable AI (XAI), such as feature importance from surrogate models to explain which factors most influenced the cost or emission outcomes. For any recommendation given to a human, the agent will accompany it with a rationale (in plain language) and reference data (like “Catalyst cost was high, so system chose an alternative catalyst which lowered cost by 15%[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=benchmark%20designs%20modeled%20in%20proprietary,prioritize%20research%2C%20development%2C%20and%20deployment)”). This practice aligns with calls for _transparency as a vital link_ to trustworthy AI in engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=together%20with%20state,solving%20bottleneck%20challenges%20in%20CE). We will evaluate the explainability by having domain experts review the agent’s reports for clarity. If needed, we might incorporate a simplified surrogate (like a decision tree) purely for explanation of a complex model’s decision.
    
- **Human Oversight and Control:** The framework is designed as a **support system** for human engineers, not a fully autonomous authority. Human oversight is built in at multiple levels. The user defines initial goals and constraints – the agent is bound to these. If the agent encounters a decision beyond its scope (e.g. a trade-off involving value judgments, such as trading cost for safety), it will flag it for human input. We will implement a mechanism where the agent can explicitly ask for confirmation or guidance if it is unsure or if the next step involves a potential risk (for example: “The optimal solution involves using a toxic solvent – proceed? [Yes/No]”). This ensures critical decisions are not made in a vacuum. During development, we will test failure modes of the agent and ensure it fails safe – e.g., if the LLM outputs an erroneous plan, the validator should catch it and either correct or seek help, rather than proceeding blindly. Ultimately, a human will review the final recommendations, and the system will be advertised as an **assistant** that improves human productivity, not a replacement for human judgment on safety or ethical trade-offs.
    
- **Data and Model Bias, Validity:** Since the agent relies on both data (for ML models) and an LLM (with its training biases), we must guard against any biases or invalid assumptions propagating. LCA data can sometimes be region-specific or outdated; the agent will make data provenance clear so that, for example, if it used a European dataset for electricity by default, a user in the US can know and replace it if needed. The LLM might have biases or hallucinations – to prevent it from generating false technical information, we constrain its actions to using the tools for any quantitative answers. Essentially, all numerical results must come from trusted computations (simulation, database, or ML model) rather than the LLM’s internal knowledge. The LLM’s role is more on planning and interpretation, which we will supervise by testing it on known problems. If it provides a misleading explanation, we will refine the prompting or add rule-based checks (for instance, cross-check any qualitative assertion it makes about chemistry or economics with known references). The MCP framework’s guardrails also help, as the agent cannot take an action that isn’t defined – limiting scope for LLM going off-track.
    
- **License and Intellectual Property Considerations:** We commit to open-sourcing the developed framework, which means carefully handling any third-party components’ licenses. All tools chosen (BioSTEAM, Brightway, OpenLCA API, etc.) are open-source, which avoids legal barriers. LCA databases, however, often have licenses (ecoinvent, for example, is not free for commercial use). We will use open databases (like the Open Life Cycle Database or USEEIO) for demonstration, and ensure the framework can plug in licensed data only when the user has obtained it (the agent will not distribute any proprietary data itself – it will just access what’s on the user’s system). The LLM used may have usage restrictions (if using a service like OpenAI GPT-4); we will abide by those and in documentation suggest appropriate models for different user needs (for an industry user concerned about IP, they might use an on-premises LLM instance). By keeping everything modular, one can swap out components to suit their licensing requirements (e.g. use Aspen Plus via a Python COM interface instead of BioSTEAM if they have a license – our agent could accommodate that with a different tool plugin).
    
- **Safety and Reliability:** While our agent deals with simulations (not directly running physical experiments), recommendations it gives could indirectly influence real-world decisions. We thus treat safety seriously. For example, if optimizing a reactor temperature, the agent will be programmed to respect known safety limits (it will have constraints like not exceeding known thermal stability limits of materials, etc., drawn from literature or user input). The agent’s error handling ensures that if a simulation returns an physically implausible result, that path is not pursued without investigation. We will incorporate a simple rule-base of “sanity checks” (e.g. yields cannot exceed 100%, costs cannot be negative, etc.) which the agent will use to validate outputs. If any such check fails, it triggers a review or requires human sign-off to continue. By providing not just what to do but why, the system encourages humans to think critically about the results, serving as a second set of eyes rather than an oracle.
    
- **Environmental and Social Impact:** The very purpose of this framework is to promote sustainable process design by integrating environmental criteria. Ethically, this aligns with sustainability principles – we are making it easier for engineers to include environmental impacts in decision-making, which may have positive societal impact (better tech decisions for climate). However, one must be cautious that the system doesn’t get misused (for instance, someone could in principle optimize purely for profit and ignore environment if they disable that objective). While we cannot enforce users’ motives, our default stance is to always include sustainability metrics and to highlight them. We will also ensure that the agent’s output includes discussion of any ethical/environmental implications it can infer (e.g. if the optimal solution in Case 3 relies on incineration, the agent should note the trade-off in environmental harm). The LLM’s training might allow it to make such connections (especially if fine-tuned on sustainability texts).
    

In conclusion, by embedding transparency, oversight, and clear ethical guidelines into the framework, we aim to create a responsible AI tool that augments human decision-making in chemical process development. This not only mitigates risks but also helps build trust in the approach, addressing the _“missing link”_ of transparency that is essential for AI adoption in chemical engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=matter%20in%20a%20way%20that,solving%20bottleneck%20challenges%20in%20CE). Through rigorous testing and an open development process, we will adhere to high standards of research integrity and ethical tech deployment.

## **8. Conclusion**

This proposal outlined a comprehensive plan to develop an autonomous, agent-driven framework for process systems analysis that integrates techno-economic analysis and life cycle assessment with advanced machine learning and optimization. By tackling current limitations – the siloed nature of TEA/LCA, the high expertise and time demands, and the underutilization of AI in the workflow – the proposed research aims to revolutionize how sustainable chemical process design is conducted. The end result will be a proof-of-concept system (validated on real case studies) showing that an AI agent, guided by domain knowledge and empowered by ML surrogates, can perform end-to-end analysis and optimization, providing human engineers with insightful recommendations while ensuring consistency, speed, and transparency.

The success of this project could mark an inflection point in process engineering methodology, demonstrating the viability of AI “co-pilots” in engineering design. It aligns with the goals of chemical engineering to accelerate innovation and with global needs for rapid development of economically and environmentally sound technologies. Moreover, the interdisciplinary knowledge generated – spanning chemical process modeling, machine learning, and AI orchestration – will contribute to the emerging field of AI for Science and Engineering.

By executing the research plan and achieving the stated objectives, this PhD will produce both significant academic contributions and a practical tool with potential for real-world impact. We anticipate that this work will not only address the immediate research gap but also pave the way for future studies (e.g. scaling to more complex processes, integrating real-time plant data for autonomous operations, etc.). In the long run, such agentic frameworks could be extended to create self-driving laboratories or digital twins that continuously learn and advise, ultimately merging with experimental efforts for closed-loop innovation.

**References:** (Selected relevant references are cited in IEEE style throughout this proposal, for brevity we have embedded them in context. Key sources include: Shi et al. 2020[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=1%20Shi%2C%20et%20al,9b07040); Cortes-Peña et al. 2020[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=and%20Engineering,9b07040) on BioSTEAM and LCA integration; Köck et al. 2023[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=life%20cycle%20inventory%20%28LCI%29%20phase,highest%20level%20of%20maturity%20of)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=the%20models%20was%20reached,development%20of%20new%20automation%20methodologies) on LCA automation; Vyas & Mercangoz 2025[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=,loop%20iteratively%20refines%20invalid%20plans)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=a%20laboratory%20TCLab%20platform%20,driven%20automation%20in%20chemical%20engineering) on LLM agent for control; Liu et al. 2024[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=,4o)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=Claude,local%20attention%20and%20global%20oversight) on autonomous simulation agent; Gao et al. 2023[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=established%20approach%20from%20machine%20learning,by%20a%20factor%20of%202) on RL for process design; and Yuan et al. 2024[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=together%20with%20state,solving%20bottleneck%20challenges%20in%20CE) on transparency in AI for chemical engineering. Full bibliographic details are available in the project repository and documentation.)


Machine Learning for process systems analysis
copilot/Agentic assisted framework for systems analysis.
systems analysis: Technoeconomic analysis and Life cycle assessment 

MCP feature for tool calling
LLM enabled interactions
Python background
Predictive tools


Opportunities , assests and tools useful
OpenLCA copilot for GUI copilot
Biosteam: library for TEA
Brightway python library for LCA

SWOM
Strength



- [ ] a formal 8–12 page proposal (with figures, a Gantt timeline, and a short related-work section)


## Draft 1

Agentic Machine Learning for Integrated Techno-Economic and Life-Cycle Systems Analysis

## Abstract

Process systems analysis (PSA) underpins technology assessment, investment decisions, and policy design, yet it is slow, expert-intensive, and difficult to reproduce. This PhD proposes an **agentic, LLM-enabled co-pilot** that orchestrates open-source PSA tools through the **Model Context Protocol (MCP)** and Python toolchains to deliver **techno-economic analysis (TEA)** and **life-cycle assessment (LCA)** with rigorous uncertainty, interpretability, and human-in-the-loop governance.
Case studies in hydrogen (PEM), biorefineries, and plastics recycling demonstrate: (i) reliable cost/impact estimates with quantified uncertainty; (ii) design space exploration for cost–carbon–water trade-offs; and (iii) an auditable conversational interface that shortens expert workflows and improves reproducibility.
Deliverables include open protocols for TEA–LCA coupling, benchmark datasets, an MCP tool suite, and validated ML models for robust decision support.

## 1) Motivation & Research Gap

- **Fragmented workflows:** TEA and LCA are often performed in separate tools with manual data handoffs, leading to mismatches in system boundaries, functional units, and cut-off rules.
- **Slow iteration:** Rigorous flowsheet simulation and LCI compilation are compute- and labor-intensive; sensitivity and Monte Carlo propagate this cost.
- **Limited accessibility:** Expert knowledge (allocation, background database choices, coproduct handling) is tacit; documentation and provenance are inconsistent.
- **Emerging opportunity:** **Agentic LLMs** can plan tasks, enforce schemas, call tools, and keep audit trails; **MCP** standardizes tool calling; **surrogates** and **active learning** can replace many expensive simulations while preserving fidelity and uncertainty.
## 2) Objectives & Hypotheses

**O1.** Build an **agentic co-pilot** that plans, executes, and explains PSA tasks across TEA and LCA tools using MCP.
**O2.** Develop **physics-aware predictive models** (e.g., GPs, PINNs, and gradient-boosted trees) as surrogates for flowsheet/calculator evaluations with calibrated uncertainty.
**O3.** Create a **robust TEA–LCA coupling layer** that harmonizes system boundaries, functional units, and allocation while retaining provenance for auditability.
**O4.** Demonstrate **uncertainty-aware, multi-objective optimization** of cost, GHG, water, and energy; quantify robustness under data and model uncertainty.
**O5.** Evaluate usability and reproducibility: measure time-to-result, error rates, and decision quality with and without the co-pilot.

**H1.** An MCP-orchestrated agent can **reduce expert time** to complete end-to-end TEA+LCA by ≥50% while maintaining methodological correctness.  
**H2.** Surrogates trained via active learning can achieve **≤5–10% error** on key metrics (e.g., MSP, GWP100) at **10–100× speedup**.  
**H3.** Provenance-first, schema-enforced workflows increase **reproducibility** and reduce methodological inconsistencies across studies.



## 5) Case Studies

1. **PEM Hydrogen Production**
    - Design variables: current density, pressure, temperature, membrane thickness, stack sizing, BoP configurations, electricity mixes.
    - Outputs: LCOH/MSP; GWP100; sensitivity to grid CI, stack life, membrane cost; **robust design** under market volatility.
        
2. **Lignocellulosic Biorefinery (BioSTEAM)**
    - Feedstock blends, enzyme loadings, distillation/fermentation choices; coproduct crediting vs allocation.
    - TEA–LCA coupling to compare product slates (ethanol, SAF blendstocks, biochemicals).
        
3. **Plastics Recycling & Waste-to-Energy Interactions**
    - MRF configurations, mechanical vs chemical recycling, WTE synergies; EPR and recycled content scenarios.
    - Policy scenarios: carbon price, EPR fees; **marginal abatement cost** curves.



----


## 1. Introduction and Problem Statement

### 1.1 Current Challenges in Process Systems Analysis

Process systems analysis, encompassing technoeconomic analysis and life cycle assessment, represents a critical bottleneck in sustainable process development. Current methodologies suffer from several limitations:

- **Expertise Barriers**: TEA and LCA require specialized domain knowledge, limiting accessibility to non-experts
- **Time Intensity**: Manual analysis processes can take weeks to months for comprehensive assessments
- **Inconsistency**: Human analysts introduce variability in methodological choices and interpretation
- **Integration Complexity**: Coupling TEA and LCA requires navigating disparate software ecosystems and data formats
- **Limited Predictive Capability**: Traditional approaches are reactive rather than predictive, limiting proactive optimization

### 1.2 Emerging Opportunities

Recent advances in machine learning, particularly large language models and agentic AI systems, present unprecedented opportunities to address these challenges. The convergence of several technological trends creates a unique research opportunity:

- **LLM Reasoning Capabilities**: Modern LLMs demonstrate sophisticated reasoning about technical domains
- **Tool Integration Protocols**: MCP enables seamless integration of specialized analytical tools
- **Mature Python Ecosystem**: Libraries like Biosteam and Brightway provide robust computational foundations
- **Agentic AI Frameworks**: Autonomous agents can orchestrate complex analytical workflows


## 2. Literature Review and Research Gap

### 2.1 Current State of Process Systems Analysis

Traditional process systems analysis relies heavily on specialized software tools and expert knowledge. Biosteam has emerged as a leading platform for biorefinery TEA, offering comprehensive process modeling capabilities. Similarly, Brightway provides LCA computational frameworks with extensive database integration capabilities. However, these tools require significant expertise and manual configuration.

Recent work in automated process analysis has focused primarily on optimization algorithms and sensitivity analysis. Limited research has explored the integration of natural language interfaces with specialized analytical tools for autonomous systems analysis.

### 2.2 Machine Learning in Process Engineering

Machine learning applications in process engineering have primarily focused on process control, optimization, and predictive maintenance. Emerging research explores LLM applications in chemical engineering, but comprehensive frameworks for autonomous systems analysis remain unexplored.

### 2.3 Research Gap

The literature reveals a significant gap in intelligent, autonomous frameworks capable of conducting end-to-end process systems analysis. No existing work combines LLM reasoning capabilities with specialized TEA/LCA tools through standardized protocols like MCP to create truly autonomous analytical copilots.


## 3. Research Objectives

### 3.1 Primary Objective

Develop and validate an intelligent, machine learning-enabled agentic framework that autonomously conducts comprehensive process systems analysis, integrating technoeconomic assessment and life cycle analysis through natural language interfaces and tool orchestration.

### 3.2 Specific Objectives

1. **Framework Architecture**: Design a modular agentic architecture integrating LLMs with specialized analytical tools via MCP
2. **Tool Integration**: Develop MCP-compliant interfaces for Biosteam, Brightway, and complementary analytical tools
3. **Intelligent Reasoning**: Implement machine learning models for automated methodological decision-making in TEA/LCA
4. **Predictive Analytics**: Create ML models for performance prediction and optimization recommendation
5. **Validation Framework**: Establish comprehensive validation methodologies for autonomous analytical results
6. **User Interface Development**: Design intuitive natural language interfaces for non-expert users


## 4. Methodology

### 4.1 Framework Architecture

The proposed framework adopts a multi-agent architecture with specialized agents for different analytical domains:

**Core Components:**

- **Orchestrator Agent**: Central coordination and workflow management
- **TEA Agent**: Specialized in technoeconomic analysis using Biosteam
- **LCA Agent**: Focused on life cycle assessment using Brightway
- **Integration Agent**: Manages data flow and result synthesis
- **Validation Agent**: Ensures result quality and methodological consistency

### 4.2 Technical Implementation Strategy

#### Phase 1: Foundation Development (Months 1-12)
- Develop MCP interfaces for Biosteam and Brightway integration
- Create basic agentic framework architecture
- Implement core LLM reasoning capabilities for process analysis
- Establish data management and workflow orchestration systems

#### Phase 2: Intelligence Enhancement (Months 13-24)
- Develop machine learning models for automated parameter selection
- Implement predictive analytics for process performance estimation
- Create natural language processing capabilities for requirement interpretation
- Build automated methodology selection algorithms

#### Phase 3: Integration and Optimization (Months 25-36)
- Integrate TEA and LCA capabilities into unified analytical workflows
- Develop GUI copilot interfaces for OpenLCA integration
- Implement advanced optimization and sensitivity analysis capabilities
- Create comprehensive validation and uncertainty quantification frameworks

#### Phase 4: Validation and Deployment (Months 37-48)
- Conduct extensive validation studies across multiple process domains
- Develop user studies and usability assessments
- Create deployment frameworks and documentation
- Establish performance benchmarks and comparative analyses



### 4.3 Machine Learning Components

#### 4.3.1 Predictive Models
- **Parameter Estimation**: Automated estimation of missing process parameters using similarity-based learning, data fetching through api, physic infromed design
- **Uncertainty Quantification**: Bayesian approaches for quantifying analytical uncertainty

#### 4.3.2 Decision Support Systems

- **Scenario Generation**: Generative models for comprehensive scenario analysis
- **Optimization Guidance**: Reinforcement learning for process optimization recommendations


### 4.4 Tool Integration Strategy

#### 4.4.1 MCP Implementation

Develop comprehensive MCP servers for:

- **Biosteam Integration**: Complete process modeling and TEA capabilities
- **Brightway Integration**: LCA computation and database access
- **Data Management**: Standardized data exchange protocols
- **Visualization**: Automated result visualization and reporting

#### 4.4.2 Python Ecosystem Leverage

Utilize the rich Python ecosystem for:

- **Scientific Computing**: NumPy, SciPy for numerical analysis
- **Machine Learning**: Scikit-learn, TensorFlow for predictive modeling
- **Data Processing**: Pandas for data manipulation and analysis
- **Visualization**: Matplotlib, Plotly for result presentation


## 5. Expected Contributions and Novelty

### 5.1 Theoretical Contributions
- **Agentic Framework Theory**: Novel architectural principles for autonomous analytical systems
- **ML-TEA/LCA Integration**: Theoretical foundations for machine learning integration in systems analysis
- **Uncertainty Propagation**: Advanced methodologies for uncertainty quantification in automated analysis
### 5.2 Methodological Contributions
- **Automated Methodology Selection**: Algorithms for optimal analytical approach selection
- **Intelligent Parameter Estimation**: ML-based approaches for missing data imputation
- **Integrated Analysis Workflows**: Seamless TEA-LCA integration methodologies
### 5.3 Technological Contributions
- **MCP-Based Tool Integration**: Standardized protocols for analytical tool orchestration
- **Natural Language Interfaces**: Intuitive interaction paradigms for complex analytical tools
- **Autonomous Validation**: Self-validating analytical frameworks
### 5.4 Practical Contributions
- **Democratized Access**: Making advanced systems analysis accessible to non-experts
- **Accelerated Analysis**: Reducing analysis time from weeks to hours
- **Improved Consistency**: Standardized analytical approaches reducing human variability


## 6. Research Plan and Timeline

### Year 1: Foundation and Architecture
- **Q1-Q2**: Literature review completion, framework design
- **Q3-Q4**: Core architecture implementation, basic MCP integration
### Year 2: Intelligence and Integration
- **Q1-Q2**: ML model development, predictive analytics implementation
- **Q3-Q4**: Advanced tool integration, natural language interface development
### Year 3: Optimization and Validation
- **Q1-Q2**: Comprehensive framework integration, optimization implementation
- **Q3-Q4**: Extensive validation studies, performance benchmarking
### Year 4: Refinement and Dissemination
- **Q1-Q2**: Framework refinement based on validation results
- **Q3-Q4**: Thesis writing, publication preparation, deployment preparation
## 7. Resources and Infrastructure Requirements

### 7.1 Computational Resources
- High-performance computing access for ML model training
- Cloud infrastructure for scalable deployment and testing
- GPU resources for deep learning model development
### 7.2 Software and Tools
- Python development environment with specialized libraries
- Access to commercial LCA databases through Brightway
- Development licenses for complementary analytical tools

### 7.3 Data Requirements
- Historical process data for ML model training
- Validated TEA/LCA case studies for framework validation
- Expert annotations for methodology validation


Citation
Cortes-Peña, Y., Guest, J. S. Jan. 30, 2020. “BioSTEAM: The Biorefinery Simulation and Techno-Economic Analysis Modules.”

Opportunities
Closed sourced
 open-source LLMs in both

Metrics
accuracy and latency.


-----

# **PhD Research Proposal:** An Agentic Machine-Learning Framework for Autonomous Process Systems Analysis Integrating Techno-Economic Analysis and Life Cycle Assessment

## **1. Introduction and Problem Statement**

Process Systems Analysis (PSA) is critical in chemical engineering for evaluating the viability and sustainability of process designs through techniques like techno-economic analysis (TEA) and life cycle assessment (LCA). TEA quantifies capital and operating costs, economics, and techno-feasibility, while LCA quantifies environmental impacts across a product’s life cycle[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20an%20open,outputs%20over%20its%20life%20cycle). Despite their importance, current PSA practices face significant challenges. Performing a rigorous LCA demands _“a great deal of time and data”_, which often limits its application during early process development[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=%28evaluation%20of%20results%29%20,and%20can%20improve%20data%20quality). TEA and LCA are typically carried out by different domain experts using separate tools (e.g. process simulators for TEA, LCA software for environmental analysis), leading to **expertise bottlenecks** and **integration complexity**. Indeed, LCA methodologies have historically been _“disconnected from biorefinery design, simulation, and techno-economic analysis”_, hampering accurate sustainability projections[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=Objective%20Biorefineries%20will%20play%20a,technology%20performance%2C%20and%20contextual%20landscape). This disconnect means engineers must manually reconcile results from disparate TEA and LCA analyses – a time-consuming and error-prone endeavor requiring multidisciplinary expertise.

Recent reviews highlight a pressing need for automation and integration in PSA. Köck _et al._ (2023) found that automating data flow between process simulations and LCA can _“make integration of environmental impacts into decisions easier, less time-consuming, and improve data quality”_[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=%28evaluation%20of%20results%29%20,and%20can%20improve%20data%20quality)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=reviews%20current%20developments%20in%20the,and%20can%20improve%20data%20quality). However, existing efforts at PSA automation remain limited. Automation has been implemented mostly via custom interfaces linking simulators to LCA databases, or via machine learning (e.g. neural networks for predicting molecular properties)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=engineering,created%20resources%20was%20only%20followed). Fewer than one-third of published methods share open code, and only 10 of 30 surveyed papers included uncertainty analysis[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=the%20models%20was%20reached,development%20of%20new%20automation%20methodologies) – indicating that current solutions are often proprietary, not reproducible, and may neglect uncertainty quantification. In summary, **the problem is that current PSA practices are labor-intensive, siloed, and not sufficiently leveraging modern AI/ML tools, resulting in a bottleneck for rapid sustainable process innovation**.

**Research Gap:** No existing framework unifies TEA, LCA, large language models (LLMs), and predictive machine learning in an _autonomous, agentic system_ for process analysis. Tools like **BioSTEAM** (an open-source process simulation and TEA platform) and its LCA extension have demonstrated the value of tighter integration – e.g. BioSTEAM-LCA automates early-stage TEA-LCA for biorefineries, enabling simultaneous economic and environmental evaluation under uncertainty[cabbi.bio](https://cabbi.bio/datasets/biosteam-and-biosteam-lca/#:~:text=Additionally%2C%20an%20agile%20life%20cycle,performance%2C%20economics%2C%20and%20environmental%20impacts)[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=projections%20of%20their%20environmental%20impacts,technology%20performance%2C%20and%20contextual%20landscape). BioSTEAM integrates uncertainty into simulations to _“streamline and automate early-stage technology evaluations”_, providing rigorous sensitivity analyses[cabbi.bio](https://cabbi.bio/datasets/biosteam-and-biosteam-lca/#:~:text=BioSTEAM%20is%20a%20fast%20and,platform%2C%20BioSTEAM%20aims%20to%20foster). It _“enables the integration of design, simulation, TEA and LCA to improve consistency and transparency”_ of sustainability metrics[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=,). Yet, even these advanced tools do not incorporate the **agentic intelligence** now possible with AI. Large Language Models have recently shown remarkable ability to plan and execute complex tasks autonomously in scientific workflows[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=,4o)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=Claude,local%20attention%20and%20global%20oversight). In chemical engineering, early studies have harnessed LLM-based agents for process control and simulation: e.g. Vyas and Mercangoz (2025) demonstrated an LLM-driven agent that plans fault recovery steps and controls a chemical process, outperforming classical methods in complex scenarios[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=,loop%20iteratively%20refines%20invalid%20plans)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=a%20laboratory%20TCLab%20platform%20,driven%20automation%20in%20chemical%20engineering). Liu _et al._ (2024) developed an autonomous simulation agent using GPT-4 that can iterate through simulation experiments and analyses with minimal human input[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=experimental%20processes%20and%20computational%20simulations,flawless)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=execution%20on%20designated%20research%20missions%2C,local%20attention%20and%20global%20oversight). These works hint at a new paradigm where AI agents handle both _symbolic reasoning_ (e.g. planning experimental or analytical steps) and _numeric computation_ (running simulations or controllers). **However, such agentic frameworks have not been applied to end-to-end process systems analysis combining TEA and LCA.** Moreover, incorporating predictive ML models (e.g. surrogates for expensive simulations) and decision-support (e.g. scenario generation, optimization) into the loop remains an open challenge. This proposal addresses that gap by developing an _agentic, machine-learning-enabled framework for autonomous PSA_, in which an AI agent orchestrates process simulations, economic calculations, environmental assessments, and optimization routines in an integrated manner. We aim to overcome current bottlenecks by automating expertise-intensive tasks, ensuring TEA-LCA consistency, and greatly accelerating design-space exploration.

In the following, we outline the proposed framework and its technical architecture (Section 2), the research objectives and methodology (Section 3), case study plans for validation (Section 4), a timeline for completion (Section 5), expected contributions (Section 6), and ethical considerations (Section 7). A brief review of related work is also included (Section 2.1) to position this research in the context of recent advances in ML-for-PSA and automation.

## **2. Background and Related Work**

**2.1 Integrated TEA-LCA in Process Analysis:** Traditional approaches treat TEA and LCA separately, often leading to misaligned assumptions. Recent efforts have integrated these analyses to provide unified insights. The BioSTEAM platform and its LCA extension (BioSTEAM-LCA) are notable examples[cabbi.bio](https://cabbi.bio/datasets/biosteam-and-biosteam-lca/#:~:text=Additionally%2C%20an%20agile%20life%20cycle,performance%2C%20economics%2C%20and%20environmental%20impacts). BioSTEAM is an open-source steady-state process simulator enabling design, simulation, and TEA under uncertainty[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=BioSTEAM%20%E2%80%93%20the%20Biorefinery%20Simulation,evaluated%20in%20BioSTEAM%20closely%20match). It automates unit operation sizing, computes capital and operating costs, and can generate data needed for LCA (e.g. waste streams, resource consumption)[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=benchmark%20designs%20modeled%20in%20proprietary,prioritize%20research%2C%20development%2C%20and%20deployment). Shi _et al._ (2020) layered an LCA module onto BioSTEAM, creating an agile TEA-LCA tool that characterizes environmental impacts alongside economics across thousands of scenarios[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=projections%20of%20their%20environmental%20impacts,data%20as%20kernel%20density%20maps)[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs). Using this, they showed that evaluating _1000 process design scenarios took only ~5 minutes_ on a PC[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs), highlighting the power of automation and surrogate-assisted computation in PSA. The integrated framework also enabled global sensitivity analysis to identify key cost and emissions drivers[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs). Other researchers have emphasized the need for such integrated analyses. A review by Yuan _et al._ (2024) notes that combining _“process performance, economics, and environmental impacts”_ in a transparent way is crucial for responsible AI in chemical engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=which%20is%20defined%20based%20on,as%20an%20example%20to%20enhance)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=matter%20in%20a%20way%20that,solving%20bottleneck%20challenges%20in%20CE). Our framework builds on these successes, using BioSTEAM (or similar) for core process simulation and TEA, and Brightway or OpenLCA for LCA, but extends them with an autonomous AI agent and advanced ML components.

**2.2 Machine Learning and Surrogate Modeling in PSA:** Machine learning has been applied to expedite process simulations and optimize designs. Gaussian Process (GP) regression is widely used to create surrogate models of expensive simulation outputs, providing fast predictions with uncertainty estimates – a cornerstone of Bayesian optimization. Physics-Informed Neural Networks (PINNs) have emerged as a way to embed physical laws into neural network models, enabling surrogates that respect conservation laws or known kinetics, thus improving extrapolative reliability. Gradient boosting machines (GBMs) and other ensemble methods are also popular for their efficiency in approximating complex relationships with modest data. In process engineering, such surrogates have proven valuable. For example, GP and neural network surrogates have been used to approximate flowsheet performance, allowing rapid evaluation in optimization algorithms (e.g. in biorefinery superstructure optimizations[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=,refinery%20structure.)). Surrogates can also facilitate uncertainty propagation by enabling Monte Carlo simulations that would be prohibitively slow with full models. Despite these benefits, integrating surrogate modeling into a general PSA workflow is non-trivial. Automated collection of training data from process simulators and continuous retraining as new scenarios are explored require coordination – a role suited for an intelligent agent. Our proposed system incorporates GP surrogates for unit process models (e.g. predicting yield or energy use as a function of operating conditions), PINNs for processes where first-principles are known (e.g. reactor kinetics), and GBMs for quick predictive tasks (such as mapping feedstock properties to process performance). These models will be orchestrated by the agent to speed up scenario evaluations and guide optimization, as detailed in Section 3.

**2.3 Autonomous Agents and LLMs in Chemical Engineering:** The advent of large language models has unlocked new paradigms for automation. LLM-based agents can interpret high-level instructions, generate plans, call software tools, and self-correct based on feedback. In chemical engineering and related fields, this concept is only beginning to be explored. Vyas & Mercangoz (2025) introduced an _“agentic framework for industrial automation”_ using LLMs for both discrete planning and continuous control[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=,loop%20iteratively%20refines%20invalid%20plans). Their system uses a finite-state-machine guided approach: an LLM planner proposes a sequence of actions to handle a process fault, a simulation agent executes each action, and a validator loop checks outcomes – iterating until a valid solution is found[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=introduce%20a%20unified%20agentic%20framework,source%20LLMs%20in%20both%20accuracy)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=recovery%20sequences%20through%20the%20FSM%2C,attains%20similar%20performance%2C%20while%20ablation). Impressively, GPT-4-based agents in this framework achieved 100% success in finding valid recovery plans across hundreds of trials, and even maintained control of a physical twin column with performance comparable to a tuned PID controller[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=In%20Case%20Study%201%2C%20across,following%20lapses%20and%20coarse%20ODE)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=a%20laboratory%20TCLab%20platform%20,driven%20automation%20in%20chemical%20engineering). These results demonstrate that LLM agents can _“unify high-level symbolic planning and low-level control”_ in chemical processes[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,driven%20automation%20in%20chemical%20engineering). Another study by Zhihan Liu _et al._ (2024) developed an Autonomous Simulation Agent (ASA) that _“automates the entire simulation research process”_ using LLM prompt engineering[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=,4o). Given a research goal, their ASA designs experiments, runs simulations on remote servers, analyzes data, and compiles reports – iterating up to 20 cycles without human intervention[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=experimental%20processes%20and%20computational%20simulations,flawless)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=execution%20on%20designated%20research%20missions%2C,local%20attention%20and%20global%20oversight). It achieved near-flawless execution on a polymer simulation case, showcasing reliability of LLM agents in long-horizon tasks[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=of%20polymer%20chain%20conformations%20as,validation%20mechanisms%2C%20and). These pioneering works inspire our approach: we will leverage LLMs (such as GPT-4 or open-source equivalents) as the “brain” of an autonomous PSA system. The agent will break down complex PSA problems into actionable steps (e.g. _“simulate process at X conditions”_, _“perform LCA with dataset Y”_, _“train a surrogate model for Z”_), execute them via integrated tools, and interpret the results to decide next steps.

**2.4 Reinforcement Learning and Decision Support for Process Design:** Beyond static optimization, reinforcement learning (RL) has been explored for process synthesis and decision-making. Gao _et al._ (2023) showed that RL algorithms can _“learn to build process flowsheets”_ by sequentially adding unit operations and connections, in tandem with a process simulator (DWSIM) providing rewards[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=learn%20to%20build%20process%20flowsheets,that%20stores%20knowledge%20gained%20while). A challenge noted is the heavy computational demand: an RL agent may require thousands of simulator calls, so researchers often resort to simplified “shortcut” models to train the agent, sacrificing accuracy[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=learn%20to%20build%20process%20flowsheets,that%20stores%20knowledge%20gained%20while). Gao _et al._ addressed this via transfer learning, pre-training the RL agent on easier models and then fine-tuning on the rigorous simulator, which yielded an economically feasible flowsheet with 8% higher revenue while halving learning time[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=RL%20agent%20demands%20numerous%20process,process%20design%20and%20apply%20it)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=separation%2C%20and%20recycles%2C%20our%20method,can%20be%20reduced%20by%20a). Other studies have applied hierarchical RL to flowsheet synthesis, and actor–critic methods to process design, confirming RL’s potential but also its need for efficient simulation (or surrogate) support[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=facilitate%20process%20design,however%2C%20lead%20to%20inaccurate%20results)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=established%20approach%20from%20machine%20learning,can%20be%20reduced%20by%20a). Our framework does not primarily focus on RL for building flowsheets from scratch; however, we incorporate RL concepts for _scenario generation and sequential decision support_. For example, the agent could use an RL policy to choose which process modifications to explore next (analogous to an expert system suggesting experiments), balancing exploration of novel configurations with exploitation of promising regions identified by surrogates. Additionally, multi-objective optimization (MOO) – particularly Multi-Objective Bayesian Optimization (MOBO) – is a key decision-support capability we will implement. MOBO will allow the automated exploration of trade-offs between economic and environmental objectives (e.g. minimizing cost versus greenhouse gas emissions) by intelligently sampling design alternatives and using surrogate models (GPs) to approximate the Pareto frontier. This is aligned with suggestions in literature to consider _“global warming impact parallel to cost estimation via multi-objective optimization”_[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=maximal%20growth%20with%20product%20concentration,). Overall, our system will synergize these AI techniques: the LLM agent can orchestrate MOBO loops and possibly embed an RL-based strategy for complex decision flows, all while ensuring the TEA and LCA data are consistently handled.

In summary, **the state of the art** reveals isolated pieces – integrated TEA/LCA toolkits, ML surrogates for speeding computation, LLM agents for automation, and RL for design – but no unified solution. This proposal’s novelty lies in **combining these advances into one coherent framework**: an autonomous “copilot” for process systems engineering that can perform end-to-end analyses (from technical simulation to economic and environmental evaluation) with minimal human intervention. The next section details the architecture and components of this proposed framework.

## **3. Proposed Framework and Methodology**

**3.1 Architecture Overview:** The system will be built around an LLM-driven _agentic orchestrator_ that interfaces with simulation engines, databases, and machine learning modules. **Figure 1** depicts the high-level architecture of the proposed framework. The central AI agent (implemented via a state-machine approach using the Model Context Protocol, MCP) serves as the coordinator, receiving high-level goals from the user and managing the workflow through various tools and sub-modules.
![[image-2.png]]




_Figure 1: Proposed architecture of the autonomous PSA framework. An LLM-based agent orchestrator (left) communicates with process simulation & TEA tools (top center, e.g. BioSTEAM or Aspen), LCA engines (center, e.g. Brightway or OpenLCA), and an ML toolkit (right) for surrogates and optimization. The agent uses the Model Context Protocol (MCP) to manage tool interactions with well-defined states (planning, executing, reviewing), enabling it to plan experiments, execute simulations, invoke LCA calculations (with background databases), train models, and iterate towards optimal solutions. Dashed lines indicate information flows or user inputs/oversight._

At the core of the architecture is the **LLM Agent Orchestrator**, which will be implemented as an AI agent with defined states and transition logic (e.g., using the MCP framework as in Claude’s agent orchestration[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts)). The agent operates through a cycle of states such as _PLANNING_ (devising a plan of action), _EXECUTING_ (calling tools to perform tasks), _REVIEWING_ (evaluating results and checking constraints), and _ERROR handling_[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts). This stateful design ensures the agent’s actions are transparent and logically structured, rather than monolithic black-box decisions. The agent is “agentic” in that it can make decisions to pursue sub-goals autonomously: for example, if initial results show high uncertainty in a cost estimate, the agent might decide to run additional simulations or refine a surrogate model to reduce that uncertainty, without explicit human instruction.

Key components and data flows in Figure 1 include:

- **Process Simulator & TEA Module:** We will use an open-source simulator (BioSTEAM as a primary candidate) to perform process material and energy balance calculations and compute techno-economic metrics (capital cost, operating cost, net present value, etc.). The agent provides _design parameters_ or flowsheet configurations to this module (e.g. operating conditions, equipment sizes, feedstock choices), and receives back simulation results: mass and energy balances, stream flows, utility usage, and itemized costs[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=benchmark%20designs%20modeled%20in%20proprietary,prioritize%20research%2C%20development%2C%20and%20deployment). BioSTEAM’s API will facilitate programmatic modification of process configurations and retrieval of results. Notably, BioSTEAM automates equipment sizing and costing, and can handle uncertainty distributions for inputs[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=through%20its%20fast%20and%20flexible,via), which we will exploit for stochastic analysis.
    
- **LCA Engine:** For life-cycle assessment, we plan to integrate the **Brightway2** LCA framework[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20an%20open,outputs%20over%20its%20life%20cycle) or alternatively leverage OpenLCA’s backend via its Python API (openLCA provides a JSON-RPC interface for external calls[greendelta.github.io](https://greendelta.github.io/openLCA-ApiDoc/#:~:text=openLCA%20provides%20an%20API%20for,to%20call%20functions%20in%20openLCA)[greendelta.github.io](https://greendelta.github.io/openLCA-ApiDoc/#:~:text=In%20the%20dialog%2C%20you%20can,browser%20using%20the%20Fetch%20API)). The agent will take inventory data from the TEA simulation (e.g. raw material requirements per functional unit, energy consumed, emissions generated on-site) and feed that into the LCA engine along with background datasets. Brightway offers high flexibility in constructing LCA models from Python and efficiently calculating impact indicators[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20designed%20to%20make,users%20from%20industry%20and%20consulting). Meanwhile, OpenLCA’s API allows connection to extensive databases (like ecoinvent or USEEIO) and even the possibility of the agent operating the OpenLCA GUI via a “copilot” mode for data curation and QA. By _“curation and QA”_, we refer to ensuring that the mapping between process simulation outputs and LCA inputs is correct – for instance, if the process produces a waste sludge, the agent (via an LLM) could search the LCA database for an appropriate disposal process, or prompt the user if multiple choices exist. We will encode guidelines so that the agent selects LCA data consistent with TEA assumptions (e.g. same geographical region, technology maturity). The LCA engine returns environmental impact results (e.g. GHG emissions, energy footprint, water use) which the agent can then interpret alongside economic results.
    
- **ML Toolkit:** This module encompasses all machine-learning components for accelerating and enhancing analysis. It includes surrogate model training (regression models like GPs, neural nets, etc.), design of experiments, and optimization algorithms. The agent can invoke functions here to, for example, fit a Gaussian Process model to simulation input-output data accumulated from multiple runs. This surrogate can then predict outcomes for new designs almost instantly, which the agent can use inside an optimization routine. We will implement Multi-Objective Bayesian Optimization (MOBO) where the agent calls a BO algorithm that uses the GP surrogates of objectives (e.g. minimized cost, minimized CO₂ emissions) to propose next candidate designs. The agent thus iteratively alternates between calling the **Process Simulator/TEA** (to get data and refine surrogates) and calling the **Optimizer** (to get improved design suggestions), until convergence on a Pareto-optimal set or until a specified number of iterations is reached. In addition, the ML toolkit can include a library of **Physics-Informed Neural Networks (PINNs)** for specific unit operations – for instance, a PINN that solves reactor differential equations faster than a conventional solver, or a PINN that predicts catalyst deactivation over time. The agent could choose to train a PINN if a certain unit’s simulation is the bottleneck in speed and if sufficient data or equations are available. Reinforcement Learning (RL) elements also reside in the ML toolkit: for example, an RL-based scenario generator that the agent can call to stochastically explore variations of a process configuration (useful in highly combinatorial design spaces where random exploration is needed beyond BO’s local search). The ML toolkit is thus the agent’s “analytical arm,” enabling it to go beyond brute-force simulation by leveraging learned models.
    
- **User Interface and Knowledge Base:** While the framework aspires to autonomy, the user (e.g. a process engineer or researcher) remains in the loop for setting initial goals, providing constraints, and reviewing final recommendations. The agent will accept a high-level problem statement from the user – for example: _“Evaluate the economic and environmental feasibility of producing 50,000 tonne/year of bio-ethanol via lignocellulosic feedstock, and suggest process improvements to minimize carbon footprint while maintaining profitability.”_ From this, the agent formulates specific tasks. The user can also impose constraints (e.g. “avoid using chromium catalysts due to toxicity” or “ensure IRR > 15%”) which the agent will treat as guardrails during optimization. Throughout the process, the agent maintains a **provenance log** of actions taken, tools invoked, data sources, and intermediate results. This log serves both transparency and reproducibility – every result can be traced (e.g. “Emission value X comes from Brightway using ecoinvent v3.8 process Y”). The agent will periodically report to the user, especially at decision points or upon completion, presenting findings in an interpretable form (charts, tables, explanations). Although much can be automated, **human oversight** is vital: at any point, the user may intervene, adjust assumptions, or override the agent’s decisions. This ensures that the domain expertise of engineers guides the AI and that trust is maintained.
    

**3.2 Tool Orchestration via MCP and Agent Design:** We will implement the agent using the **Model Context Protocol (MCP)** – an emerging open standard for connecting LLMs with tools and stateful control[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts). MCP allows us to define each external tool (e.g. a Biosteam simulation function, a Brightway LCA calculation function, a plotting or analysis function) with decorators and integrate them into the agent’s prompting framework[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=Add%20new%20tools%20by%20creating,mcp.tool). The agent’s prompt context will include descriptions of available actions (tools), similar to how a “function calling” API works with LLMs. For instance, we will have tools like `run_process_simulation(config)` which the agent can call with a JSON `config` of process parameters, or `perform_LCA(inventory)` to run an LCA on given inventory data. The MCP orchestration ensures that the agent can maintain a working memory of the conversation and state (through the state machine) and use tools appropriately. By adopting a state-based design, we can impose **guardrails**: e.g. in the PLANNING state, the agent is only allowed to output a plan (and not directly call tools); in the EXECUTING state, it can call tools but not generate new plans; in REVIEWING, it must analyze results and decide whether criteria are met or another iteration is needed. This structured approach prevents chaotic or unsafe agent behavior and makes the agent’s operation more interpretable. The guardrails will also include checks on results – for example, after a simulation, the agent might verify mass balance closure or check if cost results are within expected ranges, flagging any anomalies for review. If a plan fails (e.g. a simulation doesn’t converge, or a tool returns an error), the agent transitions to an ERROR state with a predefined strategy: it can either try an alternative method, adjust parameters (perhaps using an LLM-based guess of what might fix the error), or ask for human guidance if the issue persists. This aligns with the practice observed by Vyas _et al._ where a _“Validator-Reprompting loop”_ caught and corrected invalid plans[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=introduce%20a%20unified%20agentic%20framework,source%20LLMs%20in%20both%20accuracy)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,that%2C%20with%20structured%20feedback%20and). Our agent will similarly self-correct by analyzing error messages and using the LLM’s reasoning to adjust (e.g. if simulation failed due to a temperature out of range, the agent can reduce the temperature and retry, or query the surrogate model if available).

**3.3 TEA-LCA Coupling Approach:** A crucial methodological aspect is how we link TEA and LCA seamlessly. We will adopt a _gate-to-gate_ LCA approach focused on the process being designed, with up-stream and downstream impacts sourced from databases. The process simulator provides a detailed inventory of inputs and outputs for the system foreground (e.g. raw biomass input per batch, electricity used per kWh, emissions from fermentation, etc.). The agent will compile this inventory and map each item to an LCA dataset (for background processes). For example, if the simulation uses 1 tonne of corn stover, the agent will find an LCA entry for corn stover production; if electricity is consumed, it will choose an appropriate grid mix dataset based on region (optionally user-specified). This mapping can be done via Brightway’s search functions or via OpenLCA’s ontology if needed. To ensure consistency, **common assumptions** (like capacity factors, utility emissions factors) will be shared between TEA and LCA – possibly through a unified data schema. We will develop a schema or template that the agent fills, which includes all necessary info for both analyses (e.g. a JSON with sections for process design, economic factors, LCA factors). During each iteration of analysis, after simulation, the agent updates the inventory and pushes it to the LCA engine. The results from LCA (impact metrics) are then associated with the same scenario as the TEA results. This one-to-one coupling at each scenario enables multi-objective evaluation. If multiple scenarios are being compared (e.g. different process designs in a Pareto set), the agent ensures both TEA and LCA are computed for all before declaring one “dominant” – thereby avoiding bias towards designs that are only optimal in one dimension. We will also implement **uncertainty propagation** through this coupling: if certain inputs have probability distributions (e.g. yield uncertainty, price volatility), the agent can perform Monte Carlo sampling, feeding random draws into both TEA and LCA calculations for each run. Using vectorized or surrogate-assisted simulation, it could generate thousands of joint TEA-LCA outcomes rapidly, enabling a holistic uncertainty analysis (similar to BioSTEAM-LCA’s 1000-scenario demonstration[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs)). The agent will then use statistical analysis tools (in the ML toolkit) to summarize these results (e.g. probability of meeting cost target and emissions target simultaneously, sensitivity of outcomes to each uncertain parameter). Such integrated uncertainty analysis is a methodological advance ensuring that the framework not only finds “optimal” designs, but also quantifies confidence and risk.

**3.4 Machine Learning Components:** Several specific ML models and algorithms will be employed: (i) **Gaussian Process (GP) Surrogates** – for any model outputs that are computationally expensive or noisy. For instance, if optimizing a flowsheet with many continuous decision variables, instead of calling BioSTEAM for every evaluation, the agent will use GP regression to learn the mapping from decision variables to outcomes (cost, emissions). The GP’s inherent uncertainty estimate will guide the Bayesian optimizer in deciding where to sample next, focusing on regions of objective improvement or high uncertainty. (ii) **Physics-Informed Neural Networks (PINNs)** – for sub-process models like reactors, we will incorporate PINNs that the agent can train on-the-fly or load if pre-trained. For example, a PINN could solve the mass-transfer equations in a distillation column faster than the discrete tray-by-tray simulation, once trained on that column’s operating range. We will investigate training PINNs using data generated by first-principles models from BioSTEAM or external simulators, so that subsequent evaluations are instant. The agent’s logic will be: if a particular unit’s simulation accounts for a large fraction of total runtime and needs to be evaluated across many conditions, then train a PINN surrogate for it. This introduces adaptability in the framework’s use of ML. (iii) **Gradient Boosted Trees / Random Forests** – these will be used for quick approximations and feature importance analysis. For example, after gathering a dataset of ~100 scenario results (with various inputs), the agent might fit a tree-based model to rank which input uncertainties contribute most to outcome variance (providing explainability). Tree models can also serve as fallback surrogates when data is sparse or relationships are highly non-linear in ways GPs struggle with. (iv) **Multi-Objective Optimization algorithms** – aside from MOBO (which relies on GP), we will also implement or integrate evolutionary algorithms (like NSGA-II) as a comparison or backup for multi-objective optimization, particularly for non-smooth search spaces or when we want a diversity of solutions. The agent can deploy such an algorithm on the surrogate model (or even directly on simulation if feasible) to identify an approximate Pareto front, which it then refines via MOBO around the interesting regions. (v) **Reinforcement Learning for scenario policy** – as mentioned, we may encode certain decision problems as an RL environment. For instance, controlling a semi-batch process could be framed as an RL task where the agent chooses control actions over time to maximize yield and minimize energy. A policy learned in such environment (perhaps using deep Q-learning or actor-critic methods) could be embedded into the overall framework to handle dynamic optimization sub-problems. The agent’s planning step would then include using that policy to set, say, the heating profile of a reactor for each scenario rather than optimizing it with static DOEs. This way, continuous process control and static design decisions can be tackled together (similar in spirit to Vyas _et al._’s unification of planning and control[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,driven%20automation%20in%20chemical%20engineering)).

Throughout the development of these ML components, emphasis will be placed on **validation and verification**: each surrogate model will be validated against a holdout of simulation data or known physics to ensure fidelity. The agent will only rely on surrogates within the domain they are validated for – otherwise it will schedule new simulation runs to extend the training data (active learning). This guarantees that automated decisions are based on trustworthy models.

**3.5 Agentic Decision-Making and Workflow:** Bringing everything together, a typical workflow the agent would execute is:

1. **Initialization:** User defines the problem (e.g. process type, feedstock, product, scale) and objectives. The agent gathers necessary baseline data (perhaps running a base case simulation in BioSTEAM to establish feasibility).
    
2. **Planning:** The LLM agent in PLANNING state outlines a plan, for example: _“1) Evaluate base case TEA and LCA. 2) Identify major cost and emission drivers. 3) Train surrogates for those drivers. 4) Optimize process parameters to reduce cost and emissions. 5) Validate optimized design with full simulation and LCA. 6) Iterate if needed or analyze uncertainty.”_ This plan is internally represented and then executed step by step.
    
3. **Execution – Base Case:** The agent transitions to EXECUTING state and calls the simulation tool with base case parameters (via Biosteam API). It obtains results, then calls the LCA tool with the resulting inventory. Once results are in, it enters REVIEWING state: the LLM summarizes the base case outcomes (e.g. _“Base case cost = $X/ton, GHG = Y kg CO₂/ton. High cost due to expensive enzyme; high emissions due to natural gas usage in boiler.”_). The agent identifies that enzyme cost and boiler emissions are drivers.
    
4. **Execution – Surrogate Training:** The agent decides to vary enzyme loading rate and boiler fuel type in the simulation to see impact. It generates a design of experiments (maybe Latin hypercube) for these factors plus any others of interest (e.g. reactor temperature, catalyst loading, etc.), and batch-runs simulations (possibly in parallel, as BioSTEAM is fast). With the collected data, it trains a GP surrogate for cost as a function of those factors, and another for emissions. It reviews the model fit (e.g. R², error metrics). If fit is poor, it can decide to get more data in regions of high error (active learning).
    
5. **Execution – Optimization:** With surrogates ready, the agent calls the MOBO routine to find Pareto-optimal solutions (minimizing cost and emissions). Suppose it finds that lowering enzyme loading reduces cost but increases emissions due to longer processing time (needing more energy) – so there’s a trade-off. It identifies a few candidate solutions (one favoring cost, one favoring environment, one balanced). The agent then runs full simulations for these candidates to verify surrogate predictions, and performs LCA on them. It might refine the surrogate if discrepancies are found.
    
6. **Decision:** In REVIEWING state, the agent compares the candidates. It may apply further decision logic, e.g., eliminate any that violate constraints (like if a candidate has slightly lower cost but massively higher emissions, it might be deemed unsustainable). It might also consider risk: using uncertainty propagation, it can say _“Design A has 90% chance to meet cost target vs 70% for Design B, under market volatility.”_ Let’s say the agent picks the balanced design as recommended.
    
7. **Reporting:** The agent generates a report (in natural language with supporting tables/figures) summarizing the analysis: _“The autonomous analysis suggests that reducing enzyme usage by 20% and switching boiler fuel to biomass pellets can reduce cost by 15% while cutting GHG emissions by 25%. The expected minimum ethanol selling price is $1.30/L, with a 95% confidence interval of $1.20–1.45/L under feedstock and yield uncertainties. This design meets the sustainability target of >20% GHG reduction. Key drivers are enzyme cost and utility emissions; further R&D on enzyme efficiency could improve results. All steps and data are documented for review.”_ The user can then review this, inspect the provenance log (e.g. check if the agent considered all relevant constraints), and validate the findings.
    

If the user or agent identifies any issues (say a constraint was missed, or a scenario not considered), the process can iterate. Otherwise, the result is an optimized, thoroughly assessed process design delivered much faster and with more thorough analysis than a manual approach. This level of automation, we hypothesize, can cut down early-stage design evaluation from weeks or months of an expert’s time to mere hours of computation, thereby accelerating innovation in sustainable chemical processes.

## **4. Validation Plan and Case Studies**

We will evaluate the framework on **three case studies** spanning different sectors and challenges to demonstrate generality: (1) **PEM Hydrogen Production**, (2) **Lignocellulosic Biorefinery**, and (3) **Plastics Recycling**. These cases were chosen because they are industrially relevant, involve significant TEA and LCA considerations, and can benefit from ML acceleration due to complexity.

**Case 1: Proton Exchange Membrane (PEM) Hydrogen Production** – This case involves hydrogen generation via water electrolysis using PEM technology, a cornerstone of green hydrogen strategies. The process includes an electrolyzer unit (with catalysts, membrane), power electronics, and gas separation, with electricity as the major input. TEA issues: high capital cost of electrolyzers, electricity cost sensitivity, economies of scale. LCA issues: carbon intensity of electricity (dominant factor in GHG emissions of H₂), materials (platinum group metals in catalyst), and byproduct oxygen credit. We will set up a BioSTEAM simulation or use existing models (e.g. H2A model data) for a PEM system at a given scale (e.g. 100 MW). The agent will analyze scenarios such as different electricity sources (grid vs renewable), different load factors, and improved electrolyzer efficiency. Surrogate modeling will be valuable here to explore how efficiency and stack degradation rate affect TEA/LCA, without needing a detailed electrochemical model each time. For instance, a PINN could be trained on a mechanistic PEM cell model to quickly predict performance at various current densities and temperatures, which then feeds into the economic model. We expect surrogate-driven optimization to suggest an optimal operating current that minimizes levelized hydrogen cost while balancing efficiency (this might involve a trade-off between efficiency and capital utilization). Uncertainty analysis will include electricity price volatility and electrolyzer life uncertainty. The validation will check if our agent’s recommendations (e.g. “operate at X A/cm² using solar power for lowest emissions at acceptable cost”) align with known studies or manual analyses. We’ll measure speedup: e.g. performing an exhaustive search or Monte Carlo of 1000 scenarios in the integrated TEA-LCA might take days normally, but with our surrogates and agent coordination we aim for minutes. The outputs will be compared with DOE H₂ cost targets and literature LCA benchmarks to ensure plausibility.

**Case 2: Lignocellulosic Biorefinery (Ethanol or Bioproducts)** – This represents a complex process with biomass feed (e.g. corn stover or sugarcane bagasse) undergoing pretreatment, fermentation, and product recovery. We will likely leverage existing BioSTEAM biorefinery models (as the developers have published models for corn stover ethanol[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=scenarios%20for%20conceptual%20and%20emerging,compare%20established%20and%20early%20stage)). The TEA is complicated by many interconnected units (pretreatment reactors, enzymes, distillation, waste treatment) and significant uncertainty in yields, conversion efficiencies, enzyme costs. LCA is critical since the promise of biofuels lies in GHG reduction versus fossil fuels; factors like co-product allocation and land-use impacts must be considered. For this case, the framework will be pushed to handle a high-dimensional space: dozens of design parameters (enzyme loading, pretreatment conditions, separation reflux ratios, etc.) and similarly many outputs. The agent will focus on key drivers identified via global sensitivity (BioSTEAM’s uncertainty module can be used to sample and identify sensitive parameters). We anticipate using GP surrogates to approximate the overall process economic and environmental outcome as functions of, say, 5–10 key parameters. This will enable multi-objective optimization for minimum _MFSP_ (minimum fuel selling price) and minimum GHG emissions per MJ fuel. A specific test could be evaluating the effect of switching process configurations (like separate hydrolysis and fermentation vs. simultaneous saccharification and fermentation – a discrete choice); the agent should be able to navigate even such discrete decisions, potentially by treating configuration as categorical variables in optimization or by trying both pathways and comparing. We will validate the agent’s outputs against known results: for instance, Shi _et al._ (2020) found a certain range for sugarcane ethanol MFSP and emissions[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Demonstrated%20BioSTEAM,%E2%80%9D%20ACS)[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=projections%20of%20their%20environmental%20impacts,data%20as%20kernel%20density%20maps) – our agent’s base case should match those within tolerance. If our agent suggests a design (e.g. “increase pretreatment severity to improve yield, even if it costs more energy, because it improves downstream fermentation enough to net benefit”), we will manually verify that suggestion with targeted simulations. This case will also demonstrate the agent’s ability to maintain **reproducibility**: we will ensure that all data (feedstock compositions, LCA database entries used like fertilizer production, etc.) are documented so that an external party could replicate the analysis given the same framework. The success criterion is that the agent can find a design that significantly improves on the base case (say >10% cost reduction and >20% GHG reduction) and that it can quantify the uncertainty (perhaps the probability that the design remains economically viable if feedstock cost is 20% higher, etc.).

**Case 3: Plastics Recycling (Chemical Recycling of Waste Plastics)** – This case study addresses a different domain: the circular economy. We consider a process such as mixed plastic waste pyrolysis to produce an oil that can substitute crude in petrochemical processes. TEA factors: highly dependent on scale and feedstock cost (waste tipping fee or cost), product quality and yield (which influences revenue), and operational costs of pyrolysis and upgrading. LCA factors: credits for avoided incineration or landfill, emissions from pyrolysis (which can be energy-intensive), and benefits of offsetting virgin plastic production. We will use or build a model (possibly using BioSTEAM or another tool like Pyrolysis simulation in Python) for a plastics pyrolysis plant. The agent will explore scenarios like different reaction temperatures (affecting yield distribution of gases vs liquids vs char), addition of catalysts, or integration with downstream processes (like cracking of pyrolysis oil). This case will test the agent’s capability to handle **discrete choices and configuration**: e.g. including a hydrogenation step to upgrade oil vs selling as is, or mechanical recycling vs chemical recycling routes. We can frame this as a superstructure with options, and the agent can decide via an optimization or heuristic search which route is optimal. If this becomes combinatorially large, an RL approach (with an agent incrementally “building” the process flowsheet) could be tried, akin to Gao et al.’s approach[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the) but guided by our LLM agent at a higher level. Surrogates might be used for the pyrolysis yield as a function of temperature and residence time (since simulating the complex kinetics repeatedly is slow). We will validate results by comparing to literature: e.g. recent studies on chemical recycling TEA/LCA often find that without policy incentives, purely economic optimization might favor disposing of low-value plastic rather than recycling, whereas an environmental objective pushes toward recycling with a cost trade-off. Our agent should be able to reveal such tension and possibly suggest conditions or policies (it might incorporate a CO₂ price in TEA to internalize the LCA result). A validation metric: how well the agent’s identified Pareto front matches known trade-off curves in literature for plastic recycling vs energy recovery. Additionally, we will check that the framework can incorporate external data – for instance, if certain LCI data for plastic waste is not in our local DB, the agent should prompt for it or allow easy addition, demonstrating the _curation_ capability. We will use the OpenLCA copilot concept here: the agent might use OpenLCA’s Python interface to generate a product system for “1000 kg mixed plastic waste recycled” linking to sub-processes (collection, sorting, etc.), and ensure no double-counting or omission in LCA.

Across all case studies, we will assess the **performance** (speed, number of simulations required), **quality of results** (do the outcomes make sense, are they near-optimal compared to exhaustive search or expert expectation), and **agent robustness** (can it recover from errors like a simulation failing to converge? does it handle changes in problem definition gracefully?). We will also test partial automation modes – e.g. let the agent run in fully autonomous mode vs. a mode where it pauses for user approval at key steps – to see the impact on results and to demonstrate safe operation.

Crucially, each case study will produce a set of artifacts: a detailed report generated by the agent, the full provenance log, and comparative evaluation by us. This will allow us to showcase reproducibility and answer the question: _“Would a human analyst reach the same conclusions, and how much effort would it take them versus the agent?”_ We will gather qualitative feedback from collaborating domain experts by showing them the agent’s reports and seeing if the reasoning is convincing and if the solutions would be actionable in practice.

## **5. Work Plan and Timeline**

The project is planned for a duration of approximately 2 to 2.5 years (24–30 months), which is aggressive but feasible given the modular nature of development. Below is a Gantt chart illustrating the timeline of major tasks and milestones over this period:


![[Pasted image 20250822121053.png]]






_Figure 2: Tentative 2.5-year PhD timeline with key tasks. Overlapping bars indicate concurrent progress on multiple fronts. Early stages focus on framework design and integration (architecture, TEA-LCA coupling), followed by surrogate modeling and agent development. Optimization (MOBO) integration and case study implementations overlap in mid-phase. Final stages emphasize evaluation, dissemination (writing) and ensuring robustness of the system. Actual timing may adjust as needed._

As shown in **Figure 2**, the work plan is divided into overlapping phases:

- **Months 0–4: Schema & Architecture Development.** In this initial phase, the focus is on designing the overall framework schema and setting up the development environment. This includes defining data structures for TEA-LCA data exchange, selecting initial tools (installing BioSTEAM, Brightway, etc.), and sketching the MCP-based agent architecture. By Month 3, we aim to have a simplified “hello world” integration where the agent can run a trivial simulation and LCA (e.g. a one-unit process) via tool calls – essentially a vertical slice of the system. Literature review and related work study will be concentrated in the first 2 months to inform design choices.
    
- **Months 3–8: TEA-LCA Integration Prototype.** Here we will implement the coupling between Biosteam and Brightway in earnest. By Month ~6, the goal is a working prototype that given a fixed process design, automatically computes TEA and LCA and aggregates results. This involves writing adapters for BioSTEAM outputs to Brightway inputs, establishing a local LCI database (e.g. importing ecoinvent data into Brightway or connecting to OpenLCA IPC server). We will test this on a simple known process (perhaps an ammonia production or a toy process) for verification. In parallel, we start defining the agent’s structure (state machine logic without LLM initially, using rule-based transitions as a placeholder).
    
- **Months 6–14: Surrogate Modeling & ML Integration.** During this period, we develop the ML toolkit. We will create modules for training GPs (likely using libraries like GPyTorch or scikit-learn), PINN models (using PyTorch/TensorFlow – possibly adapting open PINN libraries for our cases), and integrate an existing Bayesian optimizer library. We will test surrogates on known functions and also on data from BioSTEAM (for example, generate data for a unit operation and see if GP can fit it well). By Month ~10, the surrogate and optimization capability should be integrated enough that we can run a closed-loop optimization on a simple problem (e.g. optimize a reactor temperature in BioSTEAM to minimize cost, using GP+BO). Simultaneously, the LLM agent interface will be developed: choosing which LLM to use (GPT-4 via API, or an open-source model locally for flexibility), and setting up MCP tool definitions for our functions. By Month 12, the agent should be able to take a simple goal and execute a short plan (e.g. optimize one parameter) using the tools. This phase also includes developing guardrails and basic validation routines (mass balance checkers, etc.). We plan to publish a conference paper or workshop paper around Month 12 on the surrogate modeling approach for integrated TEA-LCA, as a mid-term academic output.
    
- **Months 8–16: Agent Development & Tool Orchestration.** Building on the earlier groundwork, we fully implement the agent’s planning and reasoning capabilities in this phase. This involves prompt engineering for the LLM (designing prompts that instruct it on how to use tools, give it examples of plans, etc.), integrating the MCP state logic (using the MCP Python SDK[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=quality)), and testing the agent in stepwise scenarios. We will progressively increase complexity: first one tool usage, then sequences. The **agentic features** like error handling loops and provenance tracking will be implemented here. By Month ~16, we expect to have a functional autonomous agent that can run through a simple case study end-to-end (perhaps Case 1, PEM hydrogen, at a simplified level), albeit with likely need for further refinement.
    
- **Months 14–20: Multi-Objective Optimization (MOBO) and Decision Support.** At this stage, the focus is on robust decision-making integration. We will finalize the MOBO integration so that the agent can handle dual objectives (cost, emission) optimization reliably. We also incorporate any RL components or advanced scenario generation methods here if found necessary. For example, if Case 3 (recycling) requires exploring different configurations, we might implement a hierarchical decision approach (which could be RL-based or integer optimization). By Month 18, the agent should handle multi-objective scenarios and produce Pareto analyses. This is also when we start applying the agent to **full-fledged case studies** – starting likely with Case 1 (as it is relatively contained). We anticipate iterative debugging as the agent encounters real complexities of the case. Knowledge gained here will be used to refine earlier modules (e.g. add new tool abilities, improve prompts for reasoning about trade-offs).
    
- **Months 15–24: Case Study Implementations and Validation.** In this timeframe, all three case studies will be implemented and run. We plan roughly: PEM hydrogen case around Month 15-18, Lignocellulosic biorefinery around 18-22, and Plastics recycling around 20-24. There will be overlap, and learnings from one will carry to the next. For each case, we allocate time to gather any needed external data (e.g. for LCA, ensure we have relevant background data), build any missing pieces (like a pyrolysis model for plastics if not available), and run the agent in autonomous mode to generate results. We will then spend time analyzing those results, comparing with literature or alternative methods, and possibly adjusting the agent’s strategy (for instance, we might learn that the agent needs a better strategy to search discrete options, so we implement that). By Month ~24, all case studies should be completed with results documented. This phase also includes writing up results for journal publications: we anticipate at least one comprehensive journal paper covering the framework and a couple of case studies, submitted around Month 24.
    
- **Months 24–28: Evaluation, Uncertainty & Reproducibility Analysis.** In the final stretch, we will rigorously evaluate the system’s performance. We will run extensive Monte Carlo tests to assess how well the agent handles uncertainties (e.g. run the agent multiple times with different random seeds or perturbed data to see consistency). We will finalize the uncertainty propagation features if not fully utilized earlier. Also, an external validation may be done by giving the tool to a colleague to see if they can reproduce our case study results easily (testing the documentation/provenance aspect). Any remaining ethical/safety tests (like ensuring the agent doesn’t produce any unsafe recommendations without flagging, etc.) will be done now. We will compile the insights into the dissertation draft.
    
- **Months 28–30: Documentation and Dissertation Writing.** The last two months focus on writing the PhD dissertation and associated defense preparation. By this point, we expect to have multiple publications to incorporate. The documentation of the software (open-source release on a platform like GitHub) will be finalized so that others can use the framework. The timeline includes buffer for revision and addressing unforeseen challenges (for example, if an LLM API changes or if a case study requires more time due to data issues).
    

The Gantt chart (Figure 2) summarizes this schedule, showing overlapping tasks which reflect the iterative and modular development approach. Notably, many tasks run in parallel (e.g. agent development overlaps with surrogate modeling) – this is intentional, as progress in one area (like ML) can feed back to another (like agent logic) continuously.

By adhering to this timeline, we aim to complete the core research within 2 years and use the additional 0.5 year for polishing and final write-up, aligning with the goal of a timely PhD completion at a top technical university.

## **6. Expected Contributions**

This research will yield contributions across theoretical, methodological, technological, and practical dimensions:

- **Theoretical Contributions:** We will advance the theory of **integrated sustainable process design** by formulating a unified problem space that combines TEA and LCA as a multi-objective optimization under uncertainty. This includes new formulations of objective functions that blend economic and environmental criteria, and potentially new theoretical insights into how AI planning can be combined with engineering optimization. For instance, our use of LLMs for constrained planning in engineering can contribute to the theory of _AI planning with domain knowledge_ (showing how chemical engineering knowledge can be embedded in prompts or tool use). We also anticipate contributing to reinforcement learning theory for process synthesis by demonstrating how transfer learning or hierarchical RL can overcome some complexity barriers[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=RL%20agent%20demands%20numerous%20process,process%20design%20and%20apply%20it).
    
- **Methodological Contributions:** The project will develop a **novel methodology for autonomous process systems analysis**. This methodology outlines how to orchestrate simulations, data-driven models, and AI reasoning in a closed loop. The state-machine agent approach (inspired by MCP) applied to engineering analysis is a methodological innovation. We will document the design patterns for ensuring consistency between TEA and LCA models (essentially a methodology for TEA-LCA coupling that others can replicate, using open-source tools). Another key contribution is the methodology for **surrogate-based multi-objective optimization with an AI agent in the loop** – our approach to have the agent intelligently gather data and refine models on the fly can be applied to other domains as well. Additionally, we’ll propose best practices for **ensuring provenance and transparency** in AI-driven research workflows in engineering, addressing a current gap in methodology for explainable and auditable AI in process engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=together%20with%20state,solving%20bottleneck%20challenges%20in%20CE).
    
- **Technological/Software Contributions:** We will deliver a working **software framework (prototype)** that implements the proposed system. This will likely be in the form of a Python-based platform (integrating BioSTEAM, Brightway/OpenLCA, and our agent code) that can be used and extended by others. It essentially acts as a _“Digital Copilot”_ for process engineers. We plan to open-source the code under a permissive license (e.g. MIT or BSD) to align with open science principles – addressing the noted issue that only ~20% of automation studies share code[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=the%20models%20was%20reached,development%20of%20new%20automation%20methodologies). This software will include modules for: connecting to simulation engines, performing LCA calculations from Python, training ML models, and the LLM agent control logic. Even beyond our specific use cases, these modules themselves are valuable – e.g. a generic Biosteam–Brightway linkage package, or an MCP tool library for engineering tasks. The technology also includes any new PINNs or surrogate models tailored to certain processes (like a PINN for pyrolysis kinetics); these models can be the basis for future research or could be integrated into process simulators. We will also contribute to data by curating any process/LCA datasets needed for the case studies and making them available (respecting licensing, e.g. using open LCA databases or providing dummy data if needed).
    
- **Practical/Empirical Contributions:** Through the case studies, we will produce **practical insights for each domain** (H₂ production, biorefineries, plastic recycling). For example, we might uncover a promising operating regime for PEM electrolyzers or identify the conditions under which chemical recycling becomes environmentally favorable. These findings can guide industry or policy. More broadly, the demonstration that an autonomous framework can do in-depth TEA-LCA in a short time can influence how feasibility studies are conducted in practice – potentially shifting some of the workload from consultants and analysts to AI assistants, thereby reducing cost and time. The framework can serve as a training tool for new engineers – they could use it to explore scenarios quickly and learn which factors matter, augmenting human expertise. We expect to show that our approach can achieve comparable or better results than a traditional analysis done by an expert, thus proving the practicality of AI augmentation in process design. If adopted, this could accelerate development of sustainable technologies by enabling rapid iterative assessment of concepts (failing fast on non-viable ideas and honing in on viable ones). Our work will also set a **benchmark** for future AI-in-the-loop PSA – as one of the first comprehensive examples, it will be something others can improve upon.
    

In summary, the project aims to contribute a new paradigm of autonomous, AI-driven process analysis that merges the strengths of chemical engineering models with modern AI. We anticipate at least 2–3 publications in high-impact journals out of this (e.g. in sustainable engineering, process systems engineering, or AI for science venues), and the software release will amplify its impact by allowing others to build upon it. Ultimately, the contributions can be seen as a step towards the broader vision of a “self-driving laboratory” for process design – where AI agents can design, simulate, evaluate, and learn in an endless loop, constrained by human-defined goals and ethics.

## **7. Ethical Considerations**

Developing an autonomous decision-making system for engineering raises important ethical and responsible innovation issues. We address these proactively in our framework design:

- **Transparency and Explainability:** A known concern is the _“opacity of AI algorithms”_ which can impede trust in critical domains like chemical engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=The%20issue%20of%20opacity%20within,the%20mechanistic). We tackle this by ensuring that every decision by the agent is traceable. The provenance log will record not just what the agent did, but why (e.g., it will include intermediate reasoning output from the LLM where possible, and links between outcomes and the data that informed them). We will use techniques from explainable AI (XAI), such as feature importance from surrogate models to explain which factors most influenced the cost or emission outcomes. For any recommendation given to a human, the agent will accompany it with a rationale (in plain language) and reference data (like “Catalyst cost was high, so system chose an alternative catalyst which lowered cost by 15%[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=benchmark%20designs%20modeled%20in%20proprietary,prioritize%20research%2C%20development%2C%20and%20deployment)”). This practice aligns with calls for _transparency as a vital link_ to trustworthy AI in engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=together%20with%20state,solving%20bottleneck%20challenges%20in%20CE). We will evaluate the explainability by having domain experts review the agent’s reports for clarity. If needed, we might incorporate a simplified surrogate (like a decision tree) purely for explanation of a complex model’s decision.
    
- **Human Oversight and Control:** The framework is designed as a **support system** for human engineers, not a fully autonomous authority. Human oversight is built in at multiple levels. The user defines initial goals and constraints – the agent is bound to these. If the agent encounters a decision beyond its scope (e.g. a trade-off involving value judgments, such as trading cost for safety), it will flag it for human input. We will implement a mechanism where the agent can explicitly ask for confirmation or guidance if it is unsure or if the next step involves a potential risk (for example: “The optimal solution involves using a toxic solvent – proceed? [Yes/No]”). This ensures critical decisions are not made in a vacuum. During development, we will test failure modes of the agent and ensure it fails safe – e.g., if the LLM outputs an erroneous plan, the validator should catch it and either correct or seek help, rather than proceeding blindly. Ultimately, a human will review the final recommendations, and the system will be advertised as an **assistant** that improves human productivity, not a replacement for human judgment on safety or ethical trade-offs.
    
- **Data and Model Bias, Validity:** Since the agent relies on both data (for ML models) and an LLM (with its training biases), we must guard against any biases or invalid assumptions propagating. LCA data can sometimes be region-specific or outdated; the agent will make data provenance clear so that, for example, if it used a European dataset for electricity by default, a user in the US can know and replace it if needed. The LLM might have biases or hallucinations – to prevent it from generating false technical information, we constrain its actions to using the tools for any quantitative answers. Essentially, all numerical results must come from trusted computations (simulation, database, or ML model) rather than the LLM’s internal knowledge. The LLM’s role is more on planning and interpretation, which we will supervise by testing it on known problems. If it provides a misleading explanation, we will refine the prompting or add rule-based checks (for instance, cross-check any qualitative assertion it makes about chemistry or economics with known references). The MCP framework’s guardrails also help, as the agent cannot take an action that isn’t defined – limiting scope for LLM going off-track.
    
- **License and Intellectual Property Considerations:** We commit to open-sourcing the developed framework, which means carefully handling any third-party components’ licenses. All tools chosen (BioSTEAM, Brightway, OpenLCA API, etc.) are open-source, which avoids legal barriers. LCA databases, however, often have licenses (ecoinvent, for example, is not free for commercial use). We will use open databases (like the Open Life Cycle Database or USEEIO) for demonstration, and ensure the framework can plug in licensed data only when the user has obtained it (the agent will not distribute any proprietary data itself – it will just access what’s on the user’s system). The LLM used may have usage restrictions (if using a service like OpenAI GPT-4); we will abide by those and in documentation suggest appropriate models for different user needs (for an industry user concerned about IP, they might use an on-premises LLM instance). By keeping everything modular, one can swap out components to suit their licensing requirements (e.g. use Aspen Plus via a Python COM interface instead of BioSTEAM if they have a license – our agent could accommodate that with a different tool plugin).
    
- **Safety and Reliability:** While our agent deals with simulations (not directly running physical experiments), recommendations it gives could indirectly influence real-world decisions. We thus treat safety seriously. For example, if optimizing a reactor temperature, the agent will be programmed to respect known safety limits (it will have constraints like not exceeding known thermal stability limits of materials, etc., drawn from literature or user input). The agent’s error handling ensures that if a simulation returns an physically implausible result, that path is not pursued without investigation. We will incorporate a simple rule-base of “sanity checks” (e.g. yields cannot exceed 100%, costs cannot be negative, etc.) which the agent will use to validate outputs. If any such check fails, it triggers a review or requires human sign-off to continue. By providing not just what to do but why, the system encourages humans to think critically about the results, serving as a second set of eyes rather than an oracle.
    
- **Environmental and Social Impact:** The very purpose of this framework is to promote sustainable process design by integrating environmental criteria. Ethically, this aligns with sustainability principles – we are making it easier for engineers to include environmental impacts in decision-making, which may have positive societal impact (better tech decisions for climate). However, one must be cautious that the system doesn’t get misused (for instance, someone could in principle optimize purely for profit and ignore environment if they disable that objective). While we cannot enforce users’ motives, our default stance is to always include sustainability metrics and to highlight them. We will also ensure that the agent’s output includes discussion of any ethical/environmental implications it can infer (e.g. if the optimal solution in Case 3 relies on incineration, the agent should note the trade-off in environmental harm). The LLM’s training might allow it to make such connections (especially if fine-tuned on sustainability texts).
    

In conclusion, by embedding transparency, oversight, and clear ethical guidelines into the framework, we aim to create a responsible AI tool that augments human decision-making in chemical process development. This not only mitigates risks but also helps build trust in the approach, addressing the _“missing link”_ of transparency that is essential for AI adoption in chemical engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=matter%20in%20a%20way%20that,solving%20bottleneck%20challenges%20in%20CE). Through rigorous testing and an open development process, we will adhere to high standards of research integrity and ethical tech deployment.

## **8. Conclusion**

This proposal outlined a comprehensive plan to develop an autonomous, agent-driven framework for process systems analysis that integrates techno-economic analysis and life cycle assessment with advanced machine learning and optimization. By tackling current limitations – the siloed nature of TEA/LCA, the high expertise and time demands, and the underutilization of AI in the workflow – the proposed research aims to revolutionize how sustainable chemical process design is conducted. The end result will be a proof-of-concept system (validated on real case studies) showing that an AI agent, guided by domain knowledge and empowered by ML surrogates, can perform end-to-end analysis and optimization, providing human engineers with insightful recommendations while ensuring consistency, speed, and transparency.

The success of this project could mark an inflection point in process engineering methodology, demonstrating the viability of AI “co-pilots” in engineering design. It aligns with the goals of chemical engineering to accelerate innovation and with global needs for rapid development of economically and environmentally sound technologies. Moreover, the interdisciplinary knowledge generated – spanning chemical process modeling, machine learning, and AI orchestration – will contribute to the emerging field of AI for Science and Engineering.

By executing the research plan and achieving the stated objectives, this PhD will produce both significant academic contributions and a practical tool with potential for real-world impact. We anticipate that this work will not only address the immediate research gap but also pave the way for future studies (e.g. scaling to more complex processes, integrating real-time plant data for autonomous operations, etc.). In the long run, such agentic frameworks could be extended to create self-driving laboratories or digital twins that continuously learn and advise, ultimately merging with experimental efforts for closed-loop innovation.

**References:** (Selected relevant references are cited in IEEE style throughout this proposal, for brevity we have embedded them in context. Key sources include: Shi et al. 2020[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=1%20Shi%2C%20et%20al,9b07040); Cortes-Peña et al. 2020[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=and%20Engineering,9b07040) on BioSTEAM and LCA integration; Köck et al. 2023[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=life%20cycle%20inventory%20%28LCI%29%20phase,highest%20level%20of%20maturity%20of)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=the%20models%20was%20reached,development%20of%20new%20automation%20methodologies) on LCA automation; Vyas & Mercangoz 2025[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=,loop%20iteratively%20refines%20invalid%20plans)[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=a%20laboratory%20TCLab%20platform%20,driven%20automation%20in%20chemical%20engineering) on LLM agent for control; Liu et al. 2024[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=,4o)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=Claude,local%20attention%20and%20global%20oversight) on autonomous simulation agent; Gao et al. 2023[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=established%20approach%20from%20machine%20learning,by%20a%20factor%20of%202) on RL for process design; and Yuan et al. 2024[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=together%20with%20state,solving%20bottleneck%20challenges%20in%20CE) on transparency in AI for chemical engineering. Full bibliographic details are available in the project repository and documentation.)


------



# Proposal: An Agent-Driven Framework for Automated Techno-Economic and Life Cycle Analysis in Process Systems Engineering

## Introduction

Process systems analysis (PSA) plays a critical role in evaluating the viability and sustainability of chemical processes through **techno-economic analysis (TEA)** and **life cycle assessment (LCA)**. TEA assesses the economic performance (e.g. costs, profitability) of a process or product, while LCA quantifies environmental impacts across the process life cycle[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=3%20Figure%201%3A%20Product%20life,private%20industry%20to%20take%20into)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=Life%20cycle%20sustainability%20assessment%20,from%20at%20least%20two%20sustainability). Together, TEA and LCA provide complementary insights for decision-makers, but in practice they are often conducted separately and manually, leading to **time-consuming, inconsistent workflows**. Performing a full LCA can require **extensive data collection and computation**, which is _often a limiting factor_ especially for emerging technologies[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=Scope%20phase%20%28e,consuming)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=and%20can%20improve%20data%20quality). Likewise, TEA studies frequently rely on ad-hoc spreadsheet models and human expertise, making it difficult to rapidly iterate designs or integrate new data. As a result, **current PSA methods face limitations** in terms of integration, efficiency, and scope. Recent advances in machine learning (ML) and artificial intelligence (AI) – notably **large language models (LLMs)** – present an opportunity to **automate and accelerate PSA** beyond the conventional paradigm. This proposal outlines a research plan to develop a novel **LLM-driven agentic framework** that orchestrates process simulation tools, TEA/LCA models, and predictive ML in a unified system. The goal is to **streamline and enhance TEA-LCA workflows** using AI agents, enabling faster, more comprehensive process evaluations. The proposed research will (1) characterize the limitations of current TEA-LCA methods from literature, (2) identify gaps in integrating TEA, LCA, LLMs, and ML, (3) design an improved system architecture (with AI agent logic, tool orchestration, and data flows), (4) implement predictive models (Gaussian processes, PINNs, gradient boosting) alongside process simulators (e.g. Aspen Plus or BioSTEAM), LCA software (Brightway2/OpenLCA), and optimization techniques (Bayesian optimization, reinforcement learning), (5) validate the framework on multiple case studies (such as hydrogen production, biorefineries, plastics recycling) with coupled TEA-LCA metrics, (6) establish a timeline with milestones (including a Gantt chart), (7) compare our approach with existing PSA automation efforts, (8) highlight key contributions, and (9) address ethical and reproducibility considerations. By leveraging only peer-reviewed sources from high-impact journals throughout, we ensure the approach is grounded in state-of-the-art knowledge. The outcome will be a formal Ph.D. proposal suitable for a top-ranked Chemical Engineering program, aimed at advancing both the methodology and practice of sustainable process systems engineering.

## Limitations of Current PSA Methods in TEA and LCA

**Fragmented Workflows and Inconsistent Boundaries:** Conventional TEA and LCA are often performed in _siloed workflows_, using different tools and assumptions, which makes integrated analysis challenging. TEA typically takes an **investor’s perspective** focusing on plant-gate economics, whereas LCA adopts a **full life-cycle perspective**[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=3%20Figure%201%3A%20Product%20life,private%20industry%20to%20take%20into)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=Life%20cycle%20sustainability%20assessment%20,from%20at%20least%20two%20sustainability). This misalignment means system boundaries and functional units may differ: for example, TEA might consider only on-site (gate-to-gate) costs, while LCA considers cradle-to-grave impacts[nrc-publications.canada.ca](https://nrc-publications.canada.ca/eng/view/object/?id=a53ef77e-88ad-43e3-849b-328004df678d#:~:text=publications,but). Literature reviews show that differing system boundaries and perspective biases can lead to _conflicting conclusions_, unless carefully harmonized[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20Goal%20and%20scope%20,However%2C%20the%20selection%20of)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20The%20inventory%20,can%20have%20monetary%20impacts%2C%20such). **Norris (2001)** highlighted the need to integrate economic implications into LCA and vice versa, yet also noted difficulties in interpreting combined results[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=remaining%20gaps%20in%20literature%20Norris,TCA%20aims%20at%20including%20often)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=life%20cycle%20assessment%20results%2C%20companies,environmental%20results%20is%20not%20presented). In practice, many TEA-LCA studies still treat each assessment separately and then attempt to compare results post hoc, with limited coordination of assumptions[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20Goal%20and%20scope%20,However%2C%20the%20selection%20of)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20The%20impact%20calculation%20,impacts%20are%20often%20considered%20static). This **lack of integration** can cause inconsistencies – e.g. using a different benchmark or feedstock in TEA vs. LCA yields incomparable outcomes[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=benchmarks%20for%20comparison%20within%20each,benchmarks%20need%20to%20be%20identical). Azapagic et al. (2006) found that if economic and environmental benchmarks differ (cheapest vs. greenest alternative), the integrated assessment can send mixed signals[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=benchmarks%20for%20comparison%20within%20each,of%20the%20technology%2C%20the%20benchmarks). In short, current PSA methods struggle to maintain a _common basis_ for TEA and LCA, limiting their combined utility.

**Methodological Challenges and Uncertainty:** TEA and LCA each have distinct methodologies that are not trivial to combine. The underlying **data and modeling principles differ** – for instance, TEA cash-flow models include financial parameters (taxes, depreciation, etc.) and dynamic indicators like NPV or IRR, whereas LCA uses static inventory flows and impact categories[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20The%20impact%20calculation%20,impacts%20are%20often%20considered%20static)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=posing%20additional%20challenges%20for%20interpreting,impacts%20are%20often%20considered%20static). There is _no one-to-one mapping_ between physical process flows and economic flows: a material stream increase may linearly raise emissions but not costs due to economies of scale or price nonlinearities[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20The%20inventory%20,can%20have%20monetary%20impacts%2C%20such)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=different%20and%20prevent%20a%20simple,impacts%20are%20often%20considered%20static). Furthermore, TEA output units (e.g. USD) cannot be directly aggregated with LCA outputs (e.g. kg CO₂) without subjective weighting or conversion (as in single-score sustainability metrics)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20The%20impact%20calculation%20,impacts%20are%20often%20considered%20static). This makes interpreting integrated TEA-LCA results difficult – studies have noted that combining economic and environmental indicators requires careful normalization or multi-criteria analysis, often lacking standard guidelines[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments). **Wunderlich et al. (2021)** reviewed 70 papers and found an _“increasing number of contributions”_ on TEA-LCA integration, but _no commonly followed definition_ or framework for doing so[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments). The authors identified a **knowledge gap** in how to practically integrate TEA and LCA from the ground up, as most studies used case-specific or qualitative integration methods[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments). This gap leaves practitioners without clear methodological guidance. Another challenge is **uncertainty propagation** – early-stage process data are often sparse and uncertain, which cascades through both TEA and LCA. _Zimmermann et al. (2022)_ point out that at low technology readiness levels, many data are missing or not representative of future scales[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=match%20at%20L694%20A%20central,not%20yet%20measured%20or%20modeled)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=A%20central%20challenge%20when%20assessing,not%20yet%20measured%20or%20modeled). This leads to high uncertainty in both cost and impact estimates. Conventional approaches handle this via scenario analysis or sensitivity analysis, which are labor-intensive to do comprehensively. In summary, current PSA methods face _methodological limitations_ in aligning TEA and LCA calculations and in dealing with large uncertainties inherent to new technologies[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=challenges%20and%20collect%20best%20practices,and%20governmental%20literature%20and%20practice)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=Not%20only%20data%20availability%20is,efficiency%20measures%20such%20as%20heat). These issues can result in **mismatches between study results and stakeholders’ needs**, especially if TEA and LCA give divergent recommendations[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=However%2C%20current%20technology%20assessment%20practice,LCA%2C%20TEA%2C%20and%20TRL%20and)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=challenges%20for%20practitioners%20when%20adapting,streamlining%20conventional%20LCA%20and%20TEA). There is a need for more systematic, flexible approaches to integrate these analyses.

**Manual Effort and Inefficiency:** Perhaps the most practical limitation is the **intense manual effort** required to perform TEA and LCA together. Conducting an LCA involves four phases (goal/scope, inventory analysis, impact assessment, interpretation) per ISO 14040/44, and the _inventory phase (LCI) is extremely time- and resource-intensive_[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=processes%20that%20are%20still%20in,and%20can%20improve%20data%20quality)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=and%20can%20improve%20data%20quality). Collecting input/output data for potentially **thousands of unit processes** is painstaking[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=,9)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=be%20collected%20for%20the%20parameters,129). Köck et al. (2023) note that even for a single LCA, “_thousands of unit processes are usually used_” requiring large volumes of data from lab experiments, databases, and literature[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=As%20the%20most%20time,and%20uncertainty%20distribution%20should%20also)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=,9). This complexity often _deters iterative use_ of LCA during process design; instead, LCA is done after a process design is fixed[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=To%20date%2C%20chemical%20process%20simulations,integration%20of%20LCA%20in%20process). Similarly, TEA is frequently done via custom spreadsheets or process simulator economic reports, which must be updated by hand when the design changes. **Integrating LCA with process simulation is possible** (e.g. via add-ons or exporting data), but typically not automated. A review of LCA automation efforts found that while some tools link LCA software with process simulators, they often require _ad hoc scripting or even GUI trickery_ (e.g. a “Java robot mouse emulator” to transfer data between programs)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process). For example, Mutel et al. connected gPROMS process simulator to Umberto LCA software by using Excel as an intermediary and simulating user input because no direct API was available[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user). These kinds of solutions underscore the lack of seamless integration – current workflows resemble a patchwork of _“glue” code and manual steps_ prone to error. One study described that to optimize a design considering LCA and TEA, _designers had to go back-and-forth iteratively_, manually adjusting the process and re-running analyses until some compromise was found. This trial-and-error approach is **computationally and cognitively challenging**, and may miss optimal solutions. In summary, today’s PSA practice is **labor-intensive, slow, and non-autonomous** – hindering our ability to quickly evaluate new ideas or identify optimal designs under both economic and environmental criteria. The following gaps highlight where innovation is needed to overcome these limitations.

## Gap in Integrating TEA, LCA, LLMs, and Predictive ML

Despite recognition of the above challenges, **existing approaches to integrate TEA and LCA remain limited** in scope, and none have leveraged the latest AI advancements. Recent literature does show steps toward combining economic and environmental assessments – for instance, _life cycle sustainability assessment (LCSA)_ frameworks attempt to encompass economic (life cycle costing) and LCA together[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=Life%20cycle%20sustainability%20assessment%20,from%20at%20least%20two%20sustainability)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=combine%20models%20for%20economic%2C%20environmental,that%20may%20not%20be%20suitable). Also, some process simulation platforms have begun to incorporate LCA modules (e.g. BioSTEAM includes a Brightway2 LCA integration for biorefineries[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=process%20simulation%20software%20BioSTEAM%20,79)). However, these solutions are still **hard-coded and static**. A 2023 systematic review by Köck et al. found many methods with _automation potential_ for LCA data collection – such as using process simulators to auto-generate inventories or applying ML to fill data gaps – yet _“an automated connection between [process simulation and LCA] is lacking”_ in practice[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process). In other words, current frameworks do **not fully automate** the iterative loop of updating process designs and re-evaluating TEA/LCA. They typically require significant user intervention or are restricted to narrow use cases. There is a **clear gap** in having a _unified, intelligent system_ that can integrate **TEA, LCA, large language models (LLMs), and predictive machine learning (ML)** in an **agentic** (autonomous, decision-making) manner. No peer-reviewed study to date has demonstrated an AI agent that orchestrates the entire PSA workflow end-to-end.

Crucially, **LLMs have not been applied in the context of process systems engineering tools**. LLMs like GPT-4 have shown remarkable abilities in planning and tool use in other domains – for example, in chemistry, the **ChemCrow system** used an LLM to autonomously plan multi-step synthesis tasks by invoking specialized software tools[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=Large%20language%20models%20,Our)[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=accomplish%20tasks%20across%20organic%20synthesis%2C,the%20gap%20between%20experimental%20and). ChemCrow’s success in “bridging the gap between experimental and computational chemistry” by an agent that can read databases, run calculations, and suggest experiments demonstrates the _potential of LLM agents_ for scientific workflows[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=augments%20the%20LLM%20performance%20in,between%20experimental%20and%20computational%20chemistry). Similarly, in architecture, recent work integrated ML and LLMs to assist low-carbon design: an LLM (ChatGPT) was used to suggest material substitutions to reduce a building’s carbon footprint by querying emission factor databases[mdpi.com](https://www.mdpi.com/1996-1073/17/12/2997#:~:text=match%20at%20L888%20found%20a,3%7D%3B%20straw%20insulation%3A%2012.31)[mdpi.com](https://www.mdpi.com/1996-1073/17/12/2997#:~:text=match%20at%20L1093%20Case%20study,driven%20suggestions%20to). These examples highlight that LLMs, when augmented with tool access, can perform _meaningful autonomous analyses_ in technical domains. However, in the **chemical process design/analysis literature**, we find **no example of an LLM-driven PSA orchestration**. This is a striking gap given the complexity of coordinating simulations, databases, and optimization – tasks that an intelligent agent is well-suited for. The proposed framework will be one of the first to explore **LLM agents for orchestrating chemical engineering tools**, filling this gap and potentially redefining how PSA is conducted.

Another gap lies in the **integration of predictive machine learning models** with TEA-LCA. While ML has been increasingly applied in related areas (for instance, to predict properties or optimize processes), its use _within TEA/LCA studies is still rare_. A recent review identified 40 studies combining ML with LCA, finding ML mainly used to predict life cycle inventory data or impact indicators, and noted that _“deep integration of ML into LCA”_ is an important future direction[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=ML%20models%20have%20several%20merits,Despite%20these%20challenges%2C%20there)[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=This%20review%20analyzed%2040%20articles,to%20address%20environmental%20sustainability%20challenges). The review observed challenges such as limited training data, lack of uncertainty analysis, and few guidelines – indicating that **ML in LCA is still in nascent stages**[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=are%20also%20challenges,establishing%20model%20evaluation%20reporting%20procedures)[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=This%20review%20analyzed%2040%20articles,to%20address%20environmental%20sustainability%20challenges). On the TEA side, the idea of using data-driven models to complement or replace detailed simulations is only beginning to emerge. One study on sustainable aviation fuels remarked that _ML as a complementary tool to TEA is seldom reported_ in literature[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=,the%20literature%20to%20build%20a). In that work, Rogers _et al._ used ML to predict both the economic and environmental performance of a hydrogen production process, demonstrating that such approaches can accelerate analysis[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=,the%20literature%20to%20build%20a). However, those efforts typically apply ML **offline** (e.g. train a model on a dataset of pre-run simulations). There is a gap in using ML **online within an optimization or agent loop** for PSA. For example, **Gaussian Process (GP) models** could act as surrogate models to rapidly estimate outcomes of a process simulation, but linking that into a live TEA-LCA optimization loop has not been shown in literature. Likewise, **physics-informed neural networks (PINNs)** have proven capable of modeling chemical process dynamics when data or mechanistic knowledge is limited[arxiv.org](https://arxiv.org/abs/2406.01528#:~:text=,if%20respective%20constitutive%20equations%20are)[arxiv.org](https://arxiv.org/abs/2406.01528#:~:text=an%20easy,avenue%20that%20warrants%20further%20investigation), yet PINNs have not been employed to support techno-economic or LCA assessments in real-time. The gap is essentially that **no existing framework combines**: (a) **TEA & LCA integration**, (b) **LLM agent orchestration**, (c) **predictive ML surrogates**, and (d) **automated optimization** into one cohesive system. Our proposed research directly addresses this gap by developing a _unified agentic system_ that leverages each of these elements.

In summary, the state-of-the-art suggests: **(i)** TEA-LCA integration is recognized as valuable but remains cumbersome and partially automated at best[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user); **(ii)** LLM-based agents have transformative potential for automating complex workflows, but have not been applied to process systems; **(iii)** ML techniques (GPs, PINNs, GBMs, etc.) are powerful but underutilized for streamlining PSA; and **(iv)** existing automation efforts in PSA lack the flexibility and intelligence that an agent-based approach could provide. The research proposed will fill these voids, creating a novel framework that **integrates TEA, LCA, LLM agents, and ML predictive models** to automate and accelerate process systems analysis beyond what is currently possible.

## Proposed Framework and System Architecture


![[image-1.png]]



_Proposed system architecture integrating an LLM-driven agent with process simulation, TEA/LCA tools, machine learning models, and optimization engines._ The envisioned framework consists of a **central AI agent (LLM-based)** that orchestrates various modules: (1) a **Process Simulation + TEA module** (e.g. flowsheet in Aspen Plus or BioSTEAM) for calculating material/energy balances and economics, (2) an **LCA module** (e.g. Brightway2 or OpenLCA) for computing life-cycle environmental impacts, (3) a suite of **Predictive ML models** (surrogate models like Gaussian processes, PINNs, gradient-boosted trees) to approximate process outputs or speed up calculations, (4) an **Optimization module** implementing techniques such as Bayesian optimization or reinforcement learning to explore the design space, and (5) access to **knowledge bases and datasets** (such as thermodynamic databases, equipment cost databases, or LCI databases) for supporting information. The LLM agent serves as the _“brain”_ of the system, making decisions on what actions to take (which tool to run, what variables to adjust) in order to achieve the objectives (e.g. minimize cost and emissions).

**Agent Orchestration and Tool Integration:** The LLM agent (e.g. built on GPT-4 or a similar model specialized with process engineering knowledge) will be prompted with the high-level goal (for instance: _“Find the process configuration that minimizes cost per kg of product while keeping GHG emissions below a target.”_). Using a chain-of-thought reasoning approach, the agent will **plan a sequence of actions** to evaluate and optimize the process. It may start by calling the process simulation to get a baseline TEA/LCA result, then analyze those results and decide on adjustments. The agent uses tool-specific APIs or command interfaces to run simulations and LCA calculations. For example, BioSTEAM (an open-source process simulator) can be scripted in Python to output both TEA results and a life-cycle inventory compatible with Brightway2[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=process%20simulation%20software%20BioSTEAM%20,79). The agent can automate this scripting. After running a simulation, the agent obtains **performance data** (e.g. yields, energy use, equipment sizes, capital and operating costs) and **inventory flows** (amounts of raw materials, emissions, etc.). It then triggers the LCA tool by feeding it the inventory, perhaps via Brightway’s Python API, to compute environmental impact metrics (e.g. global warming potential, water footprint). By orchestrating these steps, the agent essentially performs an **integrated TEA-LCA evaluation** for a given set of design parameters – something that traditionally might require a human manually transferring data between software. Notably, this approach eliminates the need for brittle “file import/export” or GUI macro solutions seen in older integration efforts[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user). Instead, the agent ensures a _continuous data flow_ from simulation to LCA within one automated loop, enhancing reliability and repeatability[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process).

**LLM Agent Decision-Making:** The agent’s logic will incorporate both **heuristic rules and AI planning**. We will encode domain-specific rules (for safety and feasibility) to guide the agent. For instance, if the agent proposes a design outside realistic bounds (e.g. a reactor temperature above a threshold), the framework will reject or modify it, providing feedback to the agent. The LLM’s natural language reasoning allows it to interpret outputs and objectives flexibly. For example, after an initial simulation, the agent might “say”: _“The cost is high and emissions are above target; to reduce emissions, I will try increasing the percentage of renewable energy in the process power supply.”_ It can then adjust the electricity source parameter and re-run analyses. Such **autonomous iterations** mimic how an engineer might manually experiment, but here it is done by the agent at computational speeds. The agent will also be capable of consulting external knowledge: for instance, querying a database of typical equipment costs if needed or retrieving a property from a chemical database (ensuring any such queries use vetted, licensed sources). In our architecture (Figure 1), the agent has access to a **Knowledge Base** which could include thermophysical data, emission factors (for LCA), or even prior studies’ results to inform decisions. This is akin to giving the agent an expert’s background knowledge on-the-fly. The novelty is that **the agent can handle multi-objective reasoning** – balancing economic and environmental goals – by analyzing the outputs from TEA and LCA together. This addresses the current gap where such balancing is done manually (or not at all). The agent’s decisions will be transparent and logged (to address explainability; see later section), for example: “_Outcome: cost $X, GHG Y. Plan: try alternative catalyst to improve yield._” In summary, the LLM agent serves as an intelligent controller, automating the _PSA workflow (simulation → TEA → LCA → optimization)_ in a closed-loop fashion that was not possible with conventional scripting alone.

**Predictive Machine Learning Integration:** A key innovation is integrating **surrogate ML models** into the loop to accelerate and enhance analysis. We will develop and utilize at least three types of ML models: **(a) Gaussian Process (GP) regression models**, **(b) Physics-Informed Neural Networks (PINNs)**, and **(c) Gradient Boosting Machine (GBM) models** (e.g. XGBoost). Each offers unique advantages:

- **Gaussian Processes (GPs):** GP models are a powerful choice for building **surrogate models** of expensive simulations. They can fit complex nonlinear mappings with relatively small datasets and provide uncertainty estimates for their predictions. GPs have been used successfully in chemical engineering optimization – e.g. Durkin _et al._ (2022) present a framework using GPs to model process performance and even classify simulation convergence, facilitating simulation-based optimization[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations)[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=simulations,a%20conservativeness%20parameter%20to%20enable). We will train GP models on initial simulation runs (which the agent will perform across the design space) to approximate outputs such as yield, energy consumption, or cost as functions of key design variables. The agent can then **query the GP surrogate** in lieu of running a full simulation each time, dramatically speeding up exploration. For example, instead of running Aspen Plus 1000 times, the agent can train a GP on say 50 runs and thereafter get instant predictions with quantified uncertainty. This approach aligns with the idea of **Bayesian optimization**, where a GP surrogate guides the search for an optimum[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations). If the GP predicts with low uncertainty that a certain region of design space improves the objective, the agent will focus there; if uncertainty is high, the agent might run a new simulation to refine the model (active learning). In effect, the GP surrogate will help the agent to intelligently balance exploration vs. exploitation of designs. GPs also naturally handle noisy data and can incorporate constraints (e.g. via a classification GP for feasible vs infeasible designs)[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations)[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=framework%20in%20which%20Gaussian%20Process,off%20between%20process%20performance). The use of GPs will address the efficiency gap: it provides a **versatile surrogate modeling toolbox** to tackle expensive simulation optimization with far fewer runs[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations).
    
- **Physics-Informed Neural Networks (PINNs):** PINNs are neural networks that embed physical laws (typically expressed as differential equations) into the training process. They have shown great promise in modeling chemical engineering systems where some first-principles knowledge exists but data are limited[arxiv.org](https://arxiv.org/abs/2406.01528#:~:text=,if%20respective%20constitutive%20equations%20are). For instance, Velioglu _et al._ (2024) demonstrated that PINNs could infer **immeasurable states** in a chemical process (like a CSTR and a separator) with reasonable accuracy, even without full mechanistic equations for those states[arxiv.org](https://arxiv.org/abs/2406.01528#:~:text=informed%20neural%20networks%20,and%20only%20partially%20known%20mechanistic)[arxiv.org](https://arxiv.org/abs/2406.01528#:~:text=an%20easy,avenue%20that%20warrants%20further%20investigation). In our framework, PINNs can be used to create surrogate models for dynamic or unit-level behavior with built-in physical consistency (e.g. enforcing mass/energy balances or kinetic laws). One application is if the process involves time-dependent behavior (startup, batch processes) or complex unit operations where running a full high-fidelity simulation or experiments is impractical. A PINN can be trained on limited data from such a unit (or even directly from the governing equations if known) and then provide **fast predictions of unit performance** under new conditions. For example, if our case study involves a biochemical reactor with Michaelis-Menten kinetics, we could train a PINN to predict conversion and yield as functions of residence time, incorporating the kinetic ODEs as part of the loss function. The agent can then use this PINN model inside the TEA calculation (instead of a slow ODE solver) to evaluate many scenarios rapidly. PINNs thus fill the gap when combining first-principles and data – they ensure the surrogate **respects known physics**, improving reliability of predictions. This is crucial for extrapolating beyond the initial data (since the PINN won’t violate conservation laws, etc.). By integrating PINNs, our system will be capable of handling cases where purely data-driven models might falter due to limited data or need for extrapolation, thereby increasing the robustness of the automated analysis.
    
- **Gradient Boosting Machine (GBM) models:** GBMs such as **XGBoost** and **LightGBM** are proven high-performance ML models for tabular data and have seen successful use in chemical engineering contexts that involve a mix of continuous and categorical features. They often outperform neural networks on smaller datasets and are easier to interpret (through feature importance). In the TEA/LCA context, GBMs can be used to predict outcomes like **life-cycle impact indicators or costs** based on features of the process and supply chain. For example, one could train an XGBoost model to predict the carbon footprint of a product given features like energy source mix, key process yields, and material inputs. Prior work has shown GBMs performing well in related tasks: a study on LCA for chemicals found that Support Vector Machines and XGBoost could predict certain impact metrics with reasonable accuracy, where XGBoost achieved better than neural nets in some cases[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S235255092500065X#:~:text=,5811%20and%20Artificial%20Neural)[www2.aiche.org](https://www2.aiche.org/sites/default/files/2024-03/Rowan%20UEF%20Final%20Report_0.pdf#:~:text=XGBoost%20is%20a%20decision%20tree,is%20a%20supervised%20learning). Another study used gradient boosting to model yield and optimize biofuel processes, citing its efficiency on those problems[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=Five%20different%20ML%20models%20including,19)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=GBR%20is%20an%20ensemble%20machine,crucial%20to%20optimize%20the%20predictive). In our framework, after generating a database of simulation results and corresponding impacts, we can train a GBM to **directly predict TEA-LCA outputs**. The agent might use such a model in preliminary screening of a large design space – since GBM predictions are instantaneous, the agent could scan thousands of design combinations and shortlist promising ones for more detailed GP/PINN analysis. The GBMs thus act as a high-level guide or for sensitivity analysis to identify which inputs most strongly affect cost or emissions (their feature importance can provide insights). Additionally, GBMs can complement GPs by handling higher-dimensional inputs or categorical choices (e.g. choice of technology A vs B as a categorical input).
    

By integrating these ML models, the **agentic framework becomes a hybrid AI system** – leveraging _data-driven predictions_ alongside rigorous simulations. This will significantly accelerate the analysis cycle. A concrete example: for a given process design change, instead of re-running a full LCA (which might require linking to large databases and summing over hundreds of flows), a pre-trained ML model could instantly estimate the new impact within a small error margin. This addresses the current inefficiency where each design tweak requires a full recalculation. The ML models will be continuously updated as more data are gathered; the LLM agent can decide when the surrogate’s uncertainty is low enough to trust or when to fall back to the high-fidelity tool. This kind of **dynamic fidelity adjustment** is novel in PSA – it ensures speed **without sacrificing accuracy** beyond acceptable limits.

**Process Simulation and LCA Tool Orchestration:** We will utilize state-of-the-art tools to implement the TEA and LCA calculations within the framework. On the process simulation side, candidates include **Aspen Plus/HYSYS** (widely used in industry and academia for process modeling) and **BioSTEAM** (an open-source alternative focused on bio-based processes with Python integration). Aspen Plus can be controlled via its COM interface or an Excel VBA, but more directly we might opt for Python-driven simulators or surrogate models to allow tight coupling with the LLM agent. BioSTEAM is particularly attractive as it has built-in TEA capabilities (e.g. costing functions) and can output LCA inventories integrated with Brightway2[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=process%20simulation%20software%20BioSTEAM%20,79). On the LCA side, **Brightway2** (a Python LCA framework) will likely be our primary engine given its flexibility and large database support (ecoinvent, etc.). OpenLCA or SimaPro could also be used if needed via their APIs. The important aspect is that the LCA tool will take the material and energy flows from the simulation’s output and link them with background processes (from an LCI database) to calculate impacts. We will ensure that the **system boundaries are aligned**: the process simulation defines a cradle-to-gate scope for TEA, and the LCA extends to cradle-to-grave as needed, using a functional unit consistent with the TEA’s basis (e.g. per kg of product)[nrc-publications.canada.ca](https://nrc-publications.canada.ca/eng/view/object/?id=a53ef77e-88ad-43e3-849b-328004df678d#:~:text=publications,but). Any disparity (such as co-products or different lifetime assumptions) will be explicitly handled by the agent’s logic (prompting it to adjust scenarios or include allocation steps). The agent can also automatically perform **scenario analyses** by switching out background data – for example, to examine a renewable energy scenario vs. fossil energy scenario in LCA, which historically would be a manual change. By scripting these changes, the agent could ensure **paired TEA-LCA scenarios** are evaluated under consistent assumptions (addressing stakeholder needs for both economic and environmental outcomes as highlighted by Zimmermann et al.[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=match%20at%20L694%20A%20central,not%20yet%20measured%20or%20modeled)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=Overall%2C%20we%20identified%20six%20challenges,technologies%3A%20meeting%20stakeholders%27%20needs%2C%20defining)). Additionally, the agent could orchestrate **multi-objective optimization** by calling the optimization module: it can vary decision variables (via Bayesian optimizer or genetic algorithm) and collect TEA/LCA results, building up a Pareto front of cost vs. environmental impact. This process is akin to what some researchers have done manually with multi-objective genetic algorithms linking simulators and LCA[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process), but here the _search and data gathering are autonomous_.

The **Optimization module** in our architecture will provide algorithms for both _global optimization_ and _sequential decision-making_. We will implement **Bayesian optimization** (BO) for continuous process variables (temperature, pressure, feed ratios, etc.) using the GP surrogate as mentioned. BO will iteratively propose the next best set of variables to evaluate, balancing improving the objective and reducing uncertainty. This method has been proven effective for expensive engineering optimizations, often finding optima in far fewer runs than grid search or evolutionary algorithms[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations). For problems involving _sequential decisions or control policies_ (e.g. an operating profile over time, or scheduling decisions), we will investigate **Reinforcement Learning (RL)** approaches. While RL is relatively new in process systems optimization, initial studies show it can learn optimal control strategies or design policies that might be hard to derive analytically. Our agent could use an RL algorithm (e.g. deep Q-learning or policy gradients) in cases where the objective is long-term or multi-stage. For instance, in a biorefinery supply chain scenario, an RL agent could learn decisions like when to store vs. sell intermediate products to maximize profit and minimize emissions over a year. These RL decisions would feed into the TEA/LCA calculations (which provide the reward signal, e.g. profit minus an emissions penalty). The LLM agent might not directly implement RL (since RL training is numeric and iterative), but it can _invoke_ an RL training routine in the optimization module when it recognizes a sequential decision problem. By combining BO and RL as needed, the framework covers both **static optimization** (design parameters) and **dynamic optimization** (operational or planning decisions). This comprehensive optimization capability, driven by the agent, surpasses what prior integrated TEA-LCA tools have achieved – most literature examples used either brute-force search or simple parametric studies[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=match%20at%20L1352%20framework%20integrates,when%20integrated%20with%20the%20process)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process).

Finally, **data flow and management** in the framework will be carefully designed for transparency and reproducibility. Each time the agent runs a simulation or an LCA, the input and output data will be logged in a structured format (e.g. a JSON or database entry). This means we maintain a full trace of the decisions the agent made and the resulting TEA/LCA metrics. The agent can use this log to avoid repeating identical evaluations (caching results), and more importantly, it provides a record that can be audited by humans. If the agent finds an optimum solution, we will have the data to reproduce that solution’s evaluation with the stand-alone tools, which is crucial for trust. We will also integrate **uncertainty propagation** in the data flow: if the LCA module provides uncertainty ranges (many LCI databases have geometric standard deviations for flows), the agent can capture those and potentially make risk-averse decisions (e.g. optimize the 90th percentile cost rather than median). In summary, the proposed architecture (illustrated in Figure 1) combines **tool orchestration, AI agent logic, predictive ML, and optimization** in a novel way. It creates a closed-loop system where **the agent iteratively improves the process design by coordinating all components**, aiming to meet multi-dimensional performance targets. This framework directly tackles the current PSA limitations by providing integration (TEA + LCA from the start), speed (ML surrogates and automation), and intelligence (LLM reasoning and advanced optimization) in one package.

## Machine Learning Models and Agent Logic Details

_This section provides additional technical detail on the ML models and decision logic to be employed, aligning with the requirements to describe Gaussian Processes, PINNs, GBMs, and agent decision-making integration with simulations, LCA, and optimization._

**Gaussian Processes for Surrogate Modeling:** We will use Gaussian Process regression to create surrogate models of the process’s response surface. The GP will model outputs y=f(x)y = f(x)y=f(x) such as product yield, unit cost, or environmental impact as a function of design inputs xxx (which could be a vector of dozens of variables like temperatures, pressures, catalyst loading, etc.). A GP is defined by a mean function (often taken as 0 or a simple polynomial) and a covariance kernel k(x,x′)k(x, x')k(x,x′) that encodes our prior belief about the function smoothness. We may choose common kernels like the squared exponential or Matérn kernel. GPs are attractive because **every prediction is a Normal distribution** – giving a mean and a confidence interval. This uncertainty is crucial for the agent’s decision logic: if the GP is very uncertain about a region (perhaps due to lack of data), the agent can choose to sample (run a simulation) there to improve the model, an active learning strategy consistent with Bayesian optimization. In fact, algorithms like Expected Improvement or Upper Confidence Bound from BO literature will be employed by the agent when deciding the next query point for simulation[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations). For example, the agent might compute the Expected Improvement of various candidate designs based on the GP model, and select the design with highest EI to simulate next – automatically balancing exploration/exploitation. Another planned use of GPs is to model **feasibility constraints**. As noted by Durkin et al., GP classification can model the probability of a simulation converging or a design being feasible[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=simulation,converged%20simulations)[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=framework%20in%20which%20Gaussian%20Process,off%20between%20process%20performance). We can train a GP classifier on past simulation runs labeled “feasible” or “infeasible” to guide the agent away from unproductive regions (e.g. combinations of conditions where the reactor fails to converge). This reduces wasted simulation calls and also provides a safety mechanism. The **conservativeness parameter** concept from Durkin et al.[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=framework%20in%20which%20Gaussian%20Process,converged%20simulations)[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=used%20to%20model%20feasibility%20constraints,converged%20simulations) might be applied – allowing the agent to tune how aggressively it explores near the predicted feasibility boundary. Overall, the GP surrogates will serve as the agent’s “mental model” of the process: initially coarse and uncertain, but becoming more refined as data are added. This mimics how an engineer’s understanding improves with experiments, except here the improvement is quantitative and systematically used in optimization.

**PINNs and First-Principles Integration:** For processes where first-principles knowledge is strong (e.g. reaction kinetics, mass transfer relations), we will integrate that knowledge via PINNs or hybrid models. A PINN will be set up by defining the governing equations (differential or algebraic) as part of the network’s loss function. For instance, consider a plug-flow reactor in a case study: we have dCdV=−r(C)F\frac{dC}{dV} = -\frac{r(C)}{F}dVdC​=−Fr(C)​ (mass balance) with some reaction rate law r(C)r(C)r(C). A PINN can be constructed where the network outputs concentration as a function of reactor volume, C^(V;θ)\hat{C}(V; \theta)C^(V;θ), and we penalize deviations of dC^/dV+r(C^)Fd\hat{C}/dV + \frac{r(\hat{C})}{F}dC^/dV+Fr(C^)​ in the training loss (along with data points if available). This PINN would then give a continuous prediction of concentration profile and could generalize across different flow rates or inlet conditions if those are included as inputs. By training such a model, the agent can get reactor performance results almost instantly for any given design (as opposed to numerically integrating ODEs each time). Notably, PINNs have been shown to **handle scarce data regimes** effectively[arxiv.org](https://arxiv.org/abs/2406.01528#:~:text=an%20easy,avenue%20that%20warrants%20further%20investigation) – relevant for innovative processes where only a few lab experiments exist. They also provide extrapolation grounded in physics (the agent can be more confident using a PINN to extrapolate to a larger scale, for example, than using a standard neural net). We will validate any PINN against known solutions or additional high-fidelity simulations to ensure it’s sufficiently accurate. The integration of PINNs with the agent means the agent could even adjust formulating the PINN on the fly – for example, if the agent suspects a certain physical effect is important, it might include an additional term or equation in the PINN formulation (this would be quite advanced and likely guided by us initially rather than the agent spontaneously). A simpler approach we may use is _grey-box modeling_: combining a neural network with known equations (e.g. use known heat balance equations but learn an unknown heat transfer coefficient via a small neural net). This falls in the same spirit as PINNs and would be deployed as needed to ensure the agent has fast, differentiable (if needed) models of the process.

**Gradient Boosting and Tabular ML Models:** We anticipate using XGBoost or similar for tasks like _meta-modeling_ the TEA/LCA results dataset. Once the agent has compiled (say) a few hundred designs with their TEA outputs (NPV, ROI, etc.) and LCA outputs (CO₂ footprint, etc.), a GBM can be trained to predict those results from the design parameters. This is akin to building an _approximate model of the entire TEA-LCA outcome space_. One advantage of GBMs is they can naturally handle categorical variables (like different technology options or raw material choices) by one-hot encoding internally. For example, in a plastics recycling case study, we might have a categorical choice of recycling technology (mechanical, pyrolysis, glycolysis, etc.). A GP would struggle with this unless we do one-hot encoding manually with a kernel that can handle categorical inputs; an XGBoost can handle it more directly. We will use GBMs to complement GPs in such mixed discrete-continuous optimization problems (which are common in process synthesis). The agent could run a preliminary design of experiments sampling (like Latin Hypercube) across technology choices and continuous variables, train a GBM, and then use that model to quickly identify which options seem best. The interpretability of GBMs via feature importance or SHAP (Shapley Additive Explanations) values also aligns with our goal of explainability – it can highlight, for instance, that “feedstock carbon content” has the largest effect on both cost and emissions, echoing findings from literature qualitatively. We can cite the example: Samrity _et al._ reviewed ML in LCA and note that **ANNs were most common, but others like gradient boosting were also used effectively**[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=being%20the%20top%20focus%20area,used%20to%20evaluate%20ML%20model)[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=ML%20models%20have%20several%20merits,Despite%20these%20challenges%2C%20there). In our work, we’ll use whichever model (GBM vs ANN vs GP) suits the data size and complexity best, giving the agent a **library of models** to choose from. The agent, being an LLM, can be prompted to pick a model type by describing the situation (“small data, need uncertainty -> use GP; larger data, more categorical -> use XGBoost; have differential equations -> use PINN”), though these decisions will likely be encoded in the agent’s policy rather than discovered.

**Integration with Process Simulation & Databases:** The ML models will not stand alone; they will integrate tightly with the simulation and LCA tools. For example, the process simulator can be used to generate training data for ML (the agent will do this by sampling the input space systematically early in the project). Conversely, once an ML surrogate is deemed accurate, it might replace the simulator in certain optimization loops. We will maintain consistency by calibrating the ML models to the high-fidelity models and occasionally _reverting to high-fidelity calculations to verify ML predictions_. The LLM agent is key in this integration: because it operates in a reasoning mode, it can decide, for instance: _“The GP predicted an optimum at X, but to be sure, I will run a full simulation at X to verify the result.”_ This kind of oversight ensures the final recommendations are backed by the trusted, mechanistic models, using ML mainly to reduce the search effort.

To summarize, our framework’s ML and agent components will work in synergy: **GPs** provide probabilistic surrogates for efficient search, **PINNs** embed physical knowledge for reliability with limited data, **GBMs/ANNs** capture patterns in the combined TEA-LCA outcome space, and the **LLM agent** orchestrates their use, switching between surrogate and detailed models as appropriate. This multi-tier approach to modeling and decision-making is a major advancement over existing PSA methods that typically rely on either purely mechanistic models or simplistic data fits, but not an intelligent combination of both. By detailing these elements in the proposal, we clarify how each piece contributes to the overall goal of automating and accelerating process analysis.

## Case Study Domains for Application

To demonstrate and validate the proposed framework, we will apply it to **multiple case study domains** that are relevant, challenging, and cover a spectrum of process types. We plan to include at least **three case studies** in distinct areas: _(1) Green Hydrogen Production, (2) Biorefinery for Sustainable Fuels, and (3) Plastic Waste Recycling._ These were chosen because they all require coupled TEA-LCA analysis to evaluate trade-offs and are areas of active research where improved decision-support tools are needed.

**Case Study 1: Green Hydrogen Production** – _Process:_ Water electrolysis using renewable electricity (and potentially alternative H₂ production methods like methane pyrolysis for comparison). _Why:_ Hydrogen is a cornerstone of decarbonization strategies, but its viability depends on both cost (levelized cost of hydrogen, LCOH) and environmental impact (carbon footprint per kg H₂). TEA-LCA integration is crucial: “_LCA is essential for evaluating and optimizing hydrogen production by assessing environmental impacts such as Global Warming Potential_”[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S004896972500991X#:~:text=analysis%20www,), while TEA tells us if it’s economically feasible. _Scope:_ We will model a 100 MW electrolysis plant, which includes electrolyzers, power supply (grid vs. solar/wind), compression and storage. The agent will be tasked with minimizing LCOH ($/kg) while keeping life-cycle GHG emissions below a certain threshold (e.g. to qualify as “green” hydrogen, often <2 kg CO₂ per kg H₂ is targeted). _Metrics:_ **Economic** – LCOH in $/kg, capital investment, operating cost breakdown; **Environmental** – GHG emissions (kg CO₂e/kg H₂), possibly water use and other impacts. We will validate baseline numbers against literature. For instance, a recent review of LCA studies for water electrolysis notes that the carbon footprint of electrolytic H₂ strongly depends on the electricity source, ranging from near-zero (100% renewable) to significantly higher if fossil electricity is used[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S004896972500991X#:~:text=analysis%20www,). Many studies report LCOH for green hydrogen in the range of $4–8/kg at present[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S004896972500991X#:~:text=analysis%20www,); our TEA will aim to reproduce such figures under baseline assumptions. _Use of Framework:_ The LLM agent will automatically explore configurations – e.g. sizing of renewable energy, whether to add battery storage, operating current density (which affects efficiency and capital utilization), etc. The GP surrogate might be used to model how efficiency and stack lifetime trade off with operating conditions. The agent can also consider **technology switches** (like comparing alkaline vs. PEM electrolyzers, or including a methane pyrolysis unit scenario). Those discrete choices will test the agent’s ability to handle non-continuous decisions, potentially utilizing the GBM model. The integrated TEA-LCA will reveal trade-offs: e.g. a design with cheap grid power at night (lower cost) but higher emissions vs. more renewables (higher cost, lower emissions). Our framework can find the optimal mix or identify break-even carbon prices, etc., providing insights beyond separate TEA or LCA. This case will demonstrate the framework’s ability to handle an **energy-intensive, continuous chemical process** with both capital and operating decisions, under dual objectives. The output might be, for example: _“Optimal electrolyzer operating at 1.8 A/cm², 50% renewable power fraction yields LCOH = $5.2/kg, GHG = 1.9 kgCO₂/kg – meeting the target.”_ The agent’s analysis will be compared to previous studies’ conclusions to ensure realism. If available, we will cite data from sources like the International Journal of Hydrogen Energy or Applied Energy to anchor our results.

**Case Study 2: Biorefinery for Sustainable Aviation Fuel (SAF)** – _Process:_ Fast pyrolysis of biomass (e.g. forest residues) followed by bio-oil upgrading to sustainable aviation fuel. _Why:_ Biorefineries involve complex process networks and supply chains, where **feedstock properties, process conditions, and scale** heavily influence both economics and LCA results[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=produced%20by%20integrated%20chemical%20looping,prediction%20of%20the%20MSP%20of)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=Data,prediction%20of%20gas%20yield%20during). Many studies have done TEA and LCA for biofuels, but often separately or with limited integration. Our framework can shine here by handling the many degrees of freedom and uncertainty. _Scope:_ We’ll consider a pyrolysis + hydroprocessing pathway for SAF, producing e.g. 50 million liters per year of jet fuel. The system includes feedstock logistics (collection, transport), the conversion plant, and product distribution. We will incorporate possible co-products (electricity, biochar) as well. _Metrics:_ **Economic** – minimum selling price (MSP) of SAF in $/liter (or $/GJ), internal rate of return (IRR) at a given selling price; **Environmental** – GHG emissions per MJ fuel, perhaps also land use or other impact categories from LCA. For context, prior TEA-LCA studies (e.g. Rogalchuk & Markuts 2021, etc.) found SAF MSPs usually higher than fossil jet (often $1–2/L) and GHG reductions depending on feedstock[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=Several%20studies%20have%20reported%20the,Their)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=produced%20by%20integrated%20chemical%20looping,prediction%20of%20the%20MSP%20of). One recent study integrated process simulation with experimental data for pyrolysis, and noted _“several literature gaps”_ such as integrating feedstock property variation into the economic model[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=produced%20by%20integrated%20chemical%20looping,prediction%20of%20the%20MSP%20of)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=feedstock%20properties%20including%20proximate%20and,MSP%20of%20SAF%20from%20FP). We will address that by allowing the agent to vary feedstock properties (e.g. moisture, ash content) to see their effect, essentially treating feedstock as part of design. The framework might use a PINN to model how different biomass compositions affect yields and upgrading requirements, trained on lab data if available. _Use of Framework:_ The agent will coordinate a BioSTEAM simulation of the biorefinery (since BioSTEAM has been used for agile TEA-LCA in biorefineries[pubs.acs.org](https://pubs.acs.org/doi/10.1021/acssuschemeng.0c05998#:~:text=...%20pubs.acs.org%20%20BioSTEAM,offs%20across%20dimensions)). It will optimize variables like reactor temperature, hydrogen consumption in upgrading, and plant scale. It will also evaluate different **regions** or supply chain setups – thanks to integration with GIS data or a simple model for transportation, it can compare centralized vs. distributed production. Using ML, the agent could build a surrogate for the relationship between biomass properties and resulting fuel yield. In literature, ML models (including GBMs) have been applied to biofuel processes: e.g. Elmas _et al._ used ML to predict gas yields in biomass gasification[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=technology%20to%20investigate%20the%20relationship,environmental%20impact%20of%20hydrogen%20production)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=,study%20are%20as%20follows%3A%20to), and Rogers _et al._ applied ML to predict both economic and environmental outcomes for hydrogen from supercritical gasification[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=technology%20to%20investigate%20the%20relationship,environmental%20impact%20of%20hydrogen%20production)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=,study%20are%20as%20follows%3A%20to). We will extend such approaches by having the agent _actively use the ML predictions_ to guide design. The expected result is identifying an optimal design and feedstock strategy that minimizes MSP while achieving a significant GHG reduction vs. conventional jet (e.g. >70% reduction to meet CORSIA standards). The agent might find, for example, that using waste wood with higher moisture is acceptable if a larger reactor is used, because the cost impact is offset by feedstock savings – a non-intuitive insight it can back with numbers. We will compare the framework’s outcome with published ranges (from, say, Journal of Cleaner Production or Renewable & Sustainable Energy Reviews) to validate that it finds an MSP in line with prior TEA and an emissions reduction in line with LCA studies for similar pathways[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=,Subsequently%2C%20two%20deep%20learning). Any discrepancy will be analyzed, showcasing the transparency of our agent’s reasoning. This case demonstrates the framework’s capacity to handle **complex, multi-step processes with feedstock variability and co-products**.

**Case Study 3: Plastic Waste Recycling** – _Process:_ Mixed plastic waste recycling via multiple pathways (mechanical recycling, chemical recycling through pyrolysis or depolymerization, and comparison with virgin production and landfill/incineration). _Why:_ The plastics circular economy is a hot topic, where both economics and LCA are needed to choose recycling strategies. Mechanical recycling is often cheaper and lower-emission for clean waste, but chemical recycling can handle mixed or dirty waste at higher cost/environmental burden[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S092134492400689X#:~:text=,MPW%29)[pubs.acs.org](https://pubs.acs.org/doi/10.1021/acssuschemeng.2c05497#:~:text=Technical%2C%20Economic%2C%20and%20Environmental%20Comparison,lower%29%20performances). A holistic analysis is needed to decide the best approach for different conditions. _Scope:_ We will consider a municipal mixed plastic waste stream of a certain size (e.g. 100,000 ton/year). Options include: (a) **Mechanical recycling** (sorting, re-melting into pellets) – lower cost per ton, but limited to certain plastics and quality degradation; (b) **Chemical recycling (pyrolysis)** – converting plastics to oil and then to fuels or new plastics – higher processing cost and energy use, but can handle mixed waste; (c) **Monomer recycling** (e.g. glycolysis of PET) for specific polymers. The system will include necessary pre-processing (sorting, shredding) and post-processing (purification of pyrolysis oil, etc.). _Metrics:_ **Economic** – cost per ton of waste processed, or cost per kg recycled product, and possibly net profit considering products; **Environmental** – life-cycle GHG emissions per ton of plastic waste processed, or per functional unit of product (e.g. 1 kg recycled resin or 1 MJ fuel from waste), and avoided impacts by displacing virgin production. Literature provides some benchmarks: a review by Eriksen et al. (2020) indicated mechanical recycling typically has _lower environmental impacts than chemical recycling_ for mixed plastic waste[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S092134492400689X#:~:text=,MPW%29). Another study showed mechanical and certain chemical recycling (like PET glycolysis) had 7–88% lower GHG and 9–73% lower costs compared to other emerging technologies[pubs.acs.org](https://pubs.acs.org/doi/10.1021/acssuschemeng.2c05497#:~:text=Technical%2C%20Economic%2C%20and%20Environmental%20Comparison,lower%29%20performances)[pubs.acs.org](https://pubs.acs.org/doi/10.1021/acssuschemeng.2c05497#:~:text=Mechanical%20recycling%20and%20PET%20glycolysis,lower%29%20performances). However, mechanical recycling might incur higher system costs if extra sorting is needed[sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0921344924001265#:~:text=Economic%20and%20environmental%20comparison%20of,life%20cycle%20emissions%2C%20regardless). We will use such findings to validate our model. _Use of Framework:_ The agent will evaluate each pathway’s TEA and LCA, but more powerfully, it can consider **hybrid strategies**. For example, it might decide an optimal solution is to mechanically recycle part of the waste (the easily sorted PET and HDPE) and send the rest to pyrolysis. This kind of systems-level orchestration (like an agent “assigning” waste fractions to different processes) is novel. We can formulate it as an optimization where decision variables include fraction of waste to each pathway. The agent, using an MILP solver or genetic algorithm from the optimization module, can identify the split that minimizes total cost or emissions. Our integrated approach will automatically account for how diverting some waste affects economies of scale in each facility and the life-cycle impacts. The ML surrogate (e.g. a GBM) might be used to quickly predict outcome metrics for different splits and technology performance levels (since chemical recycling yields can vary widely with technology). If data is available, we will incorporate **learning curves** or future improvements (the agent could simulate scenarios for current vs. future tech performance). The output could be: _“At current conditions, a 70% mechanical / 30% chemical recycling mix gives the lowest emissions (X kg CO₂) at a cost of $Y/ton, whereas 100% mechanical yields slightly lower cost but higher emissions due to more landfilling of rejects. The agent thus recommends a combined approach.”_ We will verify that pure mechanical and pure chemical scenarios from our model align with literature – e.g. mechanical recycling should indeed show lower GHG than pyrolysis, and our numbers should mirror those in studies like Rahimi & García (which show e.g. pyrolysis can have higher CO₂ per kg product than mechanical)[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S092134492400689X#:~:text=,MPW%29). By doing this case, we stress-test the framework’s ability to handle **discrete choices, supply chain allocation, and multi-criteria optimization** (minimizing emissions subject to cost constraints or vice versa). It also highlights the agent’s ability to incorporate **policy constraints** – we could impose a recycling rate target or recycled content requirement and have the agent figure out how to meet it at least cost. This would be extremely useful for policy planning (and is something current tools do manually via scenario analysis).

Across all case studies, a common theme is demonstrating **TEA-LCA coupling and performance metrics** evaluation. Our framework will produce results such as cost-emissions tradeoff curves, optimal designs, and key sensitivity factors for each domain. We will define clear **performance metrics** for the framework itself: for example, _time saved_ in conducting the analysis (agent vs. an expert analyst), _thoroughness_ (how many design alternatives explored), and _quality of solutions_ (did it find a better solution or insight that was not obvious initially?). We expect the agent to uncover non-intuitive solutions or confirm hypotheses from literature with much less effort. Each case study will be documented in the proposal with the specific setup and the plan for validation against peer-reviewed benchmarks. This ensures our approach is credible and that the results will be meaningful to the broader community, illustrating the value-added by the integrated ML + LLM agent approach in solving real process engineering problems.

## Project Timeline and Milestones (2–2.5 Year Plan)



![[image.png]]



_Gantt chart overview of the proposed 2–2.5 year research timeline, showing project phases and their durations._ The project is structured into several phases with overlapping tasks, as illustrated in the Gantt chart above. The total duration is planned for **24 to 30 months**, aligned with a typical Ph.D. proposal timeline for preliminary results and dissertation writing. Below is a breakdown of the timeline into major tasks and milestones:

- **Months 1–3: Literature Review & Problem Definition.** In this initial phase, a comprehensive literature survey will be conducted on integrated TEA-LCA methodologies, machine learning in process systems, and applications of AI agents in engineering. We will document the limitations of current methods (building on sources cited in this proposal) and refine the problem statement. By Month 3, we expect to have a clear set of requirements for the framework and selection of initial case study (likely hydrogen) to focus on first. _Milestone:_ Complete literature review document and framework specification.
    
- **Months 3–6: Framework Design & Architecture Development.** During this period, we will design the system architecture in detail and start building the foundational code. This includes setting up the LLM agent environment (possibly using an existing platform for autonomous agents), and creating interfaces for process simulation and LCA tools. We will develop small prototypes to test communication between the agent and tools (e.g. the agent triggers a sample simulation and reads results). By Month 6, we aim to have a basic working skeleton of the framework with hardcoded sequences – essentially a **proof-of-concept** integration (without yet the learning/adaptive intelligence). _Milestone:_ Architecture diagram and prototype demo where agent goes through one TEA-LCA cycle for a simple process.
    
- **Months 6–12: Tool Integration and ML Model Development.** In this phase, heavy development occurs on two fronts: (a) **Tool Integration:** We will fully integrate BioSTEAM (or Aspen) with Brightway LCA in our software. This involves writing wrapper functions or using API bindings so that the agent can run simulations and retrieve LCA results automatically. We’ll test this on a small example (e.g. a simple flowsheet). (b) **ML Surrogate Model Development:** Concurrently, we will start developing surrogate models for the first case study. For instance, if hydrogen production is first, we gather simulation data (perhaps using Aspen or a custom model of electrolysis) and train initial GP and GBM models. We’ll also set up PINN frameworks for any first-principle models identified. During this period, we likely focus on one case to avoid spread too thin – the hydrogen case is simpler (mostly continuous variables) which is a good testbed for GP/BO. By Month 12, we should have the **agent capable of using a GP surrogate and doing a simple Bayesian optimization** on the first case. _Milestone:_ Publication-quality preliminary results for Case 1 (e.g. a conference paper or internal report showing automated optimization of hydrogen process with TEA-LCA outputs).
    
- **Months 12–18: Agent Intelligence and Decision Logic Implementation.** Now that the pieces are integrated, we will work on elevating the agent’s capabilities. This includes implementing the reasoning prompts, decision rules, and fallback strategies for the agent. We’ll incorporate the optimization module (Bayesian optimizer configured, possibly a genetic algorithm or MILP solver if needed for discrete decisions). We also enhance the agent’s ability to do multi-objective analysis (maybe by formulating composite objectives or using Pareto ranking logic in its prompt). During this time, we also address scalability and robustness: improving how the agent handles errors (e.g. simulation failures) and how it logs and learns from them. We’ll likely iterate on prompt engineering for the LLM to ensure it understands the outputs correctly (this might involve fine-tuning a smaller model on domain-specific Q&A if needed). Parallel to agent improvements, we initiate **Case Study 2 (Biorefinery)** around Month 15, feeding in knowledge from the first case. That means setting up the biorefinery simulation, defining its TEA/LCA parameters, and letting the agent start exploring it. _Milestone:_ By Month 18, the agent should autonomously handle a full optimization loop on Case 1 and be actively working on Case 2. Possibly submit a journal paper on the hydrogen case study with the framework methodology described.
    
- **Months 18–24: Case Studies Expansion and Refinement.** In this half-year, we will complete Case Study 2 (biorefinery) analysis and commence **Case Study 3 (plastics recycling)**. The framework may need adjustments for each case (for instance, adding the ability to consider discrete technology choices for plastics, or incorporating supply chain modules). We will utilize the ML models heavily here – likely training new surrogates for pyrolysis processes, etc., and maybe integrating external data (like experimental results for pyrolysis yields) to augment the models. The agent’s logic might be refined further to deal with these complex scenarios (like splitting waste flows). We will also during this phase compare the framework results with other approaches: e.g. run a traditional TEA/LCA manually for one scenario as a check, or compare the agent’s chosen optimum with a brute force grid search if feasible. If any discrepancies or surprising decisions are observed, we refine the agent’s rules or retrain the LLM on those specifics (maintaining correctness). This period is also when we measure **performance improvements** – e.g. how many simulations did the agent need vs. a baseline, etc. By Month 24 (end of Year 2), we expect to have all three case studies completed with results interpreted. _Milestone:_ Submit/publish results of Case 2 and Case 3, possibly in journals like _Computers & Chemical Engineering_ or _AIChE Journal_, demonstrating the efficacy of the framework across domains.
    
- **Months 24–30: Evaluation, Documentation, and Writing.** In the final stretch (which could be 6 months, or possibly shorter if things go well), we focus on synthesizing the outcomes, evaluating the framework’s generality and limitations, and writing the Ph.D. dissertation and associated publications. We will perform a **comparative evaluation** of our framework against existing methods (see next section on comparison) using the results from the case studies. This includes quantifying how much time/effort was saved, how close to global optima we got, and any improvements in quality of analysis (like better uncertainty coverage). We will also address any remaining issues in reproducibility and finalize the ethical safeguards (ensuring all dataset licenses are complied with, etc.). By Month ~27, a full draft of the dissertation should be ready, and a defense could be scheduled around Month 30. In terms of Gantt chart, writing overlaps with the analysis of Case 3 and evaluation, as we will not wait to start writing up earlier results. _Milestone:_ Ph.D. thesis completed and submitted by end of Year 2.5, as well as all planned papers submitted.
    

This timeline is aggressive but feasible given the tools and resources we plan to use (leveraging existing open-source platforms and focusing on computational experiments). We have built in some overlap to keep momentum (e.g. starting next case while finishing analysis of prior one) and will adjust as needed. The Gantt chart (Figure 2) shows overlapping bars, indicating parallel efforts in tool development and case applications – a strategy to manage the interdisciplinary nature of the project (AI development in tandem with domain case studies). Regular checkpoints (literature review, prototype, case results, papers) are included to ensure progress can be monitored and guided by the advisory committee.

## Comparison with Existing Automation Efforts in PSA

It is important to benchmark the proposed framework against prior attempts to automate or streamline process systems analysis. **Existing efforts** can be grouped into a few categories: integrated assessment tools, workflow automation scripts, and multi-criteria optimization frameworks. We discuss each and how our approach differs and improves upon them, based on peer-reviewed literature.

**Integrated TEA-LCA Tools:** Some recent works have sought to create holistic tools for combined economic and environmental evaluation. A prime example is the **BioSTEAM-LCA platform**, which integrates process simulation with an LCA module for biorefinery analyses[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=process%20simulation%20software%20BioSTEAM%20,79). BioSTEAM-LCA allows users to define a biorefinery flowsheet and automatically get both TEA and LCA results, enabling “agile” exploration of designs[pubs.acs.org](https://pubs.acs.org/doi/10.1021/acssuschemeng.0c05998#:~:text=...%20pubs.acs.org%20%20BioSTEAM,offs%20across%20dimensions). This is a significant step beyond separate analyses. However, BioSTEAM-LCA still requires the _user_ to manually run simulations, interpret results, and decide on changes – it does not have an autonomous decision-maker. Our framework could be seen as the next evolution: in fact, we plan to leverage BioSTEAM’s capabilities but add an LLM agent on top to orchestrate runs and optimization. Another integrated tool is described by _Yue et al._ (2021) for early-stage carbon capture technologies, which combined TEA and LCA calculations to allow consistent comparisons[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=stakeholders%27%20needs%20and%20an%20escalation,LCA%2C%20TEA%2C%20and%20TRL%20and)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=However%2C%20current%20technology%20assessment%20practice,LCA%2C%20TEA%2C%20and%20TRL%20and). They provided guidelines on doing both analyses together but again, the **automation level is low** – essentially guidelines for analysts, not an autonomous system. The **Global CO2 Initiative’s TEA & LCA Guidelines** (Mueller et al., 2020) also emphasize integrated thinking but require a human to implement the workflow[energy.gov](https://www.energy.gov/eere/iedo/life-cycle-assessment-and-techno-economic-analysis-training#:~:text=Life%20Cycle%20Assessment%20and%20Techno,support%20users%20in%20assessing%20impacts)[nrc-publications.canada.ca](https://nrc-publications.canada.ca/eng/view/object/?id=a53ef77e-88ad-43e3-849b-328004df678d#:~:text=Integration%20of%20LCA%2C%20TEA%2C%20process,but). **What sets our framework apart** is the introduction of an _agentic automation layer_. Rather than a static tool, we have an AI that actively drives the analysis. This means our system can do things like _automated sensitivity analysis_ or _automatically find optimal conditions_, which existing integrated tools do not do by themselves. As evidence of this gap, Köck et al. (2023) in their review explicitly found no cases of full automation: even when process simulation and LCA are integrated, _“an automated connection between them is lacking”_[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process). They cite examples where partial automation was achieved (like generating LCI from simulation), but no closed-loop optimization. Our framework will fill that gap, providing the missing automated connection and decision-making.

**Workflow Automation and Scripting:** In practice, many engineers write scripts or use Excel macros to speed up repetitive calculations in TEA/LCA. For instance, some studies mention linking Aspen Plus to LCA software via Excel macros to update LCA inputs whenever the simulation is run[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user). There are also publications on automating LCI data gathering using web scraping or databases (to reduce manual data collection)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=processes%20that%20are%20still%20in,and%20can%20improve%20data%20quality)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=match%20at%20L1326%20Process%20optimization,objective%20optimization%29%2C%20with%20the%20purpose). **Automation of Life Cycle Assessment – A Critical Review** by Köck et al. surveyed 30 such methods[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=For%20the%20analysis%2C%20the%20following,research%20questions%20were%20defined)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=2). They found various techniques: e.g., using **ontologies** to map process data to LCI databases, using **text mining** to extract data from literature, and even some **AI** to fill data gaps[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=context%20of%20process%20simulation%20,highest%20level%20of%20maturity%20of). However, these efforts typically target one piece of the puzzle (like automating data collection or automating multi-objective optimization) but _not the entire PSA process_. For example, _Ng et al._ (2020) implemented a Visual Basic tool “SustainPro” that connected a process simulator, an economic analysis module, and a sustainability database to do automated data transfer and eco-efficiency calculations[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=match%20at%20L1374%20sustainable%20design,and%20consists%20of%20various%20tools). While innovative, that tool was limited to certain indicators and required pre-defined models (not an AI exploring new models). Another example by _Martin et al._ (2019) linked gPROMS, MATLAB, and Umberto LCA by writing custom code including a virtual mouse to click through Umberto’s interface[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user). This underscores how **laborious and case-specific** current automation scripts are. Our agent framework, by contrast, is designed to be _flexible and general_. By using an LLM that can interpret goals and outputs, we aren’t hardcoding a fixed sequence for one problem – the agent can adapt to different processes and objectives. The framework aims to serve as a general research assistant for PSA tasks, reducing the need for one-off scripting. Moreover, our inclusion of **ML surrogates** is a key differentiator: existing scripts typically still rely on brute-force simulation runs or user-defined scenario grids. We employ intelligent surrogates to dramatically reduce the computational load, which is rarely seen in prior automation efforts. One recent perspective in _Nature_ by Bran et al. (2024) noted that automation levels in chemistry remain low partly due to lack of data and experimental nature[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=generation33%20%2C%2061%2C%20materials%20design35,lack%20of%20data%20and%20the). They suggest augmenting AI with tools to raise automation[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=Large%20language%20models%20,Our) – exactly what we do by giving the LLM multiple tools (simulator, LCA, ML). Thus, relative to workflow scripts, our approach is _more intelligent, less brittle,_ and expected to scale to more complex analyses without exponential effort.

**Multi-Objective Optimization Frameworks:** Another line of related work involves using optimization algorithms to tackle process design with environmental and economic objectives. For example, **multi-objective evolutionary algorithms (MOEAs)** have been applied to generate Pareto fronts of cost vs. emissions. _Zhang et al._ (2016) might use NSGA-II on a process model to minimize cost and GWP. Likewise, **mathematical programming** approaches (MILP/MINLP) can handle combined economic/environmental objective by embedding LCA data into the constraints/objective[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=Process%20optimization%20enables%20the%20simultaneous,objective%20optimization%29%2C%20with%20the%20purpose). A noteworthy example: _Zhang et al. (2022)_ integrated LCA calculations into a gPROMS optimization for biofuel process design, but had to connect the tools via clunky means[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user). Typically, these approaches show that including LCA in optimization changes the optimal solution, but they often assume fixed designs and don’t incorporate AI or adaptive learning. Our framework indeed performs multi-objective optimization but with a different flavor – using an AI agent and surrogates rather than a monolithic solver. The advantage is that the agent can _mix techniques_: it could use MOEA if needed (we can incorporate DEAP or Platypus libraries for evolutionary algorithms), but it can also apply engineering heuristics or filter out obviously bad solutions before optimization, something an MOEA alone wouldn’t do. Moreover, optimization studies often miss the interpretability and interactive exploration that our agent provides – they output a Pareto set but don’t explain _why_ certain designs are optimal. Our LLM agent, conversely, can provide natural language explanations for the trends it finds (e.g. _“Increasing temperature beyond X lowers yield, raising cost and emissions – hence the Pareto front flattens out.”_). This explanation capability, enabled by the LLM’s language core, is a novel benefit not present in prior frameworks. In summary, while prior multi-objective optimization work did automate part of the PSA (running many scenarios via algorithms), our approach **combines that computational power with AI-driven reasoning**, yielding both results and understanding.

A direct **head-to-head comparison** will be done in the project (and described in the dissertation) for a representative case: for instance, we could take the plastics recycling case and apply a standard MOEA with 1000 simulations, then run our agent with GP + Bayesian optimization. We anticipate our agent finds the optimum with far fewer simulations (thanks to surrogate guidance) and with comparable accuracy. Additionally, our agent can incorporate **qualitative factors** (like if an option is operationally complex or not scalable – it could be encoded as a penalty or a reasoning step), which pure optimizers can’t do easily. By citing peer-reviewed studies in optimization (e.g. the Computer Aided Chemical Engineering conference papers or AIChE journal articles on surrogate-based optimization), we back the claim that combining these methods yields better efficiency[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations)[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=simulations,a%20conservativeness%20parameter%20to%20enable).

In conclusion, **no existing approach in literature fully replicates what we propose**: a unified LLM-agent orchestrating TEA, LCA, ML surrogates, and optimization in a closed-loop manner. We improve upon integrated tools by adding autonomy and intelligence; we generalize beyond one-off scripts by using an adaptable AI agent; and we enrich optimization-driven analysis with explainability and hybrid techniques. This positions our framework as a **first-of-its-kind contribution** at the intersection of process systems engineering and AI. The anticipated result is not to replace prior methods, but to _encompass and extend them_: our agent can utilize mathematical optimization, can interface with existing tools like BioSTEAM, and can implement guidelines from TEA/LCA frameworks – but it packages these in an automated workflow that reduces the burden on the engineer and can potentially discover better solutions faster.

We will ensure to communicate these comparisons in publications: for instance, a table comparing features (Integration: ours=full automation vs others=partial; ML surrogates: ours=yes vs others=no; Decision-making: ours=AI agent vs others=user or fixed algorithm; Adaptability: high vs low). Such a summary, backed by references like the MDPI automation review[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user) and the integrated framework reviews[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=stakeholders%27%20needs%20and%20an%20escalation,LCA%2C%20TEA%2C%20and%20TRL%20and), will convincingly show the novelty and significance of our approach relative to the state-of-the-art.

## Expected Contributions

The proposed research will yield several significant contributions across different dimensions, which we detail below:

- **Methodological Contributions:** We will develop a **novel methodology for unified TEA-LCA analysis** using AI. This includes formulating how an LLM agent can be applied to orchestrate engineering assessments, which is a new concept. We introduce a methodology to incorporate _machine learning surrogates into the techno-economic and environmental evaluation loop_, something that is currently done ad hoc if at all. The framework effectively establishes a new paradigm for PSA – one that is **agent-centric and data-enhanced** – moving away from the traditional sequential manual analysis. This methodology can be generalized beyond our case studies to any process where multi-objective analysis is needed. Additionally, we contribute a blueprint for **agent-guided experiment planning in process simulation** (analogous to how self-driving labs plan physical experiments, here we plan simulation experiments). Methodologically, this bridges the gap between process systems engineering and artificial intelligence, creating a foundation for future research on “self-driving” process optimization or AI-assisted sustainability analysis.
    
- **Technological Contributions:** The project will result in a **prototype software platform** that integrates an LLM agent with chemical engineering software tools. This can be viewed as an early version of a “digital assistant” for process engineers. Technologically, we are extending the capabilities of existing tools: for example, enhancing BioSTEAM or Aspen via our agent layer, and enhancing Brightway2 LCA by linking it with AI. We will likely produce re-usable code, such as an API for sending and receiving data from Brightway, or wrappers for controlling simulators with natural language prompts. These can be open-sourced for the community. The **system architecture** (Figure 1) itself is a contribution – it shows a new way to architect complex engineering software using an AI in the loop. Also, the trained **machine learning models** (GPs, PINNs, etc.) for our case studies can be valuable artifacts for others working on those processes. For instance, a GP surrogate for hydrogen production or a PINN for a pyrolysis reactor kinetics could be published alongside to help future researchers. In summary, we provide a concrete _technology demonstrator_ of AI-agent driven process analysis, potentially inspiring more advanced versions or commercial implementations down the road (e.g. an “AspenAdvisor” agent that companies might develop, similar to GitHub’s Copilot for coding[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=,like%20basic%20mathematics%20and%20chemistry)[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=of%20GitHub%20Copilot%20in%2020216,3.5%20%28ref)).
    
- **Practical Contributions:** From an industrial and practical standpoint, the framework (if successful) can **dramatically reduce the time and effort** required to perform techno-economic and environmental assessments. This means faster turnaround in evaluating new ideas (e.g. new biorefinery concepts or recycling systems) which is crucial in the context of climate change solutions that need rapid assessment. For practitioners, having an agent that can automatically identify cost or emission hotspots and suggest improvements can augment decision-making. Our case study results will provide _actionable insights_ for the domains considered. For example, we might concretely show the cost-emission trade-off for green hydrogen under various scenarios, or the best combination of recycling methods for a given waste stream – insights that could inform policy or R&D focus. Furthermore, by emphasizing explainability and ethical considerations, we ensure the practical outputs are trustworthy. The framework could be used as an educational tool as well, teaching students how changes in a process affect economics and environment in an interactive way (similar to how some educational packages exist, but here with AI guidance). Practically, we also address the **reproducibility** problem by having everything logged and automated: companies and researchers could replicate our agent’s analyses for their own data, leading to more consistent comparisons between studies.
    
- **Theoretical Contributions:** On a more theoretical level, the research will contribute to the understanding of **how AI agents can reason about engineering systems**. We will explore how well LLMs can be guided to do arithmetic with uncertainty, to follow domain constraints, and to converge to solutions in complex search spaces. This will generate knowledge on the capabilities and limits of current AI (GPT-4, etc.) in the chemical engineering context – effectively testing their knowledge base on engineering and their ability to use external tools accurately. We anticipate learning about the _emergent behaviors_ of such agents – for example, does the agent come up with creative solutions or does it get stuck in loops? This can contribute to the literature on human-AI collaboration and AI in science. Additionally, integrating first-principles physics (via PINNs) with data-driven learning and AI planning provides a case study in unifying different modeling paradigms (symbolic, data-driven, mechanistic). The theoretical framing of how to balance **exploration vs. exploitation** in simulation-based optimization through an agent could be seen as a contribution to the field of Bayesian experimental design, but applied in silico. We might formalize an approach for _agent-based global optimization_ where the agent’s policy is essentially doing a form of reinforcement learning on the space of possible experiments to run – contributing to AI theory as well. Lastly, our approach touches on **sustainability science** theory: by treating environmental impact as an objective to be optimized alongside economic performance, we reinforce the concept (and practicality) of multidimensional optimization for sustainable design, providing a tangible framework that could encourage further theoretical developments in sustainability optimization (like new composite metrics or constraint handling techniques).
    

In summary, the contributions span: a **new integrated methodology**, a **software/technology artifact**, **practical insights and efficiency gains**, and **cross-disciplinary theoretical knowledge**. We will emphasize these in the dissertation and publications. For instance, one paper might focus on the methodological innovation and case results (contribution to engineering), while another could focus on the AI/ML aspect (contribution to AI for science). By clearly delineating contributions, we ensure the novelty is apparent: **no previous work has delivered this combination of capabilities**, so each aspect we succeed in will be a noteworthy advancement. We expect the _methodological and technological contributions_ to have immediate impact (others can adopt our approach for their processes), and the _practical and theoretical contributions_ to have longer-term impact (influencing how people think about and approach sustainable process design, and how AI is utilized in engineering research).

## Ethical and Reproducibility Considerations

Employing AI-driven automation in engineering analyses raises important ethical and reproducibility issues, which we will proactively address in this project:

**Transparency and Explainability:** One concern is that an LLM agent could become a “black box,” making decisions or recommendations without clear justification. This is problematic in engineering where trust and verification are paramount. To ensure explainability, our agent will be designed to **provide explanations for its actions**. Thanks to the nature of LLMs, we can prompt the agent to output reasoning steps (chain-of-thought) that justify each decision (e.g. why it chooses a certain design to test, or why it prefers one solution over another). We will include a logging mechanism where the agent’s internal reasoning (in natural language) is recorded alongside each result. During development, we will scrutinize these logs to ensure the reasoning is sound and not based on faulty logic or irrelevant correlations. If we find the agent giving unclear or spurious reasons, we can refine its prompt or employ **rule-based checks**. For example, we might hard-code that any final recommendation must be accompanied by a summary: "_The chosen design is optimal because ... (economic reason) ... and ... (environmental reason)_." By making the agent’s thought process visible, we allow human oversight and the ability to explain results to stakeholders, which is essential for ethical deployment of such a system.

**Validation and Oversight:** We will treat the AI agent as an assistant, not an oracle. That means all critical results will be **validated by independent means**. For instance, if the agent finds an optimum, we will re-run that scenario in a controlled manner (outside the agent) to verify the TEA and LCA outcomes match. If the agent uses an ML surrogate, we will cross-check some predictions with the original simulation or known literature values. Essentially, a human-in-the-loop approach will be maintained for final outputs. In an operational setting, one could imagine requiring a human engineer to sign off on the agent’s recommendations – our project will simulate that by the research team thoroughly reviewing agent outputs. Any discrepancies or potential mistakes by the agent (like misreading a unit or a cost) will be caught and corrected, and the agent can be retrained or prompted to avoid such errors. This addresses the ethical issue of **AI errors** – by not having blind trust and by building in verification steps. In terms of oversight, we also commit to using the agent within the bounds of its competence: the agent will be restricted to using the tools and data we give it, and not venturing opinions on things outside (since LLMs can hallucinate). For example, we won’t ask the agent a question like "Is this process safe?" unless we have provided it with data or models to evaluate safety, because otherwise it might guess. We focus the agent’s autonomy on the well-defined tasks of running simulations and calculations.

**Data Privacy and Licensing:** Our framework will utilize datasets such as LCI databases (ecoinvent, etc.), material property databases, and possibly published process data. Many of these have licenses (ecoinvent, for example, requires a paid license for full access). We will ensure that any database we use is properly licensed for our research use. Moreover, if the LLM agent has access to these data, we must ensure it does not inadvertently reveal proprietary data in its outputs. We will handle this by controlling the information flow: the agent will use database values internally for calculation but will not be asked to regurgitate large swathes of database content. Any data (like emission factors) that end up being reported will be properly cited to the source (as is standard in LCA reporting). As for _training data for the LLM_: we will likely use a pre-trained model (like GPT-4 via an API) – we do not anticipate needing to fine-tune it on sensitive data. If we did fine-tune or train a smaller model on e.g. literature text, we would only use publicly available, properly cited documents (peer-reviewed papers, which we have been citing throughout). In short, we will respect intellectual property: using open-source tools (Brightway2, etc.), open data where possible, and when using commercial data, making sure not to expose it improperly.

**Reproducibility:** Reproducibility is a core principle we uphold. To that end, we plan to **open-source the code** developed (except where certain licensed databases might preclude distributing the data). The model of our framework is that results are fully determined by the input assumptions and random seeds (for ML training). We will document all such assumptions so that another researcher can replicate the study. For example, if we use a specific version of ecoinvent for LCA or a certain Aspen version, we’ll note that. Additionally, by automating the process, we ironically make it more reproducible – the exact sequence of steps the agent took is logged, so one can replay those steps with the same agent version to get the same result. Compare this to a human analyst doing TEA/LCA; it’s easy to make slight tweaks that aren’t recorded. Our system will provide a traceable record. We will also address **uncertainty and variability**, as reproducibility isn’t just exact repetition but also understanding result robustness. The agent can be used to perform Monte Carlo analyses (e.g. varying input parameters within distributions), improving the statistical robustness of our conclusions. We’ll incorporate that where feasible (for instance, Brightway2 can do Monte Carlo LCA runs; the agent could automate 1000 runs for uncertainty analysis). All random processes (like ML model initialization) will be run with fixed seeds when comparing approaches, or multiple seeds for averaging if needed, to give a fair and reproducible performance assessment.

**Bias and Objectivity:** AI systems can inadvertently introduce bias. In our context, bias might mean the agent favors certain solutions due to how it was prompted or due to biases in the data (for example, if the training data of the LLM had a lot of content about a particular technology being favorable). To mitigate this, we will keep the agent’s guidance factual and data-driven. The agent doesn’t rely on prior training knowledge to make final calls – it uses actual TEA/LCA calculations. We mainly rely on the LLM for its reasoning and language capabilities, not for domain facts (which we provide via the integrated models). This reduces the risk of it injecting irrelevant biases. Nonetheless, we will watch for any odd preferences the agent shows. If, say, the agent consistently picks a solution that is slightly suboptimal numerically, we’ll investigate why. It could be a bias or just a suboptimal decision; either way, we can correct it by adjusting the reward structure or constraints (like making sure the agent’s objective truly reflects what we want). Another angle is **ethical bias** – for example, if our agent had to consider social or safety factors beyond cost and emissions. Right now, our scope is cost and environmental impact, which are quantifiable. If we extended to something like social impact, we’d have to be careful how the agent measures that. We will for now avoid subjective criteria, but mention them in passing if relevant (the proposal mentions possibly including social LCA considerations – though not a focus, we are aware of that dimension and would treat it similarly via data not AI judgment).

**Environmental and Social Impact of the Research:** There is an irony that using large models like GPT-4 has a carbon footprint itself. While beyond the main scope, we acknowledge that training large ML models is resource-intensive. Our use of an existing model via API means we are not adding significant training footprint, and the benefit of optimizing processes could far outweigh the computational cost of the agent’s runs (especially if we help design more efficient processes). We will keep this perspective in communicating results – e.g. if someone argues using AI is costly, we can show that if we reduce a plant’s emissions by far more, it’s a net positive. Socially, by automating tasks, one could fear job impacts for analysts. We view it as augmenting engineers – the AI takes the grunt work, freeing engineers to do higher-level creative and verification tasks. We will ensure to highlight that the human expert remains crucial in the loop (our agent is an assistant). This framing is ethically important to prevent misapplication (we wouldn’t want a company to blindly deploy an AI agent to design a plant with no human review!).

**Provenance and Dataset Documentation:** All datasets used (e.g. cost data sets, yield data from literature) will be documented, and if we create new datasets (like simulation results compiled by the agent), we will consider publishing them as supplementary material. Each figure or conclusion in our papers will be traceable to either literature or our dataset generated by the described method (the citation style we use ensures that, as demonstrated in this proposal with sources like[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S092134492400689X#:~:text=,MPW%29) backing statements). This level of detail ensures not just reproducibility but **accountability** – others can check our sources or rerun our simulation files (which we plan to provide). We will also abide by LCA standards for transparency: listing key assumptions like boundaries, functional unit, impact assessment method, etc., so that our LCA results can be contextualized and compared.

In conclusion, the research will incorporate a robust ethical compass: **transparent AI, human oversight, proper data use, and replicability** are at its core. We will regularly consult the university’s guidelines on research ethics (especially in AI) to ensure compliance. Our goal is not only to produce a powerful framework, but one that stakeholders can trust and adopt responsibly. By addressing these considerations in the proposal and execution, we demonstrate foresight and responsibility – traits crucial for a Ph.D. project aiming to break new ground in an ethical manner.

## Conclusion

This Ph.D. proposal has outlined a comprehensive plan to develop an AI-agent driven framework that integrates machine learning, large language models, and process systems tools for automated process systems analysis. We began by identifying the significant **limitations in current TEA and LCA practices** – fragmented analyses, heavy manual effort, and methodological misalignments – all supported by peer-reviewed studies. We then highlighted the clear **gap in unifying TEA, LCA, ML, and LLMs**: despite advances in each area, no existing approach ties them together into a cohesive, autonomous system. To fill this gap, we proposed an innovative **system architecture** wherein an LLM-based agent orchestrates process simulations, LCA computations, predictive ML models (GPs, PINNs, GBMs), and optimization algorithms in a closed loop (Figure 1). We detailed how this agent will function – planning experiments, querying surrogates, and iteratively improving designs – effectively automating and accelerating PSA beyond the current state-of-the-art. The framework will be tested on **multiple case studies** (hydrogen, biofuels, plastics recycling), demonstrating its versatility and its ability to navigate complex techno-economic and environmental trade-offs in each domain. A realistic **timeline** with milestones (Figure 2) was presented, showing how the project will progress over ~2–2.5 years, culminating in a working prototype and validated results published in high-impact venues.

In comparing our approach to existing efforts, we found that while there are initial steps toward automation in PSA, our framework offers a **qualitatively new level of intelligent integration**, leveraging techniques and technologies not previously combined. The **contributions** of this research will be significant: a new methodology for AI-assisted sustainable process design, a software tool that embodies this methodology, practical insights for important industrial domains, and cross-disciplinary knowledge bridging chemical engineering and AI. We also addressed **ethical and reproducibility issues** head-on – committing to an explainable, validated, and transparent use of AI that complements human expertise rather than replaces it, and ensuring that our results can be trusted and replicated by others.

Ultimately, this research aims to transform how process systems analysis is conducted: from a slow, segmented, human-centric endeavor to a fast, integrated, and AI-augmented one. By doing so, it will help identify better solutions faster for pressing challenges like low-carbon fuels, clean hydrogen, and circular plastics. These improvements align with global sustainability goals and industrial needs, making the work both timely and impactful. The proposal is aligned with the expectations of a top-tier Chemical Engineering Ph.D. program, combining rigor in engineering analysis with cutting-edge AI innovation. With support from the academic community and validation from the literature, we are confident in the feasibility and value of this project. This framework, upon completion, could serve as a blueprint for future “**autonomous labs**” in process engineering – where AI agents collaborate with simulations and humans to rapidly evaluate and optimize new processes, thereby accelerating the transition from concept to sustainable reality.

**References:** (Selected peer-reviewed sources supporting this proposal’s context and claims)

- Arno W. Zimmermann et al., _Frontiers in Climate_, 2022 – Discusses challenges in early-stage LCA and TEA integration[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=challenges%20and%20collect%20best%20practices,and%20governmental%20literature%20and%20practice)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=Not%20only%20data%20availability%20is,efficiency%20measures%20such%20as%20heat).
    
- N. Wunderlich et al., _Sustainable Energy & Fuels_, 2021 – Reviews methodological challenges in TEA-LCA integration[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments).
    
- Bianca Köck et al., _Sustainability_, 2023 – Critical review of LCA automation in inventory analysis[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user).
    
- Andres M. Bran et al., _Nature Mach. Intell._, 2024 – ChemCrow LLM agent for chemistry, demonstrating tool integration with GPT-4[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=Large%20language%20models%20,Our)[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=augments%20the%20LLM%20performance%20in,between%20experimental%20and%20computational%20chemistry).
    
- Samrity Jalota et al., _Innovative Infrastructure Solutions_, 2025 – Review of ML in LCA, noting common use of ANNs and highlighting opportunities for deeper ML integration[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=ML%20models%20have%20several%20merits,Despite%20these%20challenges%2C%20there)[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=This%20review%20analyzed%2040%20articles,to%20address%20environmental%20sustainability%20challenges).
    
- Rogers et al., _Fuel_, 2021 – ML predicted economic and environmental impact of H₂ from supercritical gasification (example of ML aiding TEA-LCA)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=technology%20to%20investigate%20the%20relationship,environmental%20impact%20of%20hydrogen%20production)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=,the%20literature%20to%20build%20a).
    
- Eriksen et al., _Environ. Sci. & Tech._, 2020 – Found mechanical recycling of plastics has lower impacts than chemical, but acknowledged cost trade-offs[sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S092134492400689X#:~:text=,MPW%29).
    
- Durkin et al., _Comp. Aided Chem. Eng._, 2022 – Used Gaussian Processes for simulation-based optimization, illustrating surrogate+optimization approach in chemical engineering[kclpure.kcl.ac.uk](https://kclpure.kcl.ac.uk/portal/en/publications/gaussian-processes-for-simulation-based-optimization-and-robust-d#:~:text=Gaussian%20Processes%20present%20a%20versatile,converged%20simulations).
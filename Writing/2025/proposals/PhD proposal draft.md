
 Agentic Machine-Learning Framework for Autonomous Process Systems Analysis Integrating Techno-Economic Analysis and Life Cycle Assessment


key words: low TRL, Generative AI, Machine learning, 
Large Language Models (LLMs)
Low-Rank Adaptation (LoRA) for low
during the early stages of the design process
pipeline
frameworks: langchain, CrewAI
LLM augumented LCA

ML applications
conventional ML predictive AI vrs generative AI
define LLM
SWOT on LLM 
open source vrs closed sources

function calling and multi-step reasoning
Agent architecture
Custom model training through LoRA, low end
Application
the four phases of LCA and its application
## 1. Introduction and Problem Statement
Process systems analysis (PSA) plays a critical role in evaluating the viability and sustainability of chemical processes through **techno-economic analysis (TEA)** and **life cycle assessment (LCA)**. TEA assesses the economic performance (e.g. costs, profitability) of a process or product, while LCA quantifies environmental impacts across the process life cycle. TEA and LCA provide complementary insights for decision-makers but require extensive data collection and computation, which is often a limiting factor especially for emerging technologies.
Most studies frequently rely on ad-hoc spreadsheet models, specialized tools and human expertise, making it difficult to rapidly iterate designs or integrate new data. 
Current PSA methods face limitations in terms of integration, efficiency, and scope. @kockAutomationLifeCycle2023 Recent advances in machine learning (ML) and artificial intelligence (AI) – notably **large language models (LLMs)** – present an opportunity to **automate and accelerate PSA** beyond the conventional paradigm.
This proposal outlines a research plan to develop a novel LLM-driven agentic framework that orchestrates process simulation tools, TEA/LCA models, and predictive ML in a unified system. The goal is to **streamline and enhance TEA-LCA workflows** using AI agents, enabling faster, more comprehensive process evaluations. 


The proposed research will 
(1) characterize the limitations of current TEA-LCA methods from literature, 
(2) identify gaps in integrating TEA, LCA, LLMs, and ML, 
(3) design an improved system architecture (with AI agent logic, tool orchestration, and data flows), 
(4) implement predictive models (Gaussian processes, PINNs, gradient boosting) alongside process simulators (e.g. Aspen Plus or BioSTEAM), LCA software (Brightway2/OpenLCA), and optimization techniques (Bayesian optimization, reinforcement learning), 
(5) validate the framework on multiple case studies (such as hydrogen production, biorefineries, plastics recycling) with coupled TEA-LCA metrics, 
(6) establish a timeline with milestones (including a Gantt chart), 
(7) compare our approach with existing PSA automation efforts, 
(8) highlight key contributions, and 
(9) address ethical and reproducibility considerations. 
By leveraging only peer-reviewed sources from high-impact journals throughout, we ensure the approach is grounded in state-of-the-art knowledge. 


## Limitations of Current PSA Methods in TEA and LCA

Conventional TEA and LCA are often performed in _siloed workflows_, using different tools and assumptions, which makes integrated analysis challenging. The  economic perspective and the lif-cycle perpective often habour different system boundaries and functional units. Literature reviews show that differing system boundaries and perspective biases can lead to _conflicting conclusions_, unless carefully harmonized[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20Goal%20and%20scope%20,However%2C%20the%20selection%20of)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20The%20inventory%20,can%20have%20monetary%20impacts%2C%20such). **Norris (2001)** highlighted the need to integrate economic implications into LCA and vice versa, yet also noted difficulties in interpreting combined results[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=remaining%20gaps%20in%20literature%20Norris,TCA%20aims%20at%20including%20often)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=life%20cycle%20assessment%20results%2C%20companies,environmental%20results%20is%20not%20presented). This **lack of integration** can cause inconsistencies – e.g. using a different benchmark or feedstock in TEA vs. LCA yields incomparable outcomes[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=benchmarks%20for%20comparison%20within%20each,benchmarks%20need%20to%20be%20identical). Azapagic et al. (2006) found that if economic and environmental benchmarks differ (cheapest vs. greenest alternative), the integrated assessment can send mixed signals[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=benchmarks%20for%20comparison%20within%20each,of%20the%20technology%2C%20the%20benchmarks). 

One challenge with integrated systems analysis is the methodological differences. TEA output units (e.g. USD) cannot be directly aggregated with LCA outputs (e.g. kg CO₂) without subjective weighting or conversion (as in single-score sustainability metrics)[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=%EF%82%B7%20The%20impact%20calculation%20,impacts%20are%20often%20considered%20static). This makes interpreting integrated TEA-LCA results difficult – studies have noted that combining economic and environmental indicators requires careful normalization or multi-criteria analysis, often lacking standard guidelines[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments). **Wunderlich et al. (2021)** reviewed 70 papers and found an _“increasing number of contributions”_ on TEA-LCA integration, but _no commonly followed definition_ or framework for doing so[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments). The authors identified a **knowledge gap** in how to practically integrate TEA and LCA from the ground up, as most studies used case-specific or qualitative integration methods[eprints.whiterose.ac.uk](https://eprints.whiterose.ac.uk/id/eprint/208376/1/Wunderlich_etal_Integration_2021.pdf#:~:text=dimensions,required%20information%20from%20additional%20assessments). This gap leaves practitioners without clear methodological guidance. 
Another challenge is **uncertainty propagation**. Early-stage process data are often sparse and uncertain, which cascades through both TEA and LCA. _Zimmermann et al. (2022)_ point out that at low technology readiness levels, many data are missing or not representative of future scales[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=match%20at%20L694%20A%20central,not%20yet%20measured%20or%20modeled)[frontiersin.org](https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2022.841907/full#:~:text=A%20central%20challenge%20when%20assessing,not%20yet%20measured%20or%20modeled). This leads to high uncertainty in both cost and impact estimates. Conventional approaches handle this via scenario analysis or sensitivity analysis, which are labor-intensive to do comprehensively

Today’s PSA practice is **labor-intensive, slow, and non-autonomous** – hindering our ability to quickly evaluate new ideas or identify optimal designs under both economic and environmental criteria. Conducting an LCA involves four phases (goal/scope, inventory analysis, impact assessment, interpretation) per ISO 14040/44, and the _inventory phase (LCI) is extremely time- and resource-intensive_[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=processes%20that%20are%20still%20in,and%20can%20improve%20data%20quality)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=and%20can%20improve%20data%20quality). Collecting input/output data for potentially **thousands of unit processes** is painstaking[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=,9)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=be%20collected%20for%20the%20parameters,129). Köck et al. (2023) note that even for a single LCA, “_thousands of unit processes are usually used_” requiring large volumes of data from lab experiments, databases, and literature[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=As%20the%20most%20time,and%20uncertainty%20distribution%20should%20also)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=,9). This complexity often _deters iterative use_ of LCA during process design; instead, LCA is done after a process design is fixed[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=To%20date%2C%20chemical%20process%20simulations,integration%20of%20LCA%20in%20process). Similarly, TEA is frequently done via custom spreadsheets or process simulator economic reports, which must be updated by hand when the design changes. **Integrating LCA with process simulation is possible** (e.g. via add-ons or exporting data), but typically not automated. A review of LCA automation efforts found that while some tools link LCA software with process simulators, they often require _ad hoc scripting or even GUI trickery_ (e.g. a “Java robot mouse emulator” to transfer data between programs)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user)[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=framework%20integrates%20LCA%20and%20process,when%20integrated%20with%20the%20process). For example, Mutel et al. connected gPROMS process simulator to Umberto LCA software by using Excel as an intermediary and simulating user input because no direct API was available[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=This%20builds%20an%20automated%20connection,emulator%2C%20which%20simulated%20manual%20user). These kinds of solutions underscore the lack of seamless integration – current workflows resemble a patchwork of _“glue” code and manual steps_ prone to error. One study described that to optimize a design considering LCA and TEA, _designers had to go back-and-forth iteratively_, manually adjusting the process and re-running analyses until some compromise was found. This trial-and-error approach is **computationally and cognitively challenging**, and may miss optimal solutions.

The following gaps highlight where innovation is needed to overcome these limitations.






Recent reviews highlight a pressing need for automation and integration in PSA. Köck _et al._ (2023) found that automating data flow between process simulations and LCA can “make integration of environmental impacts into decisions easier, less time-consuming, and improve data quality”.  However, existing efforts at PSA automation remain limited. Automation has been implemented mostly via custom interfaces linking simulators to LCA databases, or via machine learning (e.g. neural networks for predicting molecular properties). Fewer than one-third of published methods share open code, and only 10 of 30 surveyed papers included uncertainty analysis @kockAutomationLifeCycle2023 – indicating that current solutions are often proprietary, not reproducible, and may neglect uncertainty quantification.

Research Gap

No existing framework unifies TEA, LCA, large language models (LLMs), and predictive machine learning in an _autonomous, agentic system_ for process analysis. Tools like **BioSTEAM** (an open-source process simulation and TEA platform) and its LCA extension have demonstrated the value of tighter integration – e.g. BioSTEAM-LCA automates early-stage TEA-LCA for biorefineries, enabling simultaneous economic and environmental evaluation under uncertainty.
 BioSTEAM integrates uncertainty into simulations to _“streamline and automate early-stage technology evaluations”_, providing rigorous sensitivity analyses. It _“enables the integration of design, simulation, TEA and LCA to improve consistency and transparency”_ of sustainability metrics
@cortes-penaBioSTEAMBiorefinerySimulation2020; @cortes-penaBioSTEAMFastFlexible2020 . Yet, even these advanced tools do not incorporate the **agentic intelligence** now possible with AI. Large Language Models have recently shown remarkable ability to plan and execute complex tasks autonomously in scientific workflows



**2. Background and Related Work**

Machine learning has been applied to expedite process simulations and optimize designs. Gaussian Process (GP) regression is widely used to create surrogate models of expensive simulation outputs, providing fast predictions with uncertainty estimates – a cornerstone of Bayesian optimization. Physics-Informed Neural Networks (PINNs) have emerged as a way to embed physical laws into neural network models, enabling surrogates that respect conservation laws or known kinetics, thus improving extrapolative reliability. Gradient boosting machines (GBMs) and other ensemble methods are also popular for their efficiency in approximating complex relationships with modest data. In process engineering, such surrogates have proven valuable. For example, GP and neural network surrogates have been used to approximate flowsheet performance, allowing rapid evaluation in optimization algorithms (e.g. in biorefinery superstructure optimizations[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=,refinery%20structure.)).
Despite these benefits, integrating surrogate modeling into a general PSA workflow is non-trivial. 

The advent of large language models has unlocked new paradigms for automation. LLM-based agents can interpret high-level instructions, generate plans, call software tools, and self-correct based on feedback.
In chemical engineering and related fields, this concept is only beginning to be explored.
Vyas & Mercangoz (2025) introduced an _“agentic framework for industrial automation”_ using LLMs for both discrete planning and continuous control
Another study by Zhihan Liu _et al._ (2024) developed an Autonomous Simulation Agent (ASA) that _“automates the entire simulation research process”_ using LLM prompt engineering[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=,4o). Given a research goal, their ASA designs experiments, runs simulations on remote servers, analyzes data, and compiles reports – iterating up to 20 cycles without human intervention[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=experimental%20processes%20and%20computational%20simulations,flawless)[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=execution%20on%20designated%20research%20missions%2C,local%20attention%20and%20global%20oversight). It achieved near-flawless execution on a polymer simulation case, showcasing reliability of LLM agents in long-horizon tasks[arxiv.org](https://arxiv.org/abs/2408.15512#:~:text=of%20polymer%20chain%20conformations%20as,validation%20mechanisms%2C%20and).

Beyond static optimization, reinforcement learning (RL) has been explored for process synthesis and decision-making. Gao _et al._ (2023) showed that RL algorithms can _“learn to build process flowsheets”_ by sequentially adding unit operations and connections, in tandem with a process simulator (DWSIM) providing rewards[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=learn%20to%20build%20process%20flowsheets,that%20stores%20knowledge%20gained%20while). 
A challenge noted is the heavy computational demand: an RL agent may require thousands of simulator calls, so researchers often resort to simplified “shortcut” models to train the agent, sacrificing accuracy[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=learn%20to%20build%20process%20flowsheets,that%20stores%20knowledge%20gained%20while). Gao _et al._ addressed this via transfer learning, pre-training the RL agent on easier models and then fine-tuning on the rigorous simulator, which yielded an economically feasible flowsheet with 8% higher revenue while halving learning time[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=RL%20agent%20demands%20numerous%20process,process%20design%20and%20apply%20it)[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=separation%2C%20and%20recycles%2C%20our%20method,can%20be%20reduced%20by%20a).

ChemCrow’s success in “bridging the gap between experimental and computational chemistry” by an agent that can read databases, run calculations, and suggest experiments demonstrates the _potential of LLM agents_ for scientific workflows[nature.com](https://www.nature.com/articles/s42256-024-00832-8?error=cookies_not_supported&code=24276912-1dde-4eed-b6f5-bb2bd3dc5a5b#:~:text=augments%20the%20LLM%20performance%20in,between%20experimental%20and%20computational%20chemistry). Similarly, in architecture, recent work integrated ML and LLMs to assist low-carbon design: an LLM (ChatGPT) was used to suggest material substitutions to reduce a building’s carbon footprint by querying emission factor databases[mdpi.com](https://www.mdpi.com/1996-1073/17/12/2997#:~:text=match%20at%20L888%20found%20a,3%7D%3B%20straw%20insulation%3A%2012.31)[mdpi.com](https://www.mdpi.com/1996-1073/17/12/2997#:~:text=match%20at%20L1093%20Case%20study,driven%20suggestions%20to). These examples highlight that LLMs, when augmented with tool access, can perform _meaningful autonomous analyses_ in technical domains. However, in the **chemical process design/analysis literature**, we find **no example of an LLM-driven PSA orchestration**. This is a striking gap given the complexity of coordinating simulations, databases, and optimization – tasks that an intelligent agent is well-suited for. The proposed framework will be one of the first to explore **LLM agents for orchestrating chemical engineering tools**, filling this gap and potentially redefining how PSA is conducted.


Another gap lies in the **integration of predictive machine learning models** with TEA-LCA. While ML has been increasingly applied in related areas (for instance, to predict properties or optimize processes), its use _within TEA/LCA studies is still rare_. A recent review identified 40 studies combining ML with LCA, finding ML mainly used to predict life cycle inventory data or impact indicators, and noted that _“deep integration of ML into LCA”_ is an important future direction[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=ML%20models%20have%20several%20merits,Despite%20these%20challenges%2C%20there)[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=This%20review%20analyzed%2040%20articles,to%20address%20environmental%20sustainability%20challenges). The review observed challenges such as limited training data, lack of uncertainty analysis, and few guidelines – indicating that **ML in LCA is still in nascent stages**[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=are%20also%20challenges,establishing%20model%20evaluation%20reporting%20procedures)[bohrium.com](https://www.bohrium.com/paper-details/a-review-of-machine-learning-applications-in-life-cycle-assessment-studies/949130074051838066-2971#:~:text=This%20review%20analyzed%2040%20articles,to%20address%20environmental%20sustainability%20challenges)

The gap is essentially that **no existing framework combines**: (a) **TEA & LCA integration**, (b) **LLM agent orchestration**, (c) **predictive ML surrogates**, and (d) **automated optimization** into one cohesive system. Our proposed research directly addresses this gap by developing a _unified agentic system_ that leverages each of these elements.

## **3. Proposed Framework and Methodology**

**3.1 Architecture Overview:** The system will be built around an LLM-driven _agentic orchestrator_ that interfaces with simulation engines, databases, and machine learning modules. **Figure 1** depicts the high-level architecture of the proposed framework. The central AI agent (implemented via a state-machine approach using the Model Context Protocol, MCP) serves as the coordinator, receiving high-level goals from the user and managing the workflow through various tools and sub-modules.
![[image-2.png]]

_Figure 1: Proposed architecture of the autonomous PSA framework. An LLM-based agent orchestrator (left) communicates with process simulation & TEA tools (top center, e.g. BioSTEAM or Aspen), LCA engines (center, e.g. Brightway or OpenLCA), and an ML toolkit (right) for surrogates and optimization. The agent uses the Model Context Protocol (MCP) to manage tool interactions with well-defined states (planning, executing, reviewing), enabling it to plan experiments, execute simulations, invoke LCA calculations (with background databases), train models, and iterate towards optimal solutions. Dashed lines indicate information flows or user inputs/oversight

At the core of the architecture is the **LLM Agent Orchestrator**, which will be implemented as an AI agent with defined states and transition logic (e.g., using the MCP framework as in Claude’s agent orchestration[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts)). The agent operates through a cycle of states such as _PLANNING_ (devising a plan of action), _EXECUTING_ (calling tools to perform tasks), _REVIEWING_ (evaluating results and checking constraints), and _ERROR handling_[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts). This stateful design ensures the agent’s actions are transparent and logically structured, rather than monolithic black-box decisions. The agent is “agentic” in that it can make decisions to pursue sub-goals autonomously: 

Key components and data flows in Figure 1 include:
- **Process Simulator & TEA Module:** We will use an open-source simulator (BioSTEAM as a primary candidate) to perform process material and energy balance calculations and compute techno-economic metrics (capital cost, operating cost, net present value, etc.). The agent provides _design parameters_ or flowsheet configurations to this module (e.g. operating conditions, equipment sizes, feedstock choices), and receives back simulation results: mass and energy balances, stream flows, utility usage, and itemized costs[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=benchmark%20designs%20modeled%20in%20proprietary,prioritize%20research%2C%20development%2C%20and%20deployment). BioSTEAM’s API will facilitate programmatic modification of process configurations and retrieval of results. Notably, BioSTEAM automates equipment sizing and costing, and can handle uncertainty distributions for inputs[researchgate.net](https://www.researchgate.net/publication/338930278_BioSTEAM_A_Fast_and_Flexible_Platform_for_the_Design_Simulation_and_Techno-Economic_Analysis_of_Biorefineries_Under_Uncertainty#:~:text=through%20its%20fast%20and%20flexible,via), which we will exploit for stochastic analysis.

- **LCA Engine:** For life-cycle assessment, we plan to integrate the **Brightway2** LCA framework[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20an%20open,outputs%20over%20its%20life%20cycle) or alternatively leverage OpenLCA’s backend via its Python API (openLCA provides a JSON-RPC interface for external calls[greendelta.github.io](https://greendelta.github.io/openLCA-ApiDoc/#:~:text=openLCA%20provides%20an%20API%20for,to%20call%20functions%20in%20openLCA)[greendelta.github.io](https://greendelta.github.io/openLCA-ApiDoc/#:~:text=In%20the%20dialog%2C%20you%20can,browser%20using%20the%20Fetch%20API)). The agent will take inventory data from the TEA simulation (e.g. raw material requirements per functional unit, energy consumed, emissions generated on-site) and feed that into the LCA engine along with background datasets. Brightway offers high flexibility in constructing LCA models from Python and efficiently calculating impact indicators[docs.brightway.dev](https://docs.brightway.dev/en/latest/#:~:text=Brightway%20is%20designed%20to%20make,users%20from%20industry%20and%20consulting). Meanwhile, OpenLCA’s API allows connection to extensive databases (like ecoinvent or USEEIO) and even the possibility of the agent operating the OpenLCA GUI via a “copilot” mode for data curation and QA. By _“curation and QA”_, we refer to ensuring that the mapping between process simulation outputs and LCA inputs is correct – for instance, if the process produces a waste sludge, the agent (via an LLM) could search the LCA database for an appropriate disposal process, or prompt the user if multiple choices exist. We will encode guidelines so that the agent selects LCA data consistent with TEA assumptions (e.g. same geographical region, technology maturity). The LCA engine returns environmental impact results (e.g. GHG emissions, energy footprint, water use) which the agent can then interpret alongside economic results.
    

- **ML Toolkit:** This module encompasses all machine-learning components for accelerating and enhancing analysis. It includes surrogate model training (regression models like GPs, neural nets, etc.), design of experiments, and optimization algorithms. The agent can invoke functions here to, for example, fit a Gaussian Process model to simulation input-output data accumulated from multiple runs. This surrogate can then predict outcomes for new designs almost instantly, which the agent can use inside an optimization routine. We will implement Multi-Objective Bayesian Optimization (MOBO) where the agent calls a BO algorithm that uses the GP surrogates of objectives (e.g. minimized cost, minimized CO₂ emissions) to propose next candidate designs. The agent thus iteratively alternates between calling the **Process Simulator/TEA** (to get data and refine surrogates) and calling the **Optimizer** (to get improved design suggestions), until convergence on a Pareto-optimal set or until a specified number of iterations is reached. In addition, the ML toolkit can include a library of **Physics-Informed Neural Networks (PINNs)** for specific unit operations – for instance, a PINN that solves reactor differential equations faster than a conventional solver, or a PINN that predicts catalyst deactivation over time. The agent could choose to train a PINN if a certain unit’s simulation is the bottleneck in speed and if sufficient data or equations are available. Reinforcement Learning (RL) elements also reside in the ML toolkit: for example, an RL-based scenario generator that the agent can call to stochastically explore variations of a process configuration (useful in highly combinatorial design spaces where random exploration is needed beyond BO’s local search). The ML toolkit is thus the agent’s “analytical arm,” enabling it to go beyond brute-force simulation by leveraging learned models.

- **ML Toolkit:** This module encompasses all machine-learning components for accelerating and enhancing analysis. It includes surrogate model training (regression models like GPs, neural nets, etc.), design of experiments, and optimization algorithms. The agent can invoke functions here to, for example, fit a Gaussian Process model to simulation input-output data accumulated from multiple runs. This surrogate can then predict outcomes for new designs almost instantly, which the agent can use inside an optimization routine. We will implement Multi-Objective Bayesian Optimization (MOBO) where the agent calls a BO algorithm that uses the GP surrogates of objectives (e.g. minimized cost, minimized CO₂ emissions) to propose next candidate designs. The agent thus iteratively alternates between calling the **Process Simulator/TEA** (to get data and refine surrogates) and calling the **Optimizer** (to get improved design suggestions), until convergence on a Pareto-optimal set or until a specified number of iterations is reached. In addition, the ML toolkit can include a library of **Physics-Informed Neural Networks (PINNs)** for specific unit operations – for instance, a PINN that solves reactor differential equations faster than a conventional solver, or a PINN that predicts catalyst deactivation over time. The agent could choose to train a PINN if a certain unit’s simulation is the bottleneck in speed and if sufficient data or equations are available. Reinforcement Learning (RL) elements also reside in the ML toolkit: for example, an RL-based scenario generator that the agent can call to stochastically explore variations of a process configuration (useful in highly combinatorial design spaces where random exploration is needed beyond BO’s local search). The ML toolkit is thus the agent’s “analytical arm,” enabling it to go beyond brute-force simulation by leveraging learned models.


**3.2 Tool Orchestration via MCP and Agent Design:** We will implement the agent using the **Model Context Protocol (MCP)** – an emerging open standard for connecting LLMs with tools and stateful control[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=A%20state,specific%20prompts). MCP allows us to define each external tool (e.g. a Biosteam simulation function, a Brightway LCA calculation function, a plotting or analysis function) with decorators and integrate them into the agent’s prompting framework[glama.ai](https://glama.ai/mcp/servers/@aviz85/mcp-agents-orchestra#:~:text=Add%20new%20tools%20by%20creating,mcp.tool). The agent’s prompt context will include descriptions of available actions (tools), similar to how a “function calling” API works with LLMs. . The MCP orchestration ensures that the agent can maintain a working memory of the conversation and state (through the state machine) and use tools appropriately. By adopting a state-based design, we can impose **guardrails**: e.g. in the PLANNING state, the agent is only allowed to output a plan (and not directly call tools); in the EXECUTING state, it can call tools but not generate new plans; in REVIEWING, it must analyze results and decide whether criteria are met or another iteration is needed. This aligns with the practice observed by Vyas _et al._ where a _“Validator-Reprompting loop”_ caught and corrected invalid plans[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,that%2C%20with%20structured%20feedback%20and).


**3.3 TEA-LCA Coupling Approach:** A crucial methodological aspect is how we link TEA and LCA seamlessly. We will adopt a _gate-to-gate_ LCA approach focused on the process being designed, with up-stream and downstream impacts sourced from databases. The process simulator provides a detailed inventory of inputs and outputs for the system foreground (e.g. raw biomass input per batch, electricity used per kWh, emissions from fermentation, etc.). The agent will compile this inventory and map each item to an LCA dataset (for background processes). For example, if the simulation uses 1 tonne of corn stover, the agent will find an LCA entry for corn stover production; if electricity is consumed, it will choose an appropriate grid mix dataset based on region (optionally user-specified). This mapping can be done via Brightway’s search functions or via OpenLCA’s ontology if needed. To ensure consistency, **common assumptions** (like capacity factors, utility emissions factors) will be shared between TEA and LCA – possibly through a unified data schema. We will develop a schema or template that the agent fills, which includes all necessary info for both analyses (e.g. a JSON with sections for process design, economic factors, LCA factors). During each iteration of analysis, after simulation, the agent updates the inventory and pushes it to the LCA engine. The results from LCA (impact metrics) are then associated with the same scenario as the TEA results. This one-to-one coupling at each scenario enables multi-objective evaluation. If multiple scenarios are being compared (e.g. different process designs in a Pareto set), the agent ensures both TEA and LCA are computed for all before declaring one “dominant” – thereby avoiding bias towards designs that are only optimal in one dimension. We will also implement **uncertainty propagation** through this coupling: if certain inputs have probability distributions (e.g. yield uncertainty, price volatility), the agent can perform Monte Carlo sampling, feeding random draws into both TEA and LCA calculations for each run. Using vectorized or surrogate-assisted simulation, it could generate thousands of joint TEA-LCA outcomes rapidly, enabling a holistic uncertainty analysis (similar to BioSTEAM-LCA’s 1000-scenario demonstration[cabbi.bio](https://cabbi.bio/wp-content/uploads/2023/10/2021-01-DOE-Highlight-final-ACSSusChemEng_Guest.pdf#:~:text=v%20Used%20BioSTEAM,in%20environmental%20impacts%20and%20costs)). The agent will then use statistical analysis tools (in the ML toolkit) to summarize these results (e.g. probability of meeting cost target and emissions target simultaneously, sensitivity of outcomes to each uncertain parameter). Such integrated uncertainty analysis is a methodological advance ensuring that the framework not only finds “optimal” designs, but also quantifies confidence and risk.

**3.4 Machine Learning Components:** Several specific ML models and algorithms will be employed: (i) **Gaussian Process (GP) Surrogates** – for any model outputs that are computationally expensive or noisy. For instance, if optimizing a flowsheet with many continuous decision variables, instead of calling BioSTEAM for every evaluation, the agent will use GP regression to learn the mapping from decision variables to outcomes (cost, emissions). The GP’s inherent uncertainty estimate will guide the Bayesian optimizer in deciding where to sample next, focusing on regions of objective improvement or high uncertainty. (ii) **Physics-Informed Neural Networks (PINNs)** – for sub-process models like reactors, we will incorporate PINNs that the agent can train on-the-fly or load if pre-trained. For example, a PINN could solve the mass-transfer equations in a distillation column faster than the discrete tray-by-tray simulation, once trained on that column’s operating range. We will investigate training PINNs using data generated by first-principles models from BioSTEAM or external simulators, so that subsequent evaluations are instant. The agent’s logic will be: if a particular unit’s simulation accounts for a large fraction of total runtime and needs to be evaluated across many conditions, then train a PINN surrogate for it. This introduces adaptability in the framework’s use of ML. (iii) **Gradient Boosted Trees / Random Forests** – these will be used for quick approximations and feature importance analysis. For example, after gathering a dataset of ~100 scenario results (with various inputs), the agent might fit a tree-based model to rank which input uncertainties contribute most to outcome variance (providing explainability). Tree models can also serve as fallback surrogates when data is sparse or relationships are highly non-linear in ways GPs struggle with. (iv) **Multi-Objective Optimization algorithms** – aside from MOBO (which relies on GP), we will also implement or integrate evolutionary algorithms (like NSGA-II) as a comparison or backup for multi-objective optimization, particularly for non-smooth search spaces or when we want a diversity of solutions. The agent can deploy such an algorithm on the surrogate model (or even directly on simulation if feasible) to identify an approximate Pareto front, which it then refines via MOBO around the interesting regions. (v) **Reinforcement Learning for scenario policy** – as mentioned, we may encode certain decision problems as an RL environment. For instance, controlling a semi-batch process could be framed as an RL task where the agent chooses control actions over time to maximize yield and minimize energy. A policy learned in such environment (perhaps using deep Q-learning or actor-critic methods) could be embedded into the overall framework to handle dynamic optimization sub-problems. The agent’s planning step would then include using that policy to set, say, the heating profile of a reactor for each scenario rather than optimizing it with static DOEs. This way, continuous process control and static design decisions can be tackled together (similar in spirit to Vyas _et al._’s unification of planning and control[arxiv.org](https://arxiv.org/abs/2507.07115#:~:text=control%2C%20our%20LLM,driven%20automation%20in%20chemical%20engineering)).


**3.5 Agentic Decision-Making and Workflow:** Bringing everything together, a typical workflow the agent would execute is:

1. **Initialization:** User defines the problem (e.g. process type, feedstock, product, scale) and objectives. The agent gathers necessary baseline data (perhaps running a base case simulation in BioSTEAM to establish feasibility).
2. **Planning:** The LLM agent in PLANNING state outlines a plan, and orchestrator assigns the specialized agents for the subtasks.
3. **Execution – Base Case:** The agent transitions to EXECUTING state and calls the simulation tool with base case parameters (via Biosteam API). It obtains results, then calls the LCA tool with the resulting inventory. 
4. **Execution – Optimization - Decision:** The agent decides t In REVIEWING state, the agent compares the candidates. It may apply further decision logic.  The agent then runs full simulations for these candidates to verify surrogate predictions, and performs LCA on them
5. **Reporting:** The agent generates a report (in natural language with supporting tables/figures) summarizing the analysis: 

## **4. Validation Plan and Case Studies**

We will evaluate the framework on **three case studies** spanning different sectors and challenges to demonstrate generality: (1) **PEM Hydrogen Production**, (2) **Lignocellulosic Biorefinery**, and (3) **Plastics Recycling**. These cases were chosen because they are industrially relevant, involve significant TEA and LCA considerations, and can benefit from ML acceleration due to complexity.



**Case 1: Proton Exchange Membrane (PEM) Hydrogen Production** – This case involves hydrogen generation via water electrolysis using PEM technology, a cornerstone of green hydrogen strategies. The process includes an electrolyzer unit (with catalysts, membrane), power electronics, and gas separation, with electricity as the major input. TEA issues: high capital cost of electrolyzers, electricity cost sensitivity, economies of scale. LCA issues: carbon intensity of electricity (dominant factor in GHG emissions of H₂), materials (platinum group metals in catalyst), and byproduct oxygen credit. We will set up a BioSTEAM simulation or use existing models (e.g. H2A model data) for a PEM system at a given scale (e.g. 100 MW). The agent will analyze scenarios such as different electricity sources (grid vs renewable), different load factors, and improved electrolyzer efficiency. Surrogate modeling will be valuable here to explore how efficiency and stack degradation rate affect TEA/LCA, without needing a detailed electrochemical model each time. For instance, a PINN could be trained on a mechanistic PEM cell model to quickly predict performance at various current densities and temperatures, which then feeds into the economic model. We expect surrogate-driven optimization to suggest an optimal operating current that minimizes levelized hydrogen cost while balancing efficiency (this might involve a trade-off between efficiency and capital utilization). Uncertainty analysis will include electricity price volatility and electrolyzer life uncertainty. The validation will check if our agent’s recommendations (e.g. “operate at X A/cm² using solar power for lowest emissions at acceptable cost”) align with known studies or manual analyses. We’ll measure speedup: e.g. performing an exhaustive search or Monte Carlo of 1000 scenarios in the integrated TEA-LCA might take days normally, but with our surrogates and agent coordination we aim for minutes. The outputs will be compared with DOE H₂ cost targets and literature LCA benchmarks to ensure plausibility.


**Case Study 2: Biorefinery for Sustainable Aviation Fuel (SAF)** – _Process:_ Fast pyrolysis of biomass (e.g. forest residues) followed by bio-oil upgrading to sustainable aviation fuel. _Why:_ Biorefineries involve complex process networks and supply chains, where **feedstock properties, process conditions, and scale** heavily influence both economics and LCA results[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=produced%20by%20integrated%20chemical%20looping,prediction%20of%20the%20MSP%20of)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=Data,prediction%20of%20gas%20yield%20during). Many studies have done TEA and LCA for biofuels, but often separately or with limited integration. Our framework can shine here by handling the many degrees of freedom and uncertainty. _Scope:_ We’ll consider a pyrolysis + hydroprocessing pathway for SAF, producing e.g. 50 million liters per year of jet fuel. The system includes feedstock logistics (collection, transport), the conversion plant, and product distribution. We will incorporate possible co-products (electricity, biochar) as well. _Metrics:_ **Economic** – minimum selling price (MSP) of SAF in $/liter (or $/GJ), internal rate of return (IRR) at a given selling price; **Environmental** – GHG emissions per MJ fuel, perhaps also land use or other impact categories from LCA. For context, prior TEA-LCA studies (e.g. Rogalchuk & Markuts 2021, etc.) found SAF MSPs usually higher than fossil jet (often $1–2/L) and GHG reductions depending on feedstock[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=Several%20studies%20have%20reported%20the,Their)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=produced%20by%20integrated%20chemical%20looping,prediction%20of%20the%20MSP%20of). One recent study integrated process simulation with experimental data for pyrolysis, and noted _“several literature gaps”_ such as integrating feedstock property variation into the economic model[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=produced%20by%20integrated%20chemical%20looping,prediction%20of%20the%20MSP%20of)[link.springer.com](https://link.springer.com/article/10.1007/s12155-024-10803-x#:~:text=feedstock%20properties%20including%20proximate%20and,MSP%20of%20SAF%20from%20FP). We will address that by allowing the agent to vary feedstock properties (e.g. moisture, ash content) to see their effect, essentially treating feedstock as part of design. The framework might use a PINN to model how different biomass compositions affect yields and upgrading requirements, trained on lab data if available. _Use of Framework:_ The agent will coordinate a BioSTEAM simulation of the biorefinery (since BioSTEAM has been used for agile TEA-LCA in biorefineries[pubs.acs.org](https://pubs.acs.org/doi/10.1021/acssuschemeng.0c05998#:~:text=...%20pubs.acs.org%20%20BioSTEAM,offs%20across%20dimensions)). It will optimize variables like reactor temperature, hydrogen consumption in upgrading, and plant scale. It will also evaluate different **regions** or supply chain setups – thanks to integration with GIS data or a simple model for transportation, it can compare centralized vs. distributed production. Using ML, the agent could build a surrogate for the relationship between biomass properties and resulting fuel yield.

**Case 3: Plastics Recycling (Chemical Recycling of Waste Plastics)** – This case study addresses a different domain: the circular economy. We consider a process such as mixed plastic waste pyrolysis to produce an oil that can substitute crude in petrochemical processes. TEA factors: highly dependent on scale and feedstock cost (waste tipping fee or cost), product quality and yield (which influences revenue), and operational costs of pyrolysis and upgrading. LCA factors: credits for avoided incineration or landfill, emissions from pyrolysis (which can be energy-intensive), and benefits of offsetting virgin plastic production. 
 The agent will explore scenarios like different reaction temperatures (affecting yield distribution of gases vs liquids vs char), addition of catalysts, or integration with downstream processes (like cracking of pyrolysis oil). This case will test the agent’s capability to handle **discrete choices and configuration**: e.g. including a hydrogenation step to upgrade oil vs selling as is, or mechanical recycling vs chemical recycling routes.
 We can frame this as a superstructure with options, and the agent can decide via an optimization or heuristic search which route is optimal. If this becomes combinatorially large, an RL approach (with an agent incrementally “building” the process flowsheet) could be tried, akin to Gao et al.’s approach[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=,are%20employed%20to%20accelerate%20the) but guided by our LLM agent at a higher level. Surrogates might be used for the pyrolysis yield as a function of temperature and residence time (since simulating the complex kinetics repeatedly is slow). We will validate results by comparing to literature: e.g. recent studies on chemical recycling TEA/LCA often find that without policy incentives, purely economic optimization might favor disposing of low-value plastic rather than recycling, whereas an environmental objective pushes toward recycling with a cost trade-off. A validation metric: how well the agent’s identified Pareto front matches known trade-off curves in literature for plastic recycling vs energy recovery. 

 Each case study will produce a set of artifacts: a detailed report generated by the agent, the full provenance log, and comparative evaluation by us. This will allow us to showcase reproducibility and answer the question: _“Would a human analyst reach the same conclusions, and how much effort would it take them versus the agent?”_ We will gather qualitative feedback from collaborating domain experts by showing them the agent’s reports and seeing if the reasoning is convincing and if the solutions would be actionable in practice.

## **5. Work Plan and Timeline**

The project is planned for a duration of approximately 2 to 2.5 years (24–30 months), which is aggressive but feasible given the modular nature of development. Below is a Gantt chart illustrating the timeline of major tasks and milestones over this period:

![[image.png]]


- **Months 1–3: Literature Review & Problem Definition.** In this initial phase, a comprehensive literature survey will be conducted on integrated TEA-LCA methodologies, machine learning in process systems, and applications of AI agents in engineering. We will document the limitations of current methods (building on sources cited in this proposal) and refine the problem statement. By Month 3, we expect to have a clear set of requirements for the framework and selection of initial case study (likely hydrogen) to focus on first. _Milestone:_ Complete literature review document and framework specification.
    
- **Months 3–6: Framework Design & Architecture Development.** During this period, we will design the system architecture in detail and start building the foundational code. This includes setting up the LLM agent environment (possibly using an existing platform for autonomous agents), and creating interfaces for process simulation and LCA tools. We will develop small prototypes to test communication between the agent and tools (e.g. the agent triggers a sample simulation and reads results). By Month 6, we aim to have a basic working skeleton of the framework with hardcoded sequences – essentially a **proof-of-concept** integration (without yet the learning/adaptive intelligence). _Milestone:_ Architecture diagram and prototype demo where agent goes through one TEA-LCA cycle for a simple process.
    
- **Months 6–12: Tool Integration and ML Model Development.** In this phase, heavy development occurs on two fronts: (a) **Tool Integration:** We will fully integrate BioSTEAM (or Aspen) with Brightway LCA in our software. This involves writing wrapper functions or using API bindings so that the agent can run simulations and retrieve LCA results automatically. We’ll test this on a small example (e.g. a simple flowsheet). (b) **ML Surrogate Model Development:** Concurrently, we will start developing surrogate models for the first case study. For instance, if hydrogen production is first, we gather simulation data (perhaps using Aspen or a custom model of electrolysis) and train initial GP and GBM models. We’ll also set up PINN frameworks for any first-principle models identified. During this period, we likely focus on one case to avoid spread too thin – the hydrogen case is simpler (mostly continuous variables) which is a good testbed for GP/BO. By Month 12, we should have the **agent capable of using a GP surrogate and doing a simple Bayesian optimization** on the first case. _Milestone:_ Publication-quality preliminary results for Case 1 (e.g. a conference paper or internal report showing automated optimization of hydrogen process with TEA-LCA outputs).
    
- **Months 12–18: Agent Intelligence and Decision Logic Implementation.** Now that the pieces are integrated, we will work on elevating the agent’s capabilities. This includes implementing the reasoning prompts, decision rules, and fallback strategies for the agent. We’ll incorporate the optimization module (Bayesian optimizer configured, possibly a genetic algorithm or MILP solver if needed for discrete decisions). We also enhance the agent’s ability to do multi-objective analysis (maybe by formulating composite objectives or using Pareto ranking logic in its prompt). During this time, we also address scalability and robustness: improving how the agent handles errors (e.g. simulation failures) and how it logs and learns from them. We’ll likely iterate on prompt engineering for the LLM to ensure it understands the outputs correctly (this might involve fine-tuning a smaller model on domain-specific Q&A if needed). Parallel to agent improvements, we initiate **Case Study 2 (Biorefinery)** around Month 15, feeding in knowledge from the first case. That means setting up the biorefinery simulation, defining its TEA/LCA parameters, and letting the agent start exploring it. _Milestone:_ By Month 18, the agent should autonomously handle a full optimization loop on Case 1 and be actively working on Case 2. Possibly submit a journal paper on the hydrogen case study with the framework methodology described.
    
- **Months 18–24: Case Studies Expansion and Refinement.** In this half-year, we will complete Case Study 2 (biorefinery) analysis and commence **Case Study 3 (plastics recycling)**. The framework may need adjustments for each case (for instance, adding the ability to consider discrete technology choices for plastics, or incorporating supply chain modules). We will utilize the ML models heavily here – likely training new surrogates for pyrolysis processes, etc., and maybe integrating external data (like experimental results for pyrolysis yields) to augment the models. The agent’s logic might be refined further to deal with these complex scenarios (like splitting waste flows). We will also during this phase compare the framework results with other approaches: e.g. run a traditional TEA/LCA manually for one scenario as a check, or compare the agent’s chosen optimum with a brute force grid search if feasible. If any discrepancies or surprising decisions are observed, we refine the agent’s rules or retrain the LLM on those specifics (maintaining correctness). This period is also when we measure **performance improvements** – e.g. how many simulations did the agent need vs. a baseline, etc. By Month 24 (end of Year 2), we expect to have all three case studies completed with results interpreted. _Milestone:_ Submit/publish results of Case 2 and Case 3, possibly in journals like _Computers & Chemical Engineering_ or _AIChE Journal_, demonstrating the efficacy of the framework across domains.
    
- **Months 24–30: Evaluation, Documentation, and Writing.** In the final stretch (which could be 6 months, or possibly shorter if things go well), we focus on synthesizing the outcomes, evaluating the framework’s generality and limitations, and writing the Ph.D. dissertation and associated publications. We will perform a **comparative evaluation** of our framework against existing methods (see next section on comparison) using the results from the case studies. This includes quantifying how much time/effort was saved, how close to global optima we got, and any improvements in quality of analysis (like better uncertainty coverage). We will also address any remaining issues in reproducibility and finalize the ethical safeguards (ensuring all dataset licenses are complied with, etc.). By Month ~27, a full draft of the dissertation should be ready, and a defense could be scheduled around Month 30. In terms of Gantt chart, writing overlaps with the analysis of Case 3 and evaluation, as we will not wait to start writing up earlier results. _Milestone:_ Ph.D. thesis completed and submitted by end of Year 2.5, as well as all planned papers submitted.



This timeline is aggressive but feasible given the tools and resources we plan to use (leveraging existing open-source platforms and focusing on computational experiments). We have built in some overlap to keep momentum (e.g. starting next case while finishing analysis of prior one) and will adjust as needed.  Regular checkpoints (literature review, prototype, case results, papers) are included to ensure progress can be monitored and guided.



## **6. Expected Contributions**

This research will yield contributions across theoretical, methodological, technological, and practical dimensions:

- **Theoretical Contributions:** We will advance the theory of **integrated sustainable process design** by formulating a unified problem space that combines TEA and LCA as a multi-objective optimization under uncertainty. This includes new formulations of objective functions that blend economic and environmental criteria, and potentially new theoretical insights into how AI planning can be combined with engineering optimization. For instance, our use of LLMs for constrained planning in engineering can contribute to the theory of _AI planning with domain knowledge_ (showing how chemical engineering knowledge can be embedded in prompts or tool use). We also anticipate contributing to reinforcement learning theory for process synthesis by demonstrating how transfer learning or hierarchical RL can overcome some complexity barriers[arxiv.org](https://arxiv.org/abs/2302.03375#:~:text=RL%20agent%20demands%20numerous%20process,process%20design%20and%20apply%20it).
    
- **Methodological Contributions:** The project will develop a **novel methodology for autonomous process systems analysis**. This methodology outlines how to orchestrate simulations, data-driven models, and AI reasoning in a closed loop. The state-machine agent approach (inspired by MCP) applied to engineering analysis is a methodological innovation. We will document the design patterns for ensuring consistency between TEA and LCA models (essentially a methodology for TEA-LCA coupling that others can replicate, using open-source tools). Another key contribution is the methodology for **surrogate-based multi-objective optimization with an AI agent in the loop** – our approach to have the agent intelligently gather data and refine models on the fly can be applied to other domains as well. Additionally, we’ll propose best practices for **ensuring provenance and transparency** in AI-driven research workflows in engineering, addressing a current gap in methodology for explainable and auditable AI in process engineering[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=has%20become%20an%20impediment%20to,the%20mechanistic)[engineering.org.cn](https://www.engineering.org.cn/engi/EN/10.1016/j.eng.2023.11.024#:~:text=together%20with%20state,solving%20bottleneck%20challenges%20in%20CE).
    
- **Technological/Software Contributions:** We will deliver a working **software framework (prototype)** that implements the proposed system. This will likely be in the form of a Python-based platform (integrating BioSTEAM, Brightway/OpenLCA, and our agent code) that can be used and extended by others. It essentially acts as a _“Digital Copilot”_ for process engineers. We plan to open-source the code under a permissive license (e.g. MIT or BSD) to align with open science principles – addressing the noted issue that only ~20% of automation studies share code[mdpi.com](https://www.mdpi.com/2071-1050/15/6/5531#:~:text=the%20models%20was%20reached,development%20of%20new%20automation%20methodologies). This software will include modules for: connecting to simulation engines, performing LCA calculations from Python, training ML models, and the LLM agent control logic. Even beyond our specific use cases, these modules themselves are valuable – e.g. a generic Biosteam–Brightway linkage package, or an MCP tool library for engineering tasks. The technology also includes any new PINNs or surrogate models tailored to certain processes (like a PINN for pyrolysis kinetics); these models can be the basis for future research or could be integrated into process simulators. We will also contribute to data by curating any process/LCA datasets needed for the case studies and making them available (respecting licensing, e.g. using open LCA databases or providing dummy data if needed).
    
- **Practical/Empirical Contributions:** Through the case studies, we will produce **practical insights for each domain** (H₂ production, biorefineries, plastic recycling). For example, we might uncover a promising operating regime for PEM electrolyzers or identify the conditions under which chemical recycling becomes environmentally favorable. These findings can guide industry or policy. More broadly, the demonstration that an autonomous framework can do in-depth TEA-LCA in a short time can influence how feasibility studies are conducted in practice – potentially shifting some of the workload from consultants and analysts to AI assistants, thereby reducing cost and time. The framework can serve as a training tool for new engineers – they could use it to explore scenarios quickly and learn which factors matter, augmenting human expertise. We expect to show that our approach can achieve comparable or better results than a traditional analysis done by an expert, thus proving the practicality of AI augmentation in process design. If adopted, this could accelerate development of sustainable technologies by enabling rapid iterative assessment of concepts (failing fast on non-viable ideas and honing in on viable ones). Our work will also set a **benchmark** for future AI-in-the-loop PSA – as one of the first comprehensive examples, it will be something others can improve upon.
    
## 7. Ethical and Reproducibility Considerations

Employing AI-driven automation in engineering analyses raises important ethical and reproducibility issues, which we will proactively address in this project:


**Transparency and Explainability:** One concern is that an LLM agent could become a “black box,” making decisions or recommendations without clear justification. This is problematic in engineering where trust and verification are paramount. 
We will include a logging mechanism where the agent’s internal reasoning (in natural language) is recorded alongside each result. 
we can refine its prompt or employ **rule-based checks**


- **Human Oversight and Control:** The framework is designed as a **support system** for human engineers, not a fully autonomous authority. Human oversight is built in at multiple levels. The user defines initial goals and constraints – the agent is bound to these. If the agent encounters a decision beyond its scope (e.g. a trade-off involving value judgments, such as trading cost for safety), it will flag it for human input. 
- 
- **License and Intellectual Property Considerations:** We commit to open-sourcing the developed framework, which means carefully handling any third-party components’ licenses. All tools chosen (BioSTEAM, Brightway, OpenLCA API, etc.) are open-source, which avoids legal barriers. LCA databases, however, often have licenses (ecoinvent, for example, is not free for commercial use). We will use open databases (like the Open Life Cycle Database or USEEIO) for demonstration, and ensure the framework can plug in licensed data only when the user has obtained it (the agent will not distribute any proprietary data itself – it will just access what’s on the user’s system). 
- 
- **Environmental and Social Impact:** The very purpose of this framework is to promote sustainable process design by integrating environmental criteria. Ethically, this aligns with sustainability principles – we are making it easier for engineers to include environmental impacts in decision-making, which may have positive societal impact (better tech decisions for climate). However, one must be cautious that the system doesn’t get misused (for instance, someone could in principle optimize purely for profit and ignore environment if they disable that objective). While we cannot enforce users’ motives, our default stance is to always include sustainability metrics and to highlight them. We will also ensure that the agent’s output includes discussion of any ethical/environmental implications it can infer (e.g. if the optimal solution in Case 3 relies on incineration, the agent should note the trade-off in environmental harm). The LLM’s training might allow it to make such connections (especially if fine-tuned on sustainability texts).


## **8. Conclusion**

This proposal outlined a comprehensive plan to develop an autonomous, agent-driven framework for process systems analysis that integrates techno-economic analysis and life cycle assessment with advanced machine learning and optimization. By tackling current limitations – the siloed nature of TEA/LCA, the high expertise and time demands, and the underutilization of AI in the workflow – the proposed research aims to revolutionize how sustainable chemical process design is conducted. The end result will be a proof-of-concept system (validated on real case studies) showing that an AI agent, guided by domain knowledge and empowered by ML surrogates, can perform end-to-end analysis and optimization, providing human engineers with insightful recommendations while ensuring consistency, speed, and transparency.

The success of this project could mark an inflection point in process engineering methodology, demonstrating the viability of AI “co-pilots” in engineering design. It aligns with the goals of chemical engineering to accelerate innovation and with global needs for rapid development of economically and environmentally sound technologies. Moreover, the interdisciplinary knowledge generated – spanning chemical process modeling, machine learning, and AI orchestration – will contribute to the emerging field of AI for Science and Engineering.

By executing the research plan and achieving the stated objectives, this PhD will produce both significant academic contributions and a practical tool with potential for real-world impact. We anticipate that this work will not only address the immediate research gap but also pave the way for future studies (e.g. scaling to more complex processes, integrating real-time plant data for autonomous operations, etc.). In the long run, such agentic frameworks could be extended to create self-driving laboratories or digital twins that continuously learn and advise, ultimately merging with experimental efforts for closed-loop innovation.




## Reference
